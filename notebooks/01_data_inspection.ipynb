{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Data Inspection\n",
    "\n",
    "This notebook explores the SoQG and KP20k datasets to understand their structure before preprocessing.\n",
    "\n",
    "## Objectives\n",
    "- Load and inspect the SoQG dataset (110K question-context pairs)\n",
    "- Understand the 5 Socratic question types\n",
    "- Load and inspect the KP20k dataset for keyphrase extraction\n",
    "- Generate summary statistics for the report\n",
    "\n",
    "## Dataset Sources\n",
    "- **SoQG**: https://github.com/NUS-IDS/eacl23_soqg\n",
    "- **KP20k**: https://huggingface.co/datasets/midas/kp20k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "DATA_DIR = Path(\"../datasets/raw/soqg/data/soqg_dataset\")\n",
    "print(f\"Looking for data in: {DATA_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load SoQG Dataset\n",
    "\n",
    "The SoQG dataset is split into 3 training chunks, plus validation and test sets. Each row contains:\n",
    "- `input`: The context with a question type prefix (e.g., \"clarification: The earth is...\")\n",
    "- `target`: The generated Socratic question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_I = pd.read_csv(DATA_DIR / \"train_chunk_I.csv\", index_col=0)\n",
    "train_II = pd.read_csv(DATA_DIR / \"train_chunk_II.csv\", index_col=0)\n",
    "train_III = pd.read_csv(DATA_DIR / \"train_chunk_III.csv\", index_col=0)\n",
    "\n",
    "train_df = pd.concat([train_I, train_II, train_III], axis=0, ignore_index=True)\n",
    "val_df = pd.read_csv(DATA_DIR / \"val.csv\", index_col=0)\n",
    "test_df = pd.read_csv(DATA_DIR / \"test.csv\", index_col=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_df):,}\")\n",
    "print(f\"Validation samples: {len(val_df):,}\")\n",
    "print(f\"Test samples: {len(test_df):,}\")\n",
    "print(f\"Total samples: {len(train_df) + len(val_df) + len(test_df):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Inspect Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", train_df.columns.tolist())\n",
    "print(\"\\nData types:\")\n",
    "print(train_df.dtypes)\n",
    "print(\"\\nFirst 3 samples:\")\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Example input:\")\n",
    "print(train_df.iloc[0]['input'][:500])\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"\\nExample target (question):\")\n",
    "print(train_df.iloc[0]['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extract and Analyze Question Types\n",
    "\n",
    "The 5 Socratic question types from Paul & Elder's taxonomy:\n",
    "1. **clarification** - Probes ambiguity in statements\n",
    "2. **assumptions** - Examines underlying assumptions\n",
    "3. **reasons_evidence** - Asks for justification\n",
    "4. **implication_consequences** - Explores impacts\n",
    "5. **alternate_viewpoints_perspectives** - Considers other viewpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_question_type(input_text):\n",
    "    \"\"\"Extract question type prefix from input.\"\"\"\n",
    "    if ':' in input_text:\n",
    "        return input_text.split(':')[0].strip().lower()\n",
    "    return 'unknown'\n",
    "\n",
    "train_df['question_type'] = train_df['input'].apply(extract_question_type)\n",
    "val_df['question_type'] = val_df['input'].apply(extract_question_type)\n",
    "test_df['question_type'] = test_df['input'].apply(extract_question_type)\n",
    "\n",
    "print(\"Question types found:\")\n",
    "print(train_df['question_type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_counts = train_df['question_type'].value_counts()\n",
    "colors = ['#3B82F6', '#8B5CF6', '#10B981', '#F59E0B', '#EC4899']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(type_counts.index, type_counts.values, color=colors[:len(type_counts)])\n",
    "plt.xlabel('Question Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Socratic Question Types in Training Set')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "for bar, count in zip(bars, type_counts.values):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 200, \n",
    "             f'{count:,}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/question_type_distribution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Text Lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_context(input_text):\n",
    "    \"\"\"Extract context after the question type prefix.\"\"\"\n",
    "    if ':' in input_text:\n",
    "        return input_text.split(':', 1)[1].strip()\n",
    "    return input_text\n",
    "\n",
    "train_df['context'] = train_df['input'].apply(extract_context)\n",
    "train_df['context_words'] = train_df['context'].apply(lambda x: len(x.split()))\n",
    "train_df['question_words'] = train_df['target'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "print(\"Context length statistics (words):\")\n",
    "print(train_df['context_words'].describe())\n",
    "print(\"\\nQuestion length statistics (words):\")\n",
    "print(train_df['question_words'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train_df['context_words'], bins=50, color='#3B82F6', edgecolor='white', alpha=0.7)\n",
    "axes[0].axvline(train_df['context_words'].median(), color='red', linestyle='--', label=f\"Median: {train_df['context_words'].median():.0f}\")\n",
    "axes[0].set_xlabel('Word Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Context Length Distribution')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(train_df['question_words'], bins=30, color='#10B981', edgecolor='white', alpha=0.7)\n",
    "axes[1].axvline(train_df['question_words'].median(), color='red', linestyle='--', label=f\"Median: {train_df['question_words'].median():.0f}\")\n",
    "axes[1].set_xlabel('Word Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Question Length Distribution')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../docs/length_distributions.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sample Questions by Type\n",
    "\n",
    "Let's examine example questions for each type to understand the expected output format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_type in train_df['question_type'].unique():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TYPE: {q_type.upper()}\")\n",
    "    print('='*60)\n",
    "    samples = train_df[train_df['question_type'] == q_type].sample(2, random_state=42)\n",
    "    for idx, row in samples.iterrows():\n",
    "        print(f\"\\nContext (truncated): {row['context'][:200]}...\")\n",
    "        print(f\"Question: {row['target']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Check for Data Quality Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values in training set:\")\n",
    "print(train_df[['input', 'target']].isnull().sum())\n",
    "\n",
    "print(\"\\nEmpty strings:\")\n",
    "print(f\"Empty inputs: {(train_df['input'] == '').sum()}\")\n",
    "print(f\"Empty targets: {(train_df['target'] == '').sum()}\")\n",
    "\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(f\"Duplicate inputs: {train_df['input'].duplicated().sum()}\")\n",
    "print(f\"Duplicate targets: {train_df['target'].duplicated().sum()}\")\n",
    "print(f\"Exact duplicate rows: {train_df.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_contexts = train_df[train_df['context_words'] < 10]\n",
    "short_questions = train_df[train_df['question_words'] < 3]\n",
    "\n",
    "print(f\"Contexts with < 10 words: {len(short_contexts)} ({100*len(short_contexts)/len(train_df):.2f}%)\")\n",
    "print(f\"Questions with < 3 words: {len(short_questions)} ({100*len(short_questions)/len(train_df):.2f}%)\")\n",
    "\n",
    "if len(short_contexts) > 0:\n",
    "    print(\"\\nExample short context:\")\n",
    "    print(short_contexts.iloc[0]['context'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load KP20k Dataset\n",
    "\n",
    "KP20k contains scientific abstracts with extractive and abstractive keyphrases. We'll use this to train a keyphrase extraction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kp20k = load_dataset(\"midas/kp20k\", trust_remote_code=True)\n",
    "print(\"KP20k splits:\")\n",
    "print(kp20k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nKP20k features:\")\n",
    "print(kp20k['train'].features)\n",
    "\n",
    "print(\"\\nSample entry:\")\n",
    "sample = kp20k['train'][0]\n",
    "print(f\"Document (truncated): {sample['document'][:300]}...\")\n",
    "print(f\"\\nExtractive keyphrases: {sample['extractive_keyphrases']}\")\n",
    "print(f\"Abstractive keyphrases: {sample['abstractive_keyphrases']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_sample = kp20k['train'].select(range(1000))\n",
    "num_extractive = [len(x['extractive_keyphrases']) for x in kp_sample]\n",
    "num_abstractive = [len(x['abstractive_keyphrases']) for x in kp_sample]\n",
    "\n",
    "print(f\"Avg extractive keyphrases per doc: {np.mean(num_extractive):.2f}\")\n",
    "print(f\"Avg abstractive keyphrases per doc: {np.mean(num_abstractive):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics for Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    'Dataset': ['SoQG Train', 'SoQG Val', 'SoQG Test', 'KP20k Train'],\n",
    "    'Samples': [len(train_df), len(val_df), len(test_df), len(kp20k['train'])],\n",
    "    'Avg Context Words': [\n",
    "        train_df['context_words'].mean(),\n",
    "        val_df['input'].apply(lambda x: len(x.split())).mean(),\n",
    "        test_df['input'].apply(lambda x: len(x.split())).mean(),\n",
    "        np.mean([len(x['document'].split()) for x in kp20k['train'].select(range(1000))])\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "summary_df['Samples'] = summary_df['Samples'].apply(lambda x: f\"{x:,}\")\n",
    "summary_df['Avg Context Words'] = summary_df['Avg Context Words'].apply(lambda x: f\"{x:.1f}\")\n",
    "print(\"\\nDataset Summary (for report):\")\n",
    "print(summary_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_summary = train_df.groupby('question_type').agg({\n",
    "    'context_words': 'mean',\n",
    "    'question_words': 'mean',\n",
    "    'input': 'count'\n",
    "}).rename(columns={'input': 'count'})\n",
    "\n",
    "print(\"\\nQuestion Type Summary:\")\n",
    "print(type_summary.round(2).to_markdown())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Key Findings\n",
    "\n",
    "### SoQG Dataset\n",
    "- **Total samples**: ~110,000 question-context pairs\n",
    "- **5 question types**: Balanced distribution across Socratic categories\n",
    "- **Context length**: Median ~50-100 words (suitable for FLAN-T5 512 token limit)\n",
    "- **Question length**: Median ~10-15 words\n",
    "- **Quality issues**: Minimal duplicates, few empty entries\n",
    "\n",
    "### KP20k Dataset\n",
    "- **530,000+ scientific abstracts** with keyphrases\n",
    "- **Extractive keyphrases**: Words directly from text\n",
    "- **Abstractive keyphrases**: Paraphrased concepts\n",
    "\n",
    "### Next Steps\n",
    "1. Preprocess SoQG: Clean, deduplicate, format for FLAN-T5\n",
    "2. Tokenize with T5Tokenizer (max_source=512, max_target=128)\n",
    "3. Save processed datasets to disk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
