{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 — Data Preprocessing\n",
    "\n",
    "FLAN-T5 (Wei et al., 2022) is fine-tuned using instruction-style prompts that state the task in natural language. Each training example is cast into the format:\n",
    "\n",
    "```\n",
    "Input:  \"Generate a Socratic question for this context: {question_type}: {context}\"\n",
    "Target: \"[Question] {question text}\"\n",
    "```\n",
    "\n",
    "The question-type prefix (e.g., `reasons_evidence:`) is carried verbatim from the SocratiQ input column and acts as a conditioning signal for the decoder. A custom `[Question]` token is added to the tokenizer vocabulary to serve as a decoder start cue, following Ang et al. (2023, §3.1).\n",
    "\n",
    "Padding positions in the label sequence are set to -100 so the cross-entropy loss ignores them during training.\n",
    "\n",
    "**Important:** All T5-family models share the same SentencePiece vocabulary, so this tokenized dataset is used by all three training configurations (FLAN-T5-small, FLAN-T5-base, T5-base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "DRIVE_ROOT = \"/content/drive/MyDrive/socratic-path\"\n",
    "os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "print(f\"Google Drive mounted at: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers datasets sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_target = Path(DRIVE_ROOT) / \"datasets/raw/soqg\"\n",
    "\n",
    "if not (clone_target / \"data\").exists():\n",
    "    print(f\"Cloning SocratiQ dataset to {clone_target} ...\")\n",
    "    result = subprocess.run(\n",
    "        [\"git\", \"clone\", \"https://github.com/NUS-IDS/eacl23_soqg.git\", str(clone_target)],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"Clone complete.\")\n",
    "    else:\n",
    "        print(f\"Clone error: {result.stderr}\")\n",
    "else:\n",
    "    print(f\"Dataset already present at {clone_target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(DRIVE_ROOT) / \"datasets/raw/soqg/data/soqg_dataset\"\n",
    "OUTPUT_DIR = Path(DRIVE_ROOT) / \"datasets/processed\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TOKENIZER_NAME = \"google/flan-t5-base\"\n",
    "MAX_SOURCE_LENGTH = 400\n",
    "MAX_TARGET_LENGTH = 80\n",
    "\n",
    "INSTRUCTION_PREFIX = \"Generate a Socratic question for this context: \"\n",
    "TARGET_PREFIX = \"[Question] \"\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"Data directory:   {DATA_DIR}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = [\"train_chunk_I.csv\", \"train_chunk_II.csv\", \"train_chunk_III.csv\"]\n",
    "train_chunks = [pd.read_csv(DATA_DIR / f, index_col=0) for f in train_files if (DATA_DIR / f).exists()]\n",
    "train_df = pd.concat(train_chunks, axis=0, ignore_index=True)\n",
    "\n",
    "valid_df = pd.read_csv(DATA_DIR / \"valid.csv\", index_col=0)\n",
    "test_df  = pd.read_csv(DATA_DIR / \"test.csv\",  index_col=0)\n",
    "\n",
    "print(f\"Raw sizes -> Train: {len(train_df)}, Valid: {len(valid_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "for split_name, df in [(\"train\", train_df), (\"valid\", valid_df), (\"test\", test_df)]:\n",
    "    df['input']  = df['input'].apply(clean_text)\n",
    "    df['target'] = df['target'].apply(clean_text)\n",
    "    print(f\"Cleaned {split_name}: {len(df)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_empty_rows(df, split_name):\n",
    "    before = len(df)\n",
    "    df = df[df['input'].str.len() > 10].copy()\n",
    "    df = df[df['target'].str.len() > 5].copy()\n",
    "    df = df.drop_duplicates(subset=['input', 'target']).reset_index(drop=True)\n",
    "    after = len(df)\n",
    "    print(f\"{split_name}: {before} -> {after} ({before - after} removed)\")\n",
    "    return df\n",
    "\n",
    "train_df = filter_empty_rows(train_df, \"Train\")\n",
    "valid_df = filter_empty_rows(valid_df, \"Valid\")\n",
    "test_df  = filter_empty_rows(test_df,  \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Formatting\n",
    "\n",
    "Each example is reformatted as an instruction prompt. The `INSTRUCTION_PREFIX` prepends the task description; the `TARGET_PREFIX` adds the `[Question]` sentinel token to the start of every target sequence, matching the training convention in Ang et al. (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_flan_t5(row):\n",
    "    formatted_input  = INSTRUCTION_PREFIX + row['input']\n",
    "    formatted_target = TARGET_PREFIX + row['target']\n",
    "    return pd.Series({\n",
    "        'input_text':      formatted_input,\n",
    "        'target_text':     formatted_target,\n",
    "        'original_input':  row['input'],\n",
    "        'original_target': row['target']\n",
    "    })\n",
    "\n",
    "train_formatted = train_df.apply(format_for_flan_t5, axis=1)\n",
    "valid_formatted = valid_df.apply(format_for_flan_t5, axis=1)\n",
    "test_formatted  = test_df.apply(format_for_flan_t5, axis=1)\n",
    "\n",
    "print(f\"Formatted -> Train: {len(train_formatted)}, Valid: {len(valid_formatted)}, Test: {len(test_formatted)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_formatted[['input_text', 'target_text']])\n",
    "valid_dataset = Dataset.from_pandas(valid_formatted[['input_text', 'target_text']])\n",
    "test_dataset  = Dataset.from_pandas(test_formatted[['input_text', 'target_text']])\n",
    "\n",
    "print('HuggingFace datasets created:')\n",
    "print(f'  Train: {len(train_dataset):,}')\n",
    "print(f'  Valid: {len(valid_dataset):,}')\n",
    "print(f'  Test:  {len(test_dataset):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer Setup\n",
    "\n",
    "The `[Question]` token is added to the tokenizer vocabulary (extending it from 32,100 to 32,101 tokens). All T5-family models (FLAN-T5-small, FLAN-T5-base, T5-base) share the same SentencePiece vocabulary, so this single tokenizer is reused across all training configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "tokenizer.add_tokens([\"[Question]\"])\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"[Question] token ID: {tokenizer.convert_tokens_to_ids('[Question]')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text):\n",
    "    return len(tokenizer.encode(text, add_special_tokens=True))\n",
    "\n",
    "sample_size = min(5000, len(train_formatted))\n",
    "sample_df = train_formatted.sample(sample_size, random_state=RANDOM_SEED).copy()\n",
    "\n",
    "sample_df['input_tokens']  = sample_df['input_text'].apply(count_tokens)\n",
    "sample_df['target_tokens'] = sample_df['target_text'].apply(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(sample_df['input_tokens'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(MAX_SOURCE_LENGTH, color='red', linestyle='--', linewidth=2, label=f'Max: {MAX_SOURCE_LENGTH}')\n",
    "axes[0].set_title('Input Token Distribution')\n",
    "axes[0].set_xlabel('Token Count')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(sample_df['target_tokens'], bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(MAX_TARGET_LENGTH, color='red', linestyle='--', linewidth=2, label=f'Max: {MAX_TARGET_LENGTH}')\n",
    "axes[1].set_title('Target Token Distribution')\n",
    "axes[1].set_xlabel('Token Count')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "truncated_inputs  = (sample_df['input_tokens'] > MAX_SOURCE_LENGTH).sum()\n",
    "truncated_targets = (sample_df['target_tokens'] > MAX_TARGET_LENGTH).sum()\n",
    "print(f\"\\nInputs exceeding {MAX_SOURCE_LENGTH} tokens: {truncated_inputs} ({truncated_inputs/sample_size*100:.1f}%)\")\n",
    "print(f\"Targets exceeding {MAX_TARGET_LENGTH} tokens: {truncated_targets} ({truncated_targets/sample_size*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL_PAD_TOKEN_ID = -100\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"input_text\"],\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        examples[\"target_text\"],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=['input_text', 'target_text'],\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "valid_tokenized = valid_dataset.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=['input_text', 'target_text'],\n",
    "    desc=\"Tokenizing valid\"\n",
    ")\n",
    "\n",
    "test_tokenized = test_dataset.map(\n",
    "    tokenize_function, batched=True,\n",
    "    remove_columns=['input_text', 'target_text'],\n",
    "    desc=\"Tokenizing test\"\n",
    ")\n",
    "\n",
    "print(\"Tokenized dataset features:\")\n",
    "print(train_tokenized.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = OUTPUT_DIR / \"soqg_tokenized\"\n",
    "\n",
    "tokenized_dataset = DatasetDict({\n",
    "    'train':      train_tokenized,\n",
    "    'validation': valid_tokenized,\n",
    "    'test':       test_tokenized\n",
    "})\n",
    "tokenized_dataset.save_to_disk(str(dataset_path))\n",
    "print(f\"Tokenized dataset saved to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_formatted.to_parquet(OUTPUT_DIR / \"train_formatted.parquet\")\n",
    "valid_formatted.to_parquet(OUTPUT_DIR / \"valid_formatted.parquet\")\n",
    "test_formatted.to_parquet(OUTPUT_DIR / \"test_formatted.parquet\")\n",
    "print(\"Saved formatted DataFrames as Parquet files.\")\n",
    "\n",
    "tokenizer.save_pretrained(OUTPUT_DIR / \"tokenizer\")\n",
    "print(f\"Saved tokenizer with [Question] token to: {OUTPUT_DIR / 'tokenizer'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "loaded_dataset = load_from_disk(str(dataset_path))\n",
    "print(\"Loaded dataset splits:\", loaded_dataset)\n",
    "\n",
    "sample = loaded_dataset['train'][0]\n",
    "print(f\"\\nSample input_ids length: {len(sample['input_ids'])}\")\n",
    "print(f\"Sample labels length:    {len(sample['labels'])}\")\n",
    "\n",
    "decoded_input  = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "decoded_target = tokenizer.decode(sample['labels'],    skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nDecoded input:\")\n",
    "print(decoded_input[:200] + \"...\")\n",
    "print(\"\\nDecoded target:\")\n",
    "print(decoded_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The preprocessing pipeline produces four artefacts:\n",
    "\n",
    "1. **Tokenized `DatasetDict`** (`soqg_tokenized/`) — variable-length sequences (no padding), ready for training with `DataCollatorForSeq2Seq` which handles dynamic padding at batch time.\n",
    "2. **Formatted Parquet files** — human-readable text for inspection and evaluation scripts.\n",
    "3. **Extended tokenizer** (vocab size 32,101 including `[Question]`) — shared across all model configurations.\n",
    "4. **Data quality report** — token distributions and truncation statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socratic-path (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
