{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Data Preprocessing\n",
    "\n",
    "This notebook prepares the SoQG dataset for FLAN-T5 fine-tuning.\n",
    "\n",
    "## Objectives\n",
    "- Clean and deduplicate the dataset\n",
    "- Format inputs with instruction-style prompts\n",
    "- Tokenize with T5Tokenizer\n",
    "- Create train/val/test splits\n",
    "- Save processed datasets to disk\n",
    "\n",
    "## Input Format for FLAN-T5\n",
    "```\n",
    "Generate a Socratic question: {question_type}: {context}\n",
    "```\n",
    "\n",
    "## Target Format\n",
    "```\n",
    "{socratic_question}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import T5Tokenizer\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "RAW_DIR = Path(\"../datasets/raw/soqg/data/soqg_dataset\")\n",
    "PROCESSED_DIR = Path(\"../datasets/processed\")\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MAX_SOURCE_LENGTH = 512\n",
    "MAX_TARGET_LENGTH = 128\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "TASK_PREFIX = \"Generate a Socratic question\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_I = pd.read_csv(RAW_DIR / \"train_chunk_I.csv\", index_col=0)\n",
    "train_II = pd.read_csv(RAW_DIR / \"train_chunk_II.csv\", index_col=0)\n",
    "train_III = pd.read_csv(RAW_DIR / \"train_chunk_III.csv\", index_col=0)\n",
    "\n",
    "train_df = pd.concat([train_I, train_II, train_III], axis=0, ignore_index=True)\n",
    "val_df = pd.read_csv(RAW_DIR / \"val.csv\", index_col=0)\n",
    "test_df = pd.read_csv(RAW_DIR / \"test.csv\", index_col=0)\n",
    "\n",
    "print(f\"Raw sizes - Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Normalize whitespace and remove special characters.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def extract_parts(input_text):\n",
    "    \"\"\"Extract question type and context from input.\"\"\"\n",
    "    if ':' not in input_text:\n",
    "        return None, input_text\n",
    "    parts = input_text.split(':', 1)\n",
    "    q_type = parts[0].strip().lower()\n",
    "    context = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    return q_type, context\n",
    "\n",
    "def format_input(question_type, context):\n",
    "    \"\"\"Format input for FLAN-T5 instruction tuning.\"\"\"\n",
    "    return f\"{TASK_PREFIX}: {question_type}: {context}\"\n",
    "\n",
    "VALID_TYPES = {\n",
    "    'clarification',\n",
    "    'assumptions', \n",
    "    'reasons_evidence',\n",
    "    'implication_consequences',\n",
    "    'alternate_viewpoints_perspectives'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Apply Cleaning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(df, name=\"dataset\"):\n",
    "    \"\"\"Apply full cleaning pipeline to a dataframe.\"\"\"\n",
    "    df = df.copy()\n",
    "    original_len = len(df)\n",
    "    \n",
    "    df['input'] = df['input'].apply(clean_text)\n",
    "    df['target'] = df['target'].apply(clean_text)\n",
    "    \n",
    "    df = df[df['input'].str.len() > 0]\n",
    "    df = df[df['target'].str.len() > 0]\n",
    "    \n",
    "    df[['question_type', 'context']] = df['input'].apply(\n",
    "        lambda x: pd.Series(extract_parts(x))\n",
    "    )\n",
    "    \n",
    "    df = df[df['question_type'].isin(VALID_TYPES)]\n",
    "    \n",
    "    df = df[df['context'].str.split().str.len() >= 10]\n",
    "    df = df[df['target'].str.split().str.len() >= 3]\n",
    "    \n",
    "    df = df.drop_duplicates(subset=['context', 'target'])\n",
    "    \n",
    "    df['formatted_input'] = df.apply(\n",
    "        lambda row: format_input(row['question_type'], row['context']), axis=1\n",
    "    )\n",
    "    \n",
    "    print(f\"{name}: {original_len} -> {len(df)} ({100*len(df)/original_len:.1f}% retained)\")\n",
    "    return df\n",
    "\n",
    "train_clean = process_dataframe(train_df, \"Train\")\n",
    "val_clean = process_dataframe(val_df, \"Val\")\n",
    "test_clean = process_dataframe(test_df, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Verify No Data Leakage\n",
    "\n",
    "Critical: Ensure no overlap between train/val/test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_contexts = set(train_clean['context'].tolist())\n",
    "val_contexts = set(val_clean['context'].tolist())\n",
    "test_contexts = set(test_clean['context'].tolist())\n",
    "\n",
    "train_val_overlap = len(train_contexts & val_contexts)\n",
    "train_test_overlap = len(train_contexts & test_contexts)\n",
    "val_test_overlap = len(val_contexts & test_contexts)\n",
    "\n",
    "print(f\"Train-Val overlap: {train_val_overlap}\")\n",
    "print(f\"Train-Test overlap: {train_test_overlap}\")\n",
    "print(f\"Val-Test overlap: {val_test_overlap}\")\n",
    "\n",
    "if train_val_overlap > 0 or train_test_overlap > 0:\n",
    "    print(\"\\n⚠️ WARNING: Data leakage detected! Removing overlapping samples...\")\n",
    "    val_clean = val_clean[~val_clean['context'].isin(train_contexts)]\n",
    "    test_clean = test_clean[~test_clean['context'].isin(train_contexts)]\n",
    "    print(f\"After removal - Val: {len(val_clean)}, Test: {len(test_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)\n",
    "print(f\"Tokenizer vocab size: {tokenizer.vocab_size}\")\n",
    "print(f\"Model max length: {tokenizer.model_max_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyze Token Lengths\n",
    "\n",
    "Verify our max lengths (512 source, 128 target) are appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = train_clean.sample(min(1000, len(train_clean)), random_state=SEED)\n",
    "\n",
    "input_lengths = []\n",
    "target_lengths = []\n",
    "\n",
    "for _, row in tqdm(sample.iterrows(), total=len(sample), desc=\"Tokenizing samples\"):\n",
    "    input_tokens = tokenizer(row['formatted_input'], truncation=False)\n",
    "    target_tokens = tokenizer(row['target'], truncation=False)\n",
    "    input_lengths.append(len(input_tokens['input_ids']))\n",
    "    target_lengths.append(len(target_tokens['input_ids']))\n",
    "\n",
    "print(f\"\\nInput token lengths:\")\n",
    "print(f\"  Mean: {np.mean(input_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(input_lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(input_lengths, 95):.1f}\")\n",
    "print(f\"  Max: {np.max(input_lengths)}\")\n",
    "print(f\"  % over {MAX_SOURCE_LENGTH}: {100*np.mean(np.array(input_lengths) > MAX_SOURCE_LENGTH):.1f}%\")\n",
    "\n",
    "print(f\"\\nTarget token lengths:\")\n",
    "print(f\"  Mean: {np.mean(target_lengths):.1f}\")\n",
    "print(f\"  Median: {np.median(target_lengths):.1f}\")\n",
    "print(f\"  95th percentile: {np.percentile(target_lengths, 95):.1f}\")\n",
    "print(f\"  Max: {np.max(target_lengths)}\")\n",
    "print(f\"  % over {MAX_TARGET_LENGTH}: {100*np.mean(np.array(target_lengths) > MAX_TARGET_LENGTH):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Tokenization Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize inputs and targets for Seq2Seq training.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples['formatted_input'],\n",
    "        max_length=MAX_SOURCE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    labels = tokenizer(\n",
    "        examples['target'],\n",
    "        max_length=MAX_TARGET_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length'\n",
    "    )\n",
    "    \n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Convert to HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_clean[['formatted_input', 'target', 'question_type']].reset_index(drop=True)\n",
    "val_data = val_clean[['formatted_input', 'target', 'question_type']].reset_index(drop=True)\n",
    "test_data = test_clean[['formatted_input', 'target', 'question_type']].reset_index(drop=True)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data)\n",
    "val_dataset = Dataset.from_pandas(val_data)\n",
    "test_dataset = Dataset.from_pandas(test_data)\n",
    "\n",
    "print(f\"Train: {len(train_dataset)}\")\n",
    "print(f\"Val: {len(val_dataset)}\")\n",
    "print(f\"Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Apply Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokenized = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['formatted_input', 'target'],\n",
    "    desc=\"Tokenizing train\"\n",
    ")\n",
    "\n",
    "val_tokenized = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['formatted_input', 'target'],\n",
    "    desc=\"Tokenizing val\"\n",
    ")\n",
    "\n",
    "test_tokenized = test_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=['formatted_input', 'target'],\n",
    "    desc=\"Tokenizing test\"\n",
    ")\n",
    "\n",
    "print(f\"\\nTokenized features: {train_tokenized.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Verify Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = 0\n",
    "sample_input_ids = train_tokenized[sample_idx]['input_ids']\n",
    "sample_labels = train_tokenized[sample_idx]['labels']\n",
    "\n",
    "print(\"Original input:\")\n",
    "print(train_data.iloc[sample_idx]['formatted_input'][:200], \"...\")\n",
    "print(\"\\nDecoded input:\")\n",
    "print(tokenizer.decode(sample_input_ids, skip_special_tokens=True)[:200], \"...\")\n",
    "print(\"\\nOriginal target:\")\n",
    "print(train_data.iloc[sample_idx]['target'])\n",
    "print(\"\\nDecoded target:\")\n",
    "print(tokenizer.decode(sample_labels, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Processed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\n",
    "    'train': train_tokenized,\n",
    "    'validation': val_tokenized,\n",
    "    'test': test_tokenized\n",
    "})\n",
    "\n",
    "dataset_dict.save_to_disk(PROCESSED_DIR / \"soqg_tokenized\")\n",
    "print(f\"Saved tokenized dataset to {PROCESSED_DIR / 'soqg_tokenized'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean.to_csv(PROCESSED_DIR / \"train_clean.csv\", index=False)\n",
    "val_clean.to_csv(PROCESSED_DIR / \"val_clean.csv\", index=False)\n",
    "test_clean.to_csv(PROCESSED_DIR / \"test_clean.csv\", index=False)\n",
    "print(\"Saved clean CSVs for reference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Question Type Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_dist = train_clean['question_type'].value_counts()\n",
    "type_dist.to_csv(PROCESSED_DIR / \"question_type_distribution.csv\")\n",
    "print(\"Question type distribution:\")\n",
    "print(type_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Generate Preprocessing Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = f\"\"\"\n",
    "# Preprocessing Report\n",
    "\n",
    "## Configuration\n",
    "- Model: {MODEL_NAME}\n",
    "- Max source length: {MAX_SOURCE_LENGTH}\n",
    "- Max target length: {MAX_TARGET_LENGTH}\n",
    "- Random seed: {SEED}\n",
    "\n",
    "## Dataset Sizes\n",
    "| Split | Raw | Clean | Retention |\n",
    "|-------|-----|-------|----------|\n",
    "| Train | {len(train_df)} | {len(train_clean)} | {100*len(train_clean)/len(train_df):.1f}% |\n",
    "| Val | {len(val_df)} | {len(val_clean)} | {100*len(val_clean)/len(val_df):.1f}% |\n",
    "| Test | {len(test_df)} | {len(test_clean)} | {100*len(test_clean)/len(test_df):.1f}% |\n",
    "\n",
    "## Question Type Distribution (Train)\n",
    "{type_dist.to_markdown()}\n",
    "\n",
    "## Cleaning Steps Applied\n",
    "1. Removed empty inputs/targets\n",
    "2. Filtered invalid question types\n",
    "3. Removed contexts < 10 words\n",
    "4. Removed questions < 3 words\n",
    "5. Deduplicated by (context, target) pair\n",
    "6. Verified no train/test overlap\n",
    "\n",
    "## Output Files\n",
    "- `soqg_tokenized/`: HuggingFace Dataset (ready for training)\n",
    "- `train_clean.csv`, `val_clean.csv`, `test_clean.csv`: Clean CSVs\n",
    "- `question_type_distribution.csv`: Type counts\n",
    "\"\"\"\n",
    "\n",
    "with open(PROCESSED_DIR / \"preprocessing_report.md\", \"w\") as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Next Steps\n",
    "\n",
    "The preprocessed data is ready for training. Proceed to:\n",
    "1. **03_training.ipynb** - Fine-tune FLAN-T5 on the tokenized dataset\n",
    "2. Use `datasets.load_from_disk(\"../datasets/processed/soqg_tokenized\")` to load"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
