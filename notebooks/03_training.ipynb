{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Training FLAN-T5 with LoRA for Socratic Question Generation\n",
    "\n",
    "This notebook fine-tunes FLAN-T5 using **LoRA (Low-Rank Adaptation)** on the SOQG dataset. \n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load preprocessed dataset\n",
    "2. Configure LoRA adapter\n",
    "3. Train with Seq2SeqTrainer\n",
    "4. Evaluate and save adapter\n",
    "5. (Optional) Merge adapter with base model\n",
    "\n",
    "---\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "| Model | Full Fine-tuning | LoRA (r=16) | Speedup |\n",
    "|-------|-----------------|-------------|----------|\n",
    "| flan-t5-small (77M) | 8 GB | 3 GB | 2.7\u00d7 |\n",
    "| flan-t5-base (250M) | 16 GB | 6 GB | 2.7\u00d7 |\n",
    "| flan-t5-large (780M) | 32 GB | 12 GB | 2.7\u00d7 |\n",
    "\n",
    "**Apple Silicon Users:** LoRA enables training FLAN-T5-base on M1/M2 Macs! \ud83c\udf89"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "Mount Google Drive to save processed data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# DRIVE_ROOT = \"/content/drive/MyDrive/socratic-path\"\n",
    "# os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "# print(f\"Google Drive mounted at: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q peft>=0.7.0 evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device Priority:**\n",
    "1. CUDA (NVIDIA GPUs) - Full support with fp16\n",
    "2. MPS (Apple Silicon M1/M2/M3) - GPU acceleration on Mac\n",
    "3. CPU - Fallback option (slowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    if gpu_props:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(\"Warning: Training on CPU will be significantly slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Choose your model based on available hardware:\n",
    "\n",
    "- `flan-t5-small` (77M): Good for prototyping, runs on any hardware\n",
    "- `flan-t5-base` (250M): **Recommended for production**, best quality/speed trade-off\n",
    "- `flan-t5-large` (780M): Highest quality, requires 12+ GB VRAM with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "DRIVE_ROOT = \"..\"  # Change to DRIVE_ROOT if using Colab\nDATA_DIR = Path(DRIVE_ROOT) / \"datasets/processed\"\nMODEL_OUTPUT_DIR = Path(DRIVE_ROOT) / \"models/flan-t5-socratic-lora\"\nLOGS_DIR = Path(DRIVE_ROOT) / \"logs\"\n\nMODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\nLOGS_DIR.mkdir(parents=True, exist_ok=True)\n\n# NOTE: The existing trained adapter (in models/flan-t5-socratic-lora/) was trained\n# with flan-t5-small (274 minutes on Apple Silicon MPS). To re-train with base,\n# change back to flan-t5-base and run from a CUDA GPU (Colab/Kaggle).\nMODEL_NAME = \"google/flan-t5-small\"  # 77M params \u2014 matches the existing trained adapter\n# MODEL_NAME = \"google/flan-t5-base\"  # 250M params \u2014 use this on Colab with GPU\n# MODEL_NAME = \"google/flan-t5-large\" # 780M params, 12 GB VRAM with LoRA\n\nprint(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "- `r` (rank): Controls adapter capacity. Higher = more parameters, better quality\n",
    "  - `r=8`: Fast, minimal memory (~150K params)\n",
    "  - `r=16`: **Balanced, recommended** (~300K params)\n",
    "  - `r=32`: High quality (~600K params)\n",
    "\n",
    "- `lora_alpha`: Scaling factor, typically 2\u00d7r\n",
    "\n",
    "- `target_modules`: Which layers to adapt\n",
    "  - `[\"q\", \"v\"]`: Minimal, fastest\n",
    "  - `[\"q\", \"k\", \"v\", \"o\"]`: **Recommended for seq2seq**\n",
    "  - `[\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]`: Maximum capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 LoRA Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# KEY CHANGES from original:\n#\n# 1. target_modules now includes T5's feedforward layers (wi_0, wi_1, wo).\n#    For seq2seq generation tasks, FFN layers carry most of the learned\n#    \"how to phrase the output\" signal.  Attention-only LoRA (q/k/v/o) captures\n#    *what* to attend to but limits how well the decoder can generate novel text.\n#    Adding FFN layers roughly doubles trainable params (1.4% \u2192 ~2.8%) while\n#    staying well within Colab free-tier memory.\n#\n# 2. modules_to_save = [\"embed_tokens\", \"lm_head\"]\n#    CRITICAL FIX.  Because we added [Question] as a special token in\n#    02_preprocessing.ipynb, the tokenizer vocabulary grew from 32100 \u2192 32101\n#    and we called model.resize_token_embeddings(len(tokenizer)).  LoRA's\n#    default behaviour is to save ONLY the adapter delta weights, leaving the\n#    base embedding matrix untouched.  When you later reload the adapter on top\n#    of the *original* unresized base model the shapes don't match \u2192 RuntimeError.\n#    modules_to_save tells PEFT to save the full embed_tokens and lm_head tensors\n#    alongside the adapter.  Combined with the correct load sequence in the\n#    inference section (resize THEN load adapter), this eliminates the bug.\n#\n# Reference: PEFT docs \u00a7\"modules_to_save\"; Hu et al. (2022) LoRA \u00a73.\n\nLORA_CONFIG = {\n    \"r\": 16,\n    # Scaling: lora_alpha/r controls the effective learning rate of the adapter.\n    # alpha=32 with r=16 gives a scale of 2.0 \u2014 standard for seq2seq tasks.\n    \"lora_alpha\": 32,\n    # Attention + feedforward layers for T5/FLAN-T5 decoder generation quality.\n    \"target_modules\": [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n    \"lora_dropout\": 0.1,\n    \"bias\": \"none\",\n    \"task_type\": TaskType.SEQ_2_SEQ_LM,\n    # Save resized embedding layers so inference loading works after vocab resize.\n    \"modules_to_save\": [\"embed_tokens\", \"lm_head\"],\n}\n\nprint(\"LoRA Configuration:\")\nfor k, v in LORA_CONFIG.items():\n    if k != \"task_type\":\n        print(f\"  {k}: {v}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Key Differences from Full Fine-tuning:**\n",
    "\n",
    "1. **Higher learning rate** (1e-4 vs 5e-5): LoRA needs stronger signal\n",
    "2. **Larger batch size** (8 vs 4): More memory available\n",
    "3. **More epochs** (10 vs 5): Faster per epoch, so we can train longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Training Hyperparameters \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# Changes from original:\n# - num_train_epochs: 5 \u2192 10.  With LoRA (~2.8% trainable params) each epoch\n#   is fast; the model needs more passes to converge.  Early stopping (patience=5\n#   below) will halt training if validation ROUGE-L stops improving, so setting\n#   10 epochs is a safe upper bound, not a guaranteed runtime.\n# - lr_scheduler_type: \"linear\" \u2192 \"cosine\".  Cosine annealing is the standard\n#   for fine-tuning transformer models; it prevents sharp loss spikes at the end\n#   of training that linear decay can cause.\n# - eval_steps / save_steps: 1000 \u2192 500.  More frequent checkpoints mean we\n#   capture the best model earlier, especially important when Colab sessions can\n#   terminate mid-epoch.\n\nTRAINING_CONFIG = {\n    \"learning_rate\": 1e-4,          # Standard LoRA LR for seq2seq tasks\n    \"per_device_train_batch_size\": 8,\n    \"per_device_eval_batch_size\": 8,\n    \"gradient_accumulation_steps\": 2,  # Effective batch = 16\n    \"num_train_epochs\": 10,            # Upper bound; early stopping will fire sooner\n    \"lr_scheduler_type\": \"cosine\",     # Better convergence than linear for fine-tuning\n    \"warmup_steps\": 500,\n    \"weight_decay\": 0.01,\n    \"max_source_length\": 400,\n    \"max_target_length\": 80,\n    \"fp16\": torch.cuda.is_available(),  # Only on CUDA; MPS/CPU use bf16 or fp32\n    \"seed\": 42,\n    # Evaluation generation: deterministic beam search so validation ROUGE is\n    # reproducible and directly comparable to the SOQG paper's reported scores.\n    # Do NOT use sampling here \u2014 sampling introduces random variance that masks\n    # real learning signal and lowers scores vs paper benchmarks.\n    \"eval_num_beams\": 4,\n    \"eval_do_sample\": False,\n}\n\nprint(\"Training Configuration:\")\nfor k, v in TRAINING_CONFIG.items():\n    print(f\"  {k}: {v}\")\n\neffective_batch_size = (\n    TRAINING_CONFIG[\"per_device_train_batch_size\"]\n    * TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n)\nprint(f\"\\nEffective batch size: {effective_batch_size}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(TRAINING_CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 84582\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(str(DATA_DIR / \"soqg_tokenized\"))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: ../datasets/processed/tokenizer\n",
      "Tokenizer vocabulary size: 32101\n",
      "[Question] token ID: 32100\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = str(DATA_DIR / \"tokenizer\")\n",
    "print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
    "\n",
    "if not Path(tokenizer_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Tokenizer not found at {tokenizer_path}. \"\n",
    "        \"Please run 02_preprocessing.ipynb first to generate the tokenizer.\"\n",
    "    )\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"[Question] token ID: {tokenizer.convert_tokens_to_ids('[Question]')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model and Apply LoRA\n",
    "\n",
    "**This is the key difference from full fine-tuning!**\n",
    "\n",
    "Instead of training all parameters, we:\n",
    "1. Load the base model (frozen)\n",
    "2. Add LoRA adapter layers (trainable)\n",
    "3. Only train the adapter (~0.4% of parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/flan-t5-base\n",
      "Base model parameters: 247,536,384\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Base model parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 251,075,328 || trainable%: 1.4095\n",
      "\n",
      "Parameter reduction: 71\u00d7 fewer trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Enable input gradients for gradient checkpointing compatibility\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate reduction\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = model.num_parameters()\n",
    "reduction = total / trainable\n",
    "print(f\"\\nParameter reduction: {reduction:.0f}\u00d7 fewer trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Evaluation Metric (training-time) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# During training the Seq2SeqTrainer calls compute_metrics after generating\n# predictions with predict_with_generate=True.  We use deterministic beam search\n# (num_beams=4, no sampling) so that validation ROUGE-L is reproducible and\n# directly comparable to the SOQG paper's Table 3 results.\n#\n# ROUGE is computed at corpus level (all predictions vs all references in one\n# call) which is the academic standard and matches the paper's evaluation.\n\nrouge_metric = evaluate.load(\"rouge\")\n\n\ndef compute_metrics(eval_preds):\n    \"\"\"Corpus-level ROUGE for Seq2SeqTrainer.\n\n    The trainer passes raw token-id arrays; we decode them, strip padding, and\n    compute ROUGE-1/2/L over the full batch at once (corpus-level).\n    \"\"\"\n    predictions, labels = eval_preds\n\n    # Replace -100 (ignore_index) with pad_token_id before decoding\n    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Strip whitespace and the [Question] prefix used in targets\n    decoded_preds = [p.replace(\"[Question]\", \"\").strip() for p in decoded_preds]\n    decoded_labels = [l.replace(\"[Question]\", \"\").strip() for l in decoded_labels]\n\n    # Corpus-level ROUGE \u2014 one call over the entire batch\n    result = rouge_metric.compute(\n        predictions=decoded_preds,\n        references=decoded_labels,\n        use_stemmer=True,\n    )\n\n    return {\n        \"rouge1\": result[\"rouge1\"],\n        \"rouge2\": result[\"rouge2\"],\n        \"rougeL\": result[\"rougeL\"],\n    }\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Seq2SeqTrainingArguments \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# Changes from original:\n# - lr_scheduler_type: \"linear\" \u2192 \"cosine\"\n# - eval_steps / save_steps: 1000 \u2192 500  (more frequent checkpointing)\n# - save_total_limit: 2 \u2192 3              (keep best + 2 recent checkpoints)\n# - generation_num_beams: 4              (deterministic beam search at eval)\n# - generation_max_new_tokens replaces generation_max_length (avoids re-counting\n#   the input tokens in the length budget for encoder-decoder models)\n\nrun_name = f\"socratic-lora-r{LORA_CONFIG['r']}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=str(MODEL_OUTPUT_DIR / \"checkpoints\"),\n    run_name=run_name,\n\n    # \u2500\u2500 Epochs & batch \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n    per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n\n    # \u2500\u2500 Optimiser \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n    lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],   # cosine\n\n    # \u2500\u2500 Precision \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    fp16=TRAINING_CONFIG[\"fp16\"],\n\n    # \u2500\u2500 Checkpointing \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    eval_strategy=\"steps\",\n    eval_steps=500,           # More frequent \u2014 important on Colab with timeouts\n    save_strategy=\"steps\",\n    save_steps=500,\n    save_total_limit=3,       # Best + 2 most-recent checkpoints\n\n    load_best_model_at_end=True,\n    metric_for_best_model=\"rougeL\",\n    greater_is_better=True,\n\n    # \u2500\u2500 Logging \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    logging_dir=str(LOGS_DIR / run_name),\n    logging_steps=100,\n    report_to=\"tensorboard\",\n\n    # \u2500\u2500 Generation (evaluation only) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    # Deterministic beam search so validation ROUGE is reproducible and\n    # comparable to the paper's Table 3. Do NOT use do_sample=True here.\n    predict_with_generate=True,\n    generation_max_length=TRAINING_CONFIG[\"max_target_length\"],\n    generation_num_beams=TRAINING_CONFIG[\"eval_num_beams\"],    # 4\n\n    # \u2500\u2500 Reproducibility \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n    seed=TRAINING_CONFIG[\"seed\"],\n    dataloader_num_workers=0 if device == \"mps\" else 2,\n    dataloader_pin_memory=False if device == \"mps\" else True,\n)\n\nprint(f\"Run name: {run_name}\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for Colab environment\n",
    "if device == \"cuda\":\n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 enabled for faster training on compatible GPUs\")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Early Stopping & Trainer \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# patience=5 (was 3): gives the model more time to recover from plateau phases,\n# which are common in the first few thousand LoRA training steps.\n\nearly_stopping = EarlyStoppingCallback(\n    early_stopping_patience=5,\n    early_stopping_threshold=0.001,\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=dataset[\"train\"],\n    eval_dataset=dataset[\"validation\"],\n    processing_class=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n    callbacks=[early_stopping],\n)\n\nprint(\"Trainer initialised.\")\nprint(f\"  Training samples : {len(dataset['train']):,}\")\nprint(f\"  Validation samples : {len(dataset['validation']):,}\")\nprint(f\"  Early stopping patience : 5 steps\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Validation\n",
    "\n",
    "Run evaluation before training to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline evaluation (before fine-tuning)...\n",
      "\n",
      "Baseline Metrics:\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n"
     ]
    }
   ],
   "source": [
    "print(\"Running baseline evaluation (before fine-tuning)...\")\n",
    "baseline_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Metrics:\")\n",
    "for key, value in baseline_results.items():\n",
    "    if \"rouge\" in key:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA training...\n",
      "Training samples: 84582\n",
      "Validation samples: 10573\n",
      "LoRA config: r=16, \u03b1=32, targets=['q', 'k', 'v', 'o']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='26435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   93/26435 04:54 < 23:39:21, 0.31 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoRA config: r=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u03b1=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mlora_alpha\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, targets=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mtarget_modules\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting LoRA training...\")\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"LoRA config: r={LORA_CONFIG['r']}, \u03b1={LORA_CONFIG['lora_alpha']}, targets={LORA_CONFIG['target_modules']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Complete!\n",
      "==================================================\n",
      "Total training time: 16450.4 seconds (274.2 minutes)\n",
      "Training samples/second: 25.71\n",
      "Final training loss: 3.0771\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.1f} seconds ({train_result.metrics['train_runtime']/60:.1f} minutes)\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1322' max='1322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1322/1322 14:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "  eval_loss: 1.6739\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n",
      "  eval_runtime: 925.0305\n",
      "  eval_samples_per_second: 11.4300\n",
      "  eval_steps_per_second: 1.4290\n",
      "  epoch: 1.5132\n"
     ]
    }
   ],
   "source": [
    "print(\"Running final evaluation...\")\n",
    "final_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for key, value in final_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improvement over Baseline:\n",
      "  rouge1: 0.2869 -> 0.2869 (+0.0000)\n",
      "  rouge2: 0.0728 -> 0.0728 (+0.0000)\n",
      "  rougeL: 0.2726 -> 0.2726 (+0.0000)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImprovement over Baseline:\")\n",
    "for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    baseline = baseline_results.get(f\"eval_{metric}\", 0)\n",
    "    final = final_results.get(f\"eval_{metric}\", 0)\n",
    "    improvement = final - baseline\n",
    "    print(f\"  {metric}: {baseline:.4f} -> {final:.4f} ({improvement:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LoRA Adapter\n",
    "\n",
    "We save only the adapter weights (~1-2 MB), not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 LoRA adapter saved to: ../models/flan-t5-socratic-lora/adapter\n",
      "  Adapter size: ~137.8 MB\n"
     ]
    }
   ],
   "source": [
    "adapter_path = MODEL_OUTPUT_DIR / \"adapter\"\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "print(f\"\u2713 LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "import os\n",
    "adapter_size = sum(os.path.getsize(adapter_path / f) for f in os.listdir(adapter_path) if os.path.isfile(adapter_path / f)) / 1e6\n",
    "print(f\"  Adapter size: ~{adapter_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge & Save Full Model (Optional)\n",
    "\n",
    "For deployment, you can merge the adapter back into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA weights into base model...\n",
      "\u2713 Merged model saved to: ../models/flan-t5-socratic-lora/merged\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA weights into base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_path = MODEL_OUTPUT_DIR / \"merged\"\n",
    "merged_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merged_model.save_pretrained(str(merged_path))\n",
    "tokenizer.save_pretrained(str(merged_path))\n",
    "\n",
    "print(f\"\u2713 Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Training summary saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "training_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"run_name\": run_name,\n",
    "    \"lora_config\": {k: v for k, v in LORA_CONFIG.items() if k != \"task_type\"},\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    \"total_params\": model.num_parameters(),\n",
    "    \"trainable_percent\": sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters() * 100,\n",
    "    \"baseline_metrics\": {k: float(v) for k, v in baseline_results.items() if isinstance(v, (int, float))},\n",
    "    \"final_metrics\": {k: float(v) for k, v in final_results.items() if isinstance(v, (int, float))},\n",
    "    \"training_time_seconds\": train_result.metrics['train_runtime']\n",
    "}\n",
    "\n",
    "with open(adapter_path / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"\u2713 Training summary saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Quick Inference Test (post-training) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# At INFERENCE time the prompt must match the format seen during TRAINING.\n# Training data format:  \"Generate a Socratic question for this context:\n#                         {question_type}: {context_text}\"\n# e.g.:  \"Generate a Socratic question for this context:\n#          reasons_evidence: I believe fast food is harmless.\"\n#\n# For user-entered free text (no known question type), we default to\n# \"reasons_evidence\" as it is the most common type in SocratiQ (35% of samples)\n# and covers general argumentative contexts well.\n#\n# TWO generation configs:\n#   eval_config   \u2014 deterministic beam search (for reproducible ROUGE / paper\n#                   comparisons; always use this in 04_evaluation.ipynb)\n#   sample_config \u2014 stochastic sampling (for the live demo / frontend;\n#                   produces more varied, natural-sounding questions)\n\neval_config = dict(\n    max_length=TRAINING_CONFIG[\"max_target_length\"],\n    num_beams=4,\n    do_sample=False,\n)\n\nsample_config = dict(\n    max_length=TRAINING_CONFIG[\"max_target_length\"],\n    num_beams=2,\n    do_sample=True,\n    temperature=0.8,\n    top_p=0.9,\n    repetition_penalty=1.2,\n    no_repeat_ngram_size=3,\n)\n\n# Test with question-type-aware prompts (matching training format)\ntest_cases = [\n    (\"reasons_evidence\",\n     \"Climate change is not as serious as scientists claim because \"\n     \"the weather has always changed throughout history.\"),\n    (\"clarity\",\n     \"Social media is making teenagers more depressed and we should \"\n     \"ban it for anyone under 18.\"),\n    (\"implication_consequences\",\n     \"Artificial intelligence will eventually replace all human jobs \"\n     \"and we need to prepare for universal basic income.\"),\n]\n\nmodel.eval()\nprint(\"Sample Generations (beam search \u2014 deterministic)\")\nprint(\"=\" * 65)\n\nfor q_type, context in test_cases:\n    input_text = (\n        f\"Generate a Socratic question for this context: \"\n        f\"{q_type}: {context}\"\n    )\n    inputs = tokenizer(input_text, return_tensors=\"pt\",\n                       max_length=400, truncation=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **eval_config)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    generated = generated.replace(\"[Question]\", \"\").strip()\n\n    print(f\"Type    : {q_type}\")\n    print(f\"Context : {context[:90]}...\")\n    print(f\"Question: {generated}\")\n    print(\"-\" * 65)\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Adapter for Inference\n",
    "\n",
    "Here's how to load and use the trained adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Load Adapter for Inference \u2014 CORRECT SEQUENCE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The original code failed with:\n#   RuntimeError: size mismatch for embed_tokens / lm_head\n#     expected [32101, 512], got [32128, 512]\n#\n# Root cause: the base model was loaded at its default vocab size (32128) and\n# then the adapter \u2014 which was trained on a 32101-token vocab \u2014 was loaded on\n# top.  Shape mismatch.\n#\n# Fix: ALWAYS resize the base model BEFORE loading the adapter.  Since we used\n# modules_to_save=[\"embed_tokens\",\"lm_head\"] in training, the adapter directory\n# contains the correct resized weight tensors for those modules.  Loading the\n# adapter after the resize aligns the shapes correctly.\n\nprint(\"Loading tokenizer from adapter directory...\")\ntokenizer_inference = T5Tokenizer.from_pretrained(str(adapter_path))\nprint(f\"  Vocab size: {len(tokenizer_inference)}\")\n\nprint(f\"\\nLoading base model: {MODEL_NAME}\")\nbase_model_inference = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\n# CRITICAL: resize BEFORE loading the adapter\nbase_model_inference.resize_token_embeddings(len(tokenizer_inference))\nprint(f\"  Base model vocab resized to {len(tokenizer_inference)}\")\n\nprint(\"\\nLoading LoRA adapter...\")\nmodel_inference = PeftModel.from_pretrained(base_model_inference, str(adapter_path))\nmodel_inference.eval()\n\nprint(\"\\n\u2713 Model loaded successfully \u2014 ready for inference.\")\n\n# Quick sanity check\nsample_input = \"Generate a Socratic question for this context: reasons_evidence: We should ban all fast food.\"\nenc = tokenizer_inference(sample_input, return_tensors=\"pt\")\nwith torch.no_grad():\n    out = model_inference.generate(**enc, max_length=50, num_beams=4)\nprint(f\"\\nSanity check: {tokenizer_inference.decode(out[0], skip_special_tokens=True)}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Instructions\n",
    "\n",
    "To view training logs, run in terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ../logs\n",
    "```\n",
    "\n",
    "Then open http://localhost:6006 in your browser.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Complete!\n",
    "\n",
    "**Outputs:**\n",
    "- LoRA adapter: `../models/flan-t5-socratic-lora/adapter/`\n",
    "- Merged model: `../models/flan-t5-socratic-lora/merged/`\n",
    "- Checkpoints: `../models/flan-t5-socratic-lora/checkpoints/`\n",
    "- Logs: `../logs/{run_name}/`\n",
    "\n",
    "**Key Advantages:**\n",
    "- \u2705 Adapter size: ~1-2 MB (vs 300 MB for full model)\n",
    "- \u2705 Training time: 20-50% faster\n",
    "- \u2705 Memory usage: 60% less\n",
    "- \u2705 Quality: 95-98% of full fine-tuning\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare metrics with full fine-tuning (03_training.ipynb)\n",
    "2. Proceed to `04_evaluation.ipynb` for comprehensive evaluation\n",
    "3. Consider training multiple adapters for different question types (see `03_training_multi_adapter.ipynb`)\n",
    "4. Scale to FLAN-T5-base for better quality (change `MODEL_NAME` above)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}