{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Training FLAN-T5 with LoRA for Socratic Question Generation\n",
    "\n",
    "This notebook fine-tunes FLAN-T5 using **LoRA (Low-Rank Adaptation)** on the SOQG dataset. \n",
    "\n",
    "---\n",
    "\n",
    "## Workflow\n",
    "\n",
    "1. Load preprocessed dataset\n",
    "2. Configure LoRA adapter\n",
    "3. Train with Seq2SeqTrainer\n",
    "4. Evaluate and save adapter\n",
    "5. (Optional) Merge adapter with base model\n",
    "\n",
    "---\n",
    "\n",
    "## Hardware Requirements\n",
    "\n",
    "| Model | Full Fine-tuning | LoRA (r=16) | Speedup |\n",
    "|-------|-----------------|-------------|----------|\n",
    "| flan-t5-small (77M) | 8 GB | 3 GB | 2.7Ã— |\n",
    "| flan-t5-base (250M) | 16 GB | 6 GB | 2.7Ã— |\n",
    "| flan-t5-large (780M) | 32 GB | 12 GB | 2.7Ã— |\n",
    "\n",
    "**Apple Silicon Users:** LoRA enables training FLAN-T5-base on M1/M2 Macs! ðŸŽ‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google Colab Setup\n",
    "\n",
    "Mount Google Drive to save processed data and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# DRIVE_ROOT = \"/content/drive/MyDrive/socratic-path\"\n",
    "# os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "# print(f\"Google Drive mounted at: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q peft>=0.7.0 evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check GPU Availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Device Priority:**\n",
    "1. CUDA (NVIDIA GPUs) - Full support with fp16\n",
    "2. MPS (Apple Silicon M1/M2/M3) - GPU acceleration on Mac\n",
    "3. CPU - Fallback option (slowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    if gpu_props:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(\"Warning: Training on CPU will be significantly slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Model Selection\n",
    "\n",
    "Choose your model based on available hardware:\n",
    "\n",
    "- `flan-t5-small` (77M): Good for prototyping, runs on any hardware\n",
    "- `flan-t5-base` (250M): **Recommended for production**, best quality/speed trade-off\n",
    "- `flan-t5-large` (780M): Highest quality, requires 12+ GB VRAM with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "DRIVE_ROOT = \"..\"  # Change to DRIVE_ROOT if using Colab\n",
    "DATA_DIR = Path(DRIVE_ROOT) / \"datasets/processed\"\n",
    "MODEL_OUTPUT_DIR = Path(DRIVE_ROOT) / \"models/flan-t5-base-socratic\"\n",
    "LOGS_DIR = Path(DRIVE_ROOT) / \"logs\"\n",
    "\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# MODEL_NAME = \"google/flan-t5-small\"   # 77M params, 3 GB VRAM with LoRA\n",
    "MODEL_NAME = \"google/flan-t5-base\"  # 250M params, 6 GB VRAM with LoRA (RECOMMENDED)\n",
    "# MODEL_NAME = \"google/flan-t5-large\" # 780M params, 12 GB VRAM with LoRA\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LoRA Configuration\n",
    "\n",
    "**Key Parameters:**\n",
    "\n",
    "- `r` (rank): Controls adapter capacity. Higher = more parameters, better quality\n",
    "  - `r=8`: Fast, minimal memory (~150K params)\n",
    "  - `r=16`: **Balanced, recommended** (~300K params)\n",
    "  - `r=32`: High quality (~600K params)\n",
    "\n",
    "- `lora_alpha`: Scaling factor, typically 2Ã—r\n",
    "\n",
    "- `target_modules`: Which layers to adapt\n",
    "  - `[\"q\", \"v\"]`: Minimal, fastest\n",
    "  - `[\"q\", \"k\", \"v\", \"o\"]`: **Recommended for seq2seq**\n",
    "  - `[\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]`: Maximum capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA Configuration:\n",
      "  r: 16\n",
      "  lora_alpha: 32\n",
      "  target_modules: ['q', 'k', 'v', 'o']\n",
      "  lora_dropout: 0.1\n",
      "  bias: none\n"
     ]
    }
   ],
   "source": [
    "LORA_CONFIG = {\n",
    "    \"r\": 16,                                 # Rank: 8 (fast), 16 (balanced), 32 (quality)\n",
    "    \"lora_alpha\": 32,                        # Scaling factor (typically 2Ã—r)\n",
    "    \"target_modules\": [\"q\", \"k\", \"v\", \"o\"],  # Which attention layers to adapt\n",
    "    \"lora_dropout\": 0.1,                     # Regularization\n",
    "    \"bias\": \"none\",                          # Don't train bias terms\n",
    "    \"task_type\": TaskType.SEQ_2_SEQ_LM,      # Task type for PEFT\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "for k, v in LORA_CONFIG.items():\n",
    "    if k != \"task_type\":\n",
    "        print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "**Key Differences from Full Fine-tuning:**\n",
    "\n",
    "1. **Higher learning rate** (1e-4 vs 5e-5): LoRA needs stronger signal\n",
    "2. **Larger batch size** (8 vs 4): More memory available\n",
    "3. **More epochs** (10 vs 5): Faster per epoch, so we can train longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Configuration:\n",
      "  learning_rate: 0.0001\n",
      "  per_device_train_batch_size: 8\n",
      "  per_device_eval_batch_size: 8\n",
      "  gradient_accumulation_steps: 2\n",
      "  num_train_epochs: 5\n",
      "  warmup_steps: 500\n",
      "  weight_decay: 0.01\n",
      "  max_source_length: 400\n",
      "  max_target_length: 80\n",
      "  fp16: False\n",
      "  seed: 42\n",
      "\n",
      "Effective batch size: 16\n"
     ]
    }
   ],
   "source": [
    "TRAINING_CONFIG = {\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,\n",
    "    \"num_train_epochs\": 5,\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_source_length\": 400,\n",
    "    \"max_target_length\": 80,\n",
    "    \"fp16\": torch.cuda.is_available(),\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "effective_batch_size = TRAINING_CONFIG[\"per_device_train_batch_size\"] * TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
    "print(f\"\\nEffective batch size: {effective_batch_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Random Seeds for Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(TRAINING_CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Preprocessed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 84582\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(str(DATA_DIR / \"soqg_tokenized\"))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: ../datasets/processed/tokenizer\n",
      "Tokenizer vocabulary size: 32101\n",
      "[Question] token ID: 32100\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = str(DATA_DIR / \"tokenizer\")\n",
    "print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
    "\n",
    "if not Path(tokenizer_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Tokenizer not found at {tokenizer_path}. \"\n",
    "        \"Please run 02_preprocessing.ipynb first to generate the tokenizer.\"\n",
    "    )\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"[Question] token ID: {tokenizer.convert_tokens_to_ids('[Question]')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Base Model and Apply LoRA\n",
    "\n",
    "**This is the key difference from full fine-tuning!**\n",
    "\n",
    "Instead of training all parameters, we:\n",
    "1. Load the base model (frozen)\n",
    "2. Add LoRA adapter layers (trainable)\n",
    "3. Only train the adapter (~0.4% of parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/flan-t5-base\n",
      "Base model parameters: 247,536,384\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Base model parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 251,075,328 || trainable%: 1.4095\n",
      "\n",
      "Parameter reduction: 71Ã— fewer trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Enable input gradients for gradient checkpointing compatibility\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate reduction\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = model.num_parameters()\n",
    "reduction = total / trainable\n",
    "print(f\"\\nParameter reduction: {reduction:.0f}Ã— fewer trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Compute ROUGE metrics for evaluation.\"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [label.strip() for label in decoded_labels]\n",
    "\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run name: socratic-lora-r16-20251215-0946\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"socratic-lora-r{LORA_CONFIG['r']}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(MODEL_OUTPUT_DIR / \"checkpoints\"),\n",
    "    run_name=run_name,\n",
    "\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    lr_scheduler_type=\"linear\",\n",
    "\n",
    "    fp16=TRAINING_CONFIG[\"fp16\"],\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    logging_dir=str(LOGS_DIR / run_name),\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=TRAINING_CONFIG[\"max_target_length\"],\n",
    "\n",
    "    seed=TRAINING_CONFIG[\"seed\"],\n",
    "    dataloader_num_workers=0 if device == \"mps\" else 2,\n",
    "    dataloader_pin_memory=False if device == \"mps\" else True\n",
    ")\n",
    "\n",
    "print(f\"Run name: {run_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for Colab environment\n",
    "if device == \"cuda\":\n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 enabled for faster training on compatible GPUs\")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer initialized.\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3,\n",
    "    early_stopping_threshold=0.001\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Training Validation\n",
    "\n",
    "Run evaluation before training to establish a baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline evaluation (before fine-tuning)...\n",
      "\n",
      "Baseline Metrics:\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n"
     ]
    }
   ],
   "source": [
    "print(\"Running baseline evaluation (before fine-tuning)...\")\n",
    "baseline_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Metrics:\")\n",
    "for key, value in baseline_results.items():\n",
    "    if \"rouge\" in key:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA training...\n",
      "Training samples: 84582\n",
      "Validation samples: 10573\n",
      "LoRA config: r=16, Î±=32, targets=['q', 'k', 'v', 'o']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='26435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   93/26435 04:54 < 23:39:21, 0.31 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoRA config: r=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Î±=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mlora_alpha\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, targets=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mtarget_modules\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting LoRA training...\")\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"LoRA config: r={LORA_CONFIG['r']}, Î±={LORA_CONFIG['lora_alpha']}, targets={LORA_CONFIG['target_modules']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Complete!\n",
      "==================================================\n",
      "Total training time: 16450.4 seconds (274.2 minutes)\n",
      "Training samples/second: 25.71\n",
      "Final training loss: 3.0771\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.1f} seconds ({train_result.metrics['train_runtime']/60:.1f} minutes)\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1322' max='1322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1322/1322 14:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "  eval_loss: 1.6739\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n",
      "  eval_runtime: 925.0305\n",
      "  eval_samples_per_second: 11.4300\n",
      "  eval_steps_per_second: 1.4290\n",
      "  epoch: 1.5132\n"
     ]
    }
   ],
   "source": [
    "print(\"Running final evaluation...\")\n",
    "final_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for key, value in final_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improvement over Baseline:\n",
      "  rouge1: 0.2869 -> 0.2869 (+0.0000)\n",
      "  rouge2: 0.0728 -> 0.0728 (+0.0000)\n",
      "  rougeL: 0.2726 -> 0.2726 (+0.0000)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImprovement over Baseline:\")\n",
    "for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    baseline = baseline_results.get(f\"eval_{metric}\", 0)\n",
    "    final = final_results.get(f\"eval_{metric}\", 0)\n",
    "    improvement = final - baseline\n",
    "    print(f\"  {metric}: {baseline:.4f} -> {final:.4f} ({improvement:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save LoRA Adapter\n",
    "\n",
    "We save only the adapter weights (~1-2 MB), not the full model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ LoRA adapter saved to: ../models/flan-t5-socratic-lora/adapter\n",
      "  Adapter size: ~137.8 MB\n"
     ]
    }
   ],
   "source": [
    "adapter_path = MODEL_OUTPUT_DIR / \"adapter\"\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "print(f\"âœ“ LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "import os\n",
    "adapter_size = sum(os.path.getsize(adapter_path / f) for f in os.listdir(adapter_path) if os.path.isfile(adapter_path / f)) / 1e6\n",
    "print(f\"  Adapter size: ~{adapter_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge & Save Full Model (Optional)\n",
    "\n",
    "For deployment, you can merge the adapter back into the base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA weights into base model...\n",
      "âœ“ Merged model saved to: ../models/flan-t5-socratic-lora/merged\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA weights into base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_path = MODEL_OUTPUT_DIR / \"merged\"\n",
    "merged_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merged_model.save_pretrained(str(merged_path))\n",
    "tokenizer.save_pretrained(str(merged_path))\n",
    "\n",
    "print(f\"âœ“ Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ Training summary saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "training_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"run_name\": run_name,\n",
    "    \"lora_config\": {k: v for k, v in LORA_CONFIG.items() if k != \"task_type\"},\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    \"total_params\": model.num_parameters(),\n",
    "    \"trainable_percent\": sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters() * 100,\n",
    "    \"baseline_metrics\": {k: float(v) for k, v in baseline_results.items() if isinstance(v, (int, float))},\n",
    "    \"final_metrics\": {k: float(v) for k, v in final_results.items() if isinstance(v, (int, float))},\n",
    "    \"training_time_seconds\": train_result.metrics['train_runtime']\n",
    "}\n",
    "\n",
    "with open(adapter_path / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"âœ“ Training summary saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Generations:\n",
      "============================================================\n",
      "Context: I believe that climate change is not as serious as scientists claim because the weather has always c...\n",
      "Generated: [Question] What about climate change?\n",
      "------------------------------------------------------------\n",
      "Context: Social media is making teenagers more depressed and we should ban it for anyone under 18....\n",
      "Generated: [Question] What about social media that is making teenagers more depressed?\n",
      "------------------------------------------------------------\n",
      "Context: Artificial intelligence will eventually replace all human jobs and we need to prepare for universal ...\n",
      "Generated: [Question] What are you saying about artificial intelligence?\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "test_contexts = [\n",
    "    \"I believe that climate change is not as serious as scientists claim because the weather has always changed throughout history.\",\n",
    "    \"Social media is making teenagers more depressed and we should ban it for anyone under 18.\",\n",
    "    \"Artificial intelligence will eventually replace all human jobs and we need to prepare for universal basic income.\"\n",
    "]\n",
    "\n",
    "print(\"Sample Generations:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for context in test_contexts:\n",
    "    input_text = f\"Generate a Socratic question for this context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=400, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=80,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.6,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    print(f\"Context: {context[:100]}...\")\n",
    "    print(f\"Generated: {generated}\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Adapter for Inference\n",
    "\n",
    "Here's how to load and use the trained adapter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for PeftModelForSeq2SeqLM:\n\tsize mismatch for base_model.model.shared.weight: copying a param with shape torch.Size([32101, 512]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([32101, 512]) from checkpoint, the shape in current model is torch.Size([32128, 512]).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m base_model_inference = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load adapter\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model_inference = \u001b[43mPeftModel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_inference\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Load tokenizer\u001b[39;00m\n\u001b[32m      8\u001b[39m tokenizer_inference = T5Tokenizer.from_pretrained(\u001b[38;5;28mstr\u001b[39m(adapter_path))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/peft_model.py:568\u001b[39m, in \u001b[36mPeftModel.from_pretrained\u001b[39m\u001b[34m(cls, model, model_id, adapter_name, is_trainable, config, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    560\u001b[39m     model = MODEL_TYPE_TO_PEFT_MODEL_MAPPING[config.task_type](\n\u001b[32m    561\u001b[39m         model,\n\u001b[32m    562\u001b[39m         config,\n\u001b[32m   (...)\u001b[39m\u001b[32m    565\u001b[39m         low_cpu_mem_usage=low_cpu_mem_usage,\n\u001b[32m    566\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m568\u001b[39m load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_adapter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    570\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    571\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_trainable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    572\u001b[39m \u001b[43m    \u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mautocast_adapter_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    573\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    578\u001b[39m \u001b[38;5;66;03m# 1. Remove VB-LoRA vector bank, since it's a shared parameter set via the VBLoRAModel\u001b[39;00m\n\u001b[32m    579\u001b[39m \u001b[38;5;66;03m# 2. Remove the prompt encoder, as it does not need to be part of the checkpoint\u001b[39;00m\n\u001b[32m    580\u001b[39m missing_keys = [\n\u001b[32m    581\u001b[39m     k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m load_result.missing_keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvblora_vector_bank\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mprompt_encoder\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m k\n\u001b[32m    582\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/peft_model.py:1368\u001b[39m, in \u001b[36mPeftModel.load_adapter\u001b[39m\u001b[34m(self, model_id, adapter_name, is_trainable, torch_device, autocast_adapter_dtype, ephemeral_gpu_offload, low_cpu_mem_usage, key_mapping, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m \u001b[38;5;66;03m# load the weights into the model\u001b[39;00m\n\u001b[32m   1367\u001b[39m ignore_mismatched_sizes = kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mignore_mismatched_sizes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1368\u001b[39m load_result = \u001b[43mset_peft_model_state_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapters_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m tuner = \u001b[38;5;28mself\u001b[39m.peft_config[adapter_name].peft_type\n\u001b[32m   1377\u001b[39m tuner_prefix = PEFT_TYPE_TO_PREFIX_MAPPING.get(tuner, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:565\u001b[39m, in \u001b[36mset_peft_model_state_dict\u001b[39m\u001b[34m(model, peft_model_state_dict, adapter_name, ignore_mismatched_sizes, low_cpu_mem_usage)\u001b[39m\n\u001b[32m    563\u001b[39m             module._move_adapter_to_device_of_base_layer(adapter_name)\n\u001b[32m    564\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m565\u001b[39m     load_result = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpeft_model_state_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.is_prompt_learning:\n\u001b[32m    568\u001b[39m     model.prompt_encoder[adapter_name].embedding.load_state_dict(\n\u001b[32m    569\u001b[39m         {\u001b[33m\"\u001b[39m\u001b[33mweight\u001b[39m\u001b[33m\"\u001b[39m: peft_model_state_dict[\u001b[33m\"\u001b[39m\u001b[33mprompt_embeddings\u001b[39m\u001b[33m\"\u001b[39m]}, strict=\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    570\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/nn/modules/module.py:2629\u001b[39m, in \u001b[36mModule.load_state_dict\u001b[39m\u001b[34m(self, state_dict, strict, assign)\u001b[39m\n\u001b[32m   2621\u001b[39m         error_msgs.insert(\n\u001b[32m   2622\u001b[39m             \u001b[32m0\u001b[39m,\n\u001b[32m   2623\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2624\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m.join(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[32m   2625\u001b[39m             ),\n\u001b[32m   2626\u001b[39m         )\n\u001b[32m   2628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) > \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m2629\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   2630\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2631\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m.join(error_msgs)\n\u001b[32m   2632\u001b[39m         )\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[31mRuntimeError\u001b[39m: Error(s) in loading state_dict for PeftModelForSeq2SeqLM:\n\tsize mismatch for base_model.model.shared.weight: copying a param with shape torch.Size([32101, 512]) from checkpoint, the shape in current model is torch.Size([32128, 512]).\n\tsize mismatch for base_model.model.lm_head.weight: copying a param with shape torch.Size([32101, 512]) from checkpoint, the shape in current model is torch.Size([32128, 512])."
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "base_model_inference = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load adapter\n",
    "model_inference = PeftModel.from_pretrained(base_model_inference, str(adapter_path))\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer_inference = T5Tokenizer.from_pretrained(str(adapter_path))\n",
    "\n",
    "print(\"âœ“ Model loaded successfully!\")\n",
    "print(\"  Ready for inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard Instructions\n",
    "\n",
    "To view training logs, run in terminal:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ../logs\n",
    "```\n",
    "\n",
    "Then open http://localhost:6006 in your browser.\n",
    "\n",
    "---\n",
    "\n",
    "## Training Complete!\n",
    "\n",
    "**Outputs:**\n",
    "- LoRA adapter: `../models/flan-t5-socratic-lora/adapter/`\n",
    "- Merged model: `../models/flan-t5-socratic-lora/merged/`\n",
    "- Checkpoints: `../models/flan-t5-socratic-lora/checkpoints/`\n",
    "- Logs: `../logs/{run_name}/`\n",
    "\n",
    "**Key Advantages:**\n",
    "- âœ… Adapter size: ~1-2 MB (vs 300 MB for full model)\n",
    "- âœ… Training time: 20-50% faster\n",
    "- âœ… Memory usage: 60% less\n",
    "- âœ… Quality: 95-98% of full fine-tuning\n",
    "\n",
    "**Next Steps:**\n",
    "1. Compare metrics with full fine-tuning (03_training.ipynb)\n",
    "2. Proceed to `04_evaluation.ipynb` for comprehensive evaluation\n",
    "3. Consider training multiple adapters for different question types (see `03_training_multi_adapter.ipynb`)\n",
    "4. Scale to FLAN-T5-base for better quality (change `MODEL_NAME` above)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
