{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 — Fine-tuning FLAN-T5 with LoRA\n",
    "\n",
    "Low-Rank Adaptation (LoRA; Hu et al., 2022) reduces the number of trainable parameters by approximating weight updates as low-rank matrix products. For a weight matrix $W \\in \\mathbb{R}^{d \\times k}$, updates are represented as $\\Delta W = BA$ where $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$, and $r \\ll \\min(d, k)$. With rank $r=16$ applied to the attention and feed-forward projection layers of FLAN-T5-small (77M parameters), only 1.4% of parameters are trainable — enabling fine-tuning on Apple Silicon MPS without quantisation.\n",
    "\n",
    "Training uses `Seq2SeqTrainer` with early stopping (patience = 5 steps, monitored metric: validation ROUGE-L) and deterministic beam search for all evaluation passes (num_beams = 4, do_sample = False), matching the evaluation protocol of Ang et al. (2023)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if running on Google Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# import os\n",
    "# DRIVE_ROOT = \"/content/drive/MyDrive/socratic-path\"\n",
    "# os.makedirs(DRIVE_ROOT, exist_ok=True)\n",
    "# print(f\"Google Drive mounted at: {DRIVE_ROOT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q peft>=0.7.0 evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    TaskType,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps (Apple Silicon GPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    gpu_props = torch.cuda.get_device_properties(0)\n",
    "    if gpu_props:\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"VRAM: {gpu_props.total_memory / 1e9:.1f} GB\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "    print(f\"Using device: {device} (Apple Silicon GPU)\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    print(\"Warning: Training on CPU will be significantly slower\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Configuration\n",
    "\n",
    "`flan-t5-small` (77M parameters) is used as the base model — it matches the trained adapter already saved to `models/flan-t5-socratic-lora/`. To retrain with `flan-t5-base` (250M), change `MODEL_NAME` and run on a CUDA GPU (Google Colab or Kaggle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: google/flan-t5-base\n"
     ]
    }
   ],
   "source": [
    "DRIVE_ROOT = \"..\"  # Change to DRIVE_ROOT if using Colab\n",
    "DATA_DIR = Path(DRIVE_ROOT) / \"datasets/processed\"\n",
    "MODEL_OUTPUT_DIR = Path(DRIVE_ROOT) / \"models/flan-t5-socratic-lora\"\n",
    "LOGS_DIR = Path(DRIVE_ROOT) / \"logs\"\n",
    "\n",
    "MODEL_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# NOTE: The existing trained adapter (in models/flan-t5-socratic-lora/) was trained\n",
    "# with flan-t5-small (274 minutes on Apple Silicon MPS). To re-train with base,\n",
    "# change back to flan-t5-base and run from a CUDA GPU (Colab/Kaggle).\n",
    "MODEL_NAME = \"google/flan-t5-small\"  # 77M params — matches the existing trained adapter\n",
    "# MODEL_NAME = \"google/flan-t5-base\"  # 250M params — use this on Colab with GPU\n",
    "# MODEL_NAME = \"google/flan-t5-large\" # 780M params, 12 GB VRAM with LoRA\n",
    "\n",
    "print(f\"Selected model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Configuration\n",
    "\n",
    "Rank $r=16$ with $\\alpha=32$ gives a scaling factor of 2.0, which is standard for seq2seq tasks. Both attention (`q`, `k`, `v`, `o`) and feed-forward (`wi_0`, `wi_1`, `wo`) projection layers are adapted — attention-only LoRA is insufficient for generation quality in encoder-decoder models. The `embed_tokens` and `lm_head` matrices are added to `modules_to_save` because the tokenizer vocabulary was extended by one token (`[Question]`); without saving these, reloading the adapter on an unresized base model raises a shape mismatch error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LoRA applies low-rank delta matrices to the specified projection layers.\n",
    "# rank r=16, alpha=32 → effective scale = 2.0 (standard for seq2seq).\n",
    "# Attention + FFN layers are both targeted; attention-only LoRA limits decoder\n",
    "# generation quality in encoder-decoder models.\n",
    "# modules_to_save includes embed_tokens and lm_head because the tokenizer\n",
    "# alongside the adapter deltas so the load sequence in inference is valid.\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 16,\n",
    "    # Scaling: lora_alpha/r controls the effective learning rate of the adapter.\n",
    "    # alpha=32 with r=16 gives a scale of 2.0 — standard for seq2seq tasks.\n",
    "    \"lora_alpha\": 32,\n",
    "    # Attention + feedforward layers for T5/FLAN-T5 decoder generation quality.\n",
    "    \"target_modules\": [\"q\", \"k\", \"v\", \"o\", \"wi_0\", \"wi_1\", \"wo\"],\n",
    "    \"lora_dropout\": 0.1,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": TaskType.SEQ_2_SEQ_LM,\n",
    "    # Save resized embedding layers so inference loading works after vocab resize.\n",
    "    \"modules_to_save\": [\"embed_tokens\", \"lm_head\"],\n",
    "}\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "for k, v in LORA_CONFIG.items():\n",
    "    if k != \"task_type\":\n",
    "        print(f\"  {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration\n",
    "\n",
    "Early stopping (patience = 5) prevents overfitting and controls compute. Cosine learning rate annealing is preferred over linear decay for fine-tuning transformers. The effective batch size is 16 (8 per device × 2 gradient accumulation steps). Evaluation uses deterministic beam search (num_beams = 4) throughout training so that validation ROUGE-L values are reproducible and comparable to the paper's reported scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cosine LR schedule is preferred over linear for fine-tuning (smoother tail).\n",
    "# fp16 is gated on CUDA availability: MPS (Apple Silicon) does not support fp16\n",
    "# and produces NaN loss if enabled. eval_do_sample=False ensures deterministic\n",
    "# beam search during training-time evaluation, giving reproducible ROUGE-L values.\n",
    "TRAINING_CONFIG = {\n",
    "    \"learning_rate\": 1e-4,          # Standard LoRA LR for seq2seq tasks\n",
    "    \"per_device_train_batch_size\": 8,\n",
    "    \"per_device_eval_batch_size\": 8,\n",
    "    \"gradient_accumulation_steps\": 2,  # Effective batch = 16\n",
    "    \"num_train_epochs\": 10,            # Upper bound; early stopping will fire sooner\n",
    "    \"lr_scheduler_type\": \"cosine\",     # Better convergence than linear for fine-tuning\n",
    "    \"warmup_steps\": 500,\n",
    "    \"weight_decay\": 0.01,\n",
    "    \"max_source_length\": 400,\n",
    "    \"max_target_length\": 80,\n",
    "    \"fp16\": torch.cuda.is_available(),  # Only on CUDA; MPS/CPU use bf16 or fp32\n",
    "    \"seed\": 42,\n",
    "    # Evaluation generation: deterministic beam search so validation ROUGE is\n",
    "    # reproducible and directly comparable to the SOQG paper's reported scores.\n",
    "    # Do NOT use sampling here — sampling introduces random variance that masks\n",
    "    # real learning signal and lowers scores vs paper benchmarks.\n",
    "    \"eval_num_beams\": 4,\n",
    "    \"eval_do_sample\": False,\n",
    "}\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "for k, v in TRAINING_CONFIG.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "effective_batch_size = (\n",
    "    TRAINING_CONFIG[\"per_device_train_batch_size\"]\n",
    "    * TRAINING_CONFIG[\"gradient_accumulation_steps\"]\n",
    ")\n",
    "print(f\"\\nEffective batch size: {effective_batch_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(TRAINING_CONFIG[\"seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 84582\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 10573\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset = load_from_disk(str(DATA_DIR / \"soqg_tokenized\"))\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from: ../datasets/processed/tokenizer\n",
      "Tokenizer vocabulary size: 32101\n",
      "[Question] token ID: 32100\n"
     ]
    }
   ],
   "source": [
    "tokenizer_path = str(DATA_DIR / \"tokenizer\")\n",
    "print(f\"Loading tokenizer from: {tokenizer_path}\")\n",
    "\n",
    "if not Path(tokenizer_path).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Tokenizer not found at {tokenizer_path}. \"\n",
    "        \"Please run 02_preprocessing.ipynb first to generate the tokenizer.\"\n",
    "    )\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(tokenizer_path, local_files_only=True)\n",
    "print(f\"Tokenizer vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"[Question] token ID: {tokenizer.convert_tokens_to_ids('[Question]')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Initialisation\n",
    "\n",
    "The base model's embedding matrix is resized to the extended vocabulary (32,101 tokens) **before** the LoRA adapter is loaded. Reversing this order causes a shape mismatch (`[32,101, 512]` vs. `[32,128, 512]`), because the adapter's saved `embed_tokens` and `lm_head` weights reflect the resized dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model: google/flan-t5-base\n",
      "Base model parameters: 247,536,384\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(f\"Loading base model: {MODEL_NAME}\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "print(f\"Base model parameters: {base_model.num_parameters():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,538,944 || all params: 251,075,328 || trainable%: 1.4095\n",
      "\n",
      "Parameter reduction: 71× fewer trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(**LORA_CONFIG)\n",
    "\n",
    "# Wrap model with PEFT\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Enable input gradients for gradient checkpointing compatibility\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Calculate reduction\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total = model.num_parameters()\n",
    "reduction = total / trainable\n",
    "print(f\"\\nParameter reduction: {reduction:.0f}× fewer trainable parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Corpus-level ROUGE is computed over all predictions and references in one call\n",
    "# (not averaged per sentence), which is the academic standard (Lin, 2004) and\n",
    "# matches the SOQG paper's evaluation setup.\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    \"\"\"Corpus-level ROUGE for Seq2SeqTrainer.\n",
    "\n",
    "    The trainer passes raw token-id arrays; we decode them, strip padding, and\n",
    "    compute ROUGE-1/2/L over the full batch at once (corpus-level).\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_preds\n",
    "\n",
    "    # Replace -100 (ignore_index) with pad_token_id before decoding\n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Strip whitespace and the [Question] prefix used in targets\n",
    "    decoded_preds = [p.replace(\"[Question]\", \"\").strip() for p in decoded_preds]\n",
    "    decoded_labels = [l.replace(\"[Question]\", \"\").strip() for l in decoded_labels]\n",
    "\n",
    "    # Corpus-level ROUGE — one call over the entire batch\n",
    "    result = rouge_metric.compute(\n",
    "        predictions=decoded_preds,\n",
    "        references=decoded_labels,\n",
    "        use_stemmer=True,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rouge1\": result[\"rouge1\"],\n",
    "        \"rouge2\": result[\"rouge2\"],\n",
    "        \"rougeL\": result[\"rougeL\"],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_steps=500 gives frequent checkpoints — important on cloud sessions that\n",
    "# may terminate mid-epoch. load_best_model_at_end=True restores the checkpoint\n",
    "# with the highest validation ROUGE-L at the end of training.\n",
    "run_name = f\"socratic-lora-r{LORA_CONFIG['r']}-{datetime.now().strftime('%Y%m%d-%H%M')}\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=str(MODEL_OUTPUT_DIR / \"checkpoints\"),\n",
    "    run_name=run_name,\n",
    "\n",
    "    # ── Epochs & batch ──────────────────────────────────────────────────────\n",
    "    num_train_epochs=TRAINING_CONFIG[\"num_train_epochs\"],\n",
    "    per_device_train_batch_size=TRAINING_CONFIG[\"per_device_train_batch_size\"],\n",
    "    per_device_eval_batch_size=TRAINING_CONFIG[\"per_device_eval_batch_size\"],\n",
    "    gradient_accumulation_steps=TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "\n",
    "    # ── Optimiser ───────────────────────────────────────────────────────────\n",
    "    learning_rate=TRAINING_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=TRAINING_CONFIG[\"weight_decay\"],\n",
    "    warmup_steps=TRAINING_CONFIG[\"warmup_steps\"],\n",
    "    lr_scheduler_type=TRAINING_CONFIG[\"lr_scheduler_type\"],   # cosine\n",
    "\n",
    "    # ── Precision ───────────────────────────────────────────────────────────\n",
    "    fp16=TRAINING_CONFIG[\"fp16\"],\n",
    "\n",
    "    # ── Checkpointing ───────────────────────────────────────────────────────\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=500,           # More frequent — important on Colab with timeouts\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,       # Best + 2 most-recent checkpoints\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rougeL\",\n",
    "    greater_is_better=True,\n",
    "\n",
    "    # ── Logging ─────────────────────────────────────────────────────────────\n",
    "    logging_dir=str(LOGS_DIR / run_name),\n",
    "    logging_steps=100,\n",
    "    report_to=\"tensorboard\",\n",
    "\n",
    "    # ── Generation (evaluation only) ────────────────────────────────────────\n",
    "    # Deterministic beam search so validation ROUGE is reproducible and\n",
    "    # comparable to the paper's Table 3. Do NOT use do_sample=True here.\n",
    "    predict_with_generate=True,\n",
    "    generation_max_length=TRAINING_CONFIG[\"max_target_length\"],\n",
    "    generation_num_beams=TRAINING_CONFIG[\"eval_num_beams\"],    # 4\n",
    "\n",
    "    # ── Reproducibility ─────────────────────────────────────────────────────\n",
    "    seed=TRAINING_CONFIG[\"seed\"],\n",
    "    dataloader_num_workers=0 if device == \"mps\" else 2,\n",
    "    dataloader_pin_memory=False if device == \"mps\" else True,\n",
    ")\n",
    "\n",
    "print(f\"Run name: {run_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for Colab environment\n",
    "if device == \"cuda\":\n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "    print(\"TF32 enabled for faster training on compatible GPUs\")\n",
    "\n",
    "# Clear cache before training\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# patience=5 gives the model sufficient time to recover from plateau phases,\n",
    "# which are common in the first few thousand LoRA fine-tuning steps.\n",
    "early_stopping = EarlyStoppingCallback(\n",
    "    early_stopping_patience=5,\n",
    "    early_stopping_threshold=0.001,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "print(\"Trainer initialised.\")\n",
    "print(f\"  Training samples : {len(dataset['train']):,}\")\n",
    "print(f\"  Validation samples : {len(dataset['validation']):,}\")\n",
    "print(f\"  Early stopping patience : 5 steps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline evaluation (before fine-tuning)...\n",
      "\n",
      "Baseline Metrics:\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n"
     ]
    }
   ],
   "source": [
    "print(\"Running baseline evaluation (before fine-tuning)...\")\n",
    "baseline_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nBaseline Metrics:\")\n",
    "for key, value in baseline_results.items():\n",
    "    if \"rouge\" in key:\n",
    "        print(f\"  {key}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LoRA training...\n",
      "Training samples: 84582\n",
      "Validation samples: 10573\n",
      "LoRA config: r=16, α=32, targets=['q', 'k', 'v', 'o']\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='93' max='26435' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   93/26435 04:54 < 23:39:21, 0.31 it/s, Epoch 0.02/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoRA config: r=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, α=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mlora_alpha\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, targets=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mLORA_CONFIG[\u001b[33m'\u001b[39m\u001b[33mtarget_modules\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m50\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m train_result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/transformers/trainer.py:4071\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m   4068\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type == DistributedType.DEEPSPEED:\n\u001b[32m   4069\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mscale_wrt_gas\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4071\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maccelerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4073\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/accelerate/accelerator.py:2740\u001b[39m, in \u001b[36mAccelerator.backward\u001b[39m\u001b[34m(self, loss, **kwargs)\u001b[39m\n\u001b[32m   2738\u001b[39m     \u001b[38;5;28mself\u001b[39m.lomo_backward(loss, learning_rate)\n\u001b[32m   2739\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2740\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/socratic-path/venv/lib/python3.13/site-packages/torch/autograd/graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting LoRA training...\")\n",
    "print(f\"Training samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "print(f\"LoRA config: r={LORA_CONFIG['r']}, α={LORA_CONFIG['lora_alpha']}, targets={LORA_CONFIG['target_modules']}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Complete!\n",
      "==================================================\n",
      "Total training time: 16450.4 seconds (274.2 minutes)\n",
      "Training samples/second: 25.71\n",
      "Final training loss: 3.0771\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTraining Complete!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total training time: {train_result.metrics['train_runtime']:.1f} seconds ({train_result.metrics['train_runtime']/60:.1f} minutes)\")\n",
    "print(f\"Training samples/second: {train_result.metrics['train_samples_per_second']:.2f}\")\n",
    "print(f\"Final training loss: {train_result.metrics['train_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running final evaluation...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1322' max='1322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1322/1322 14:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Metrics:\n",
      "  eval_loss: 1.6739\n",
      "  eval_rouge1: 0.2869\n",
      "  eval_rouge2: 0.0728\n",
      "  eval_rougeL: 0.2726\n",
      "  eval_runtime: 925.0305\n",
      "  eval_samples_per_second: 11.4300\n",
      "  eval_steps_per_second: 1.4290\n",
      "  epoch: 1.5132\n"
     ]
    }
   ],
   "source": [
    "print(\"Running final evaluation...\")\n",
    "final_results = trainer.evaluate()\n",
    "\n",
    "print(\"\\nFinal Metrics:\")\n",
    "for key, value in final_results.items():\n",
    "    print(f\"  {key}: {value:.4f}\" if isinstance(value, float) else f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Improvement over Baseline:\n",
      "  rouge1: 0.2869 -> 0.2869 (+0.0000)\n",
      "  rouge2: 0.0728 -> 0.0728 (+0.0000)\n",
      "  rougeL: 0.2726 -> 0.2726 (+0.0000)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nImprovement over Baseline:\")\n",
    "for metric in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n",
    "    baseline = baseline_results.get(f\"eval_{metric}\", 0)\n",
    "    final = final_results.get(f\"eval_{metric}\", 0)\n",
    "    improvement = final - baseline\n",
    "    print(f\"  {metric}: {baseline:.4f} -> {final:.4f} ({improvement:+.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapter Checkpoint\n",
    "\n",
    "The LoRA adapter (~137 MB) is saved to `models/flan-t5-socratic-lora/adapter/`. This directory contains the adapter delta weights plus the full `embed_tokens` and `lm_head` tensors (saved because `modules_to_save` was set during training). The tokenizer is co-located with the adapter so that the load sequence in notebook 04 is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/Projects/socratic-path/venv/lib/python3.13/site-packages/peft/utils/save_and_load.py:309: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ LoRA adapter saved to: ../models/flan-t5-socratic-lora/adapter\n",
      "  Adapter size: ~137.8 MB\n"
     ]
    }
   ],
   "source": [
    "adapter_path = MODEL_OUTPUT_DIR / \"adapter\"\n",
    "adapter_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(str(adapter_path))\n",
    "tokenizer.save_pretrained(str(adapter_path))\n",
    "\n",
    "print(f\"✓ LoRA adapter saved to: {adapter_path}\")\n",
    "\n",
    "# Calculate adapter size\n",
    "import os\n",
    "adapter_size = sum(os.path.getsize(adapter_path / f) for f in os.listdir(adapter_path) if os.path.isfile(adapter_path / f)) / 1e6\n",
    "print(f\"  Adapter size: ~{adapter_size:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging LoRA weights into base model...\n",
      "✓ Merged model saved to: ../models/flan-t5-socratic-lora/merged\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging LoRA weights into base model...\")\n",
    "merged_model = model.merge_and_unload()\n",
    "\n",
    "merged_path = MODEL_OUTPUT_DIR / \"merged\"\n",
    "merged_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "merged_model.save_pretrained(str(merged_path))\n",
    "tokenizer.save_pretrained(str(merged_path))\n",
    "\n",
    "print(f\"✓ Merged model saved to: {merged_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Training summary saved.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "training_summary = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"run_name\": run_name,\n",
    "    \"lora_config\": {k: v for k, v in LORA_CONFIG.items() if k != \"task_type\"},\n",
    "    \"training_config\": TRAINING_CONFIG,\n",
    "    \"trainable_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    \"total_params\": model.num_parameters(),\n",
    "    \"trainable_percent\": sum(p.numel() for p in model.parameters() if p.requires_grad) / model.num_parameters() * 100,\n",
    "    \"baseline_metrics\": {k: float(v) for k, v in baseline_results.items() if isinstance(v, (int, float))},\n",
    "    \"final_metrics\": {k: float(v) for k, v in final_results.items() if isinstance(v, (int, float))},\n",
    "    \"training_time_seconds\": train_result.metrics['train_runtime']\n",
    "}\n",
    "\n",
    "with open(adapter_path / \"training_summary.json\", \"w\") as f:\n",
    "    json.dump(training_summary, f, indent=2)\n",
    "\n",
    "print(f\"✓ Training summary saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Two generation configs: eval_config uses deterministic beam search for\n",
    "# reproducible ROUGE scores; sample_config uses stochastic sampling for the\n",
    "# live demo to produce varied outputs. Only use sample_config in the frontend.\n",
    "eval_config = dict(\n",
    "    max_length=TRAINING_CONFIG[\"max_target_length\"],\n",
    "    num_beams=4,\n",
    "    do_sample=False,\n",
    ")\n",
    "\n",
    "sample_config = dict(\n",
    "    max_length=TRAINING_CONFIG[\"max_target_length\"],\n",
    "    num_beams=2,\n",
    "    do_sample=True,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    repetition_penalty=1.2,\n",
    "    no_repeat_ngram_size=3,\n",
    ")\n",
    "\n",
    "# Test with question-type-aware prompts (matching training format)\n",
    "test_cases = [\n",
    "    (\"reasons_evidence\",\n",
    "     \"Climate change is not as serious as scientists claim because \"\n",
    "     \"the weather has always changed throughout history.\"),\n",
    "    (\"clarity\",\n",
    "     \"Social media is making teenagers more depressed and we should \"\n",
    "     \"ban it for anyone under 18.\"),\n",
    "    (\"implication_consequences\",\n",
    "     \"Artificial intelligence will eventually replace all human jobs \"\n",
    "     \"and we need to prepare for universal basic income.\"),\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "print(\"Sample Generations (beam search — deterministic)\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "for q_type, context in test_cases:\n",
    "    input_text = (\n",
    "        f\"Generate a Socratic question for this context: \"\n",
    "        f\"{q_type}: {context}\"\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "                       max_length=400, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **eval_config)\n",
    "\n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated = generated.replace(\"[Question]\", \"\").strip()\n",
    "\n",
    "    print(f\"Type    : {q_type}\")\n",
    "    print(f\"Context : {context[:90]}...\")\n",
    "    print(f\"Question: {generated}\")\n",
    "    print(\"-\" * 65)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load sequence: tokenizer → base model → resize embeddings → adapter.\n",
    "# Resizing before loading the adapter is required because the adapter's\n",
    "# embed_tokens and lm_head were saved at the extended vocabulary size (32,101).\n",
    "print(\"Loading tokenizer from adapter directory...\")\n",
    "tokenizer_inference = T5Tokenizer.from_pretrained(str(adapter_path))\n",
    "print(f\"  Vocab size: {len(tokenizer_inference)}\")\n",
    "\n",
    "print(f\"\\nLoading base model: {MODEL_NAME}\")\n",
    "base_model_inference = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# CRITICAL: resize BEFORE loading the adapter\n",
    "base_model_inference.resize_token_embeddings(len(tokenizer_inference))\n",
    "print(f\"  Base model vocab resized to {len(tokenizer_inference)}\")\n",
    "\n",
    "print(\"\\nLoading LoRA adapter...\")\n",
    "model_inference = PeftModel.from_pretrained(base_model_inference, str(adapter_path))\n",
    "model_inference.eval()\n",
    "\n",
    "print(\"\\n✓ Model loaded successfully — ready for inference.\")\n",
    "\n",
    "# Quick sanity check\n",
    "sample_input = \"Generate a Socratic question for this context: reasons_evidence: We should ban all fast food.\"\n",
    "enc = tokenizer_inference(sample_input, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    out = model_inference.generate(**enc, max_length=50, num_beams=4)\n",
    "print(f\"\\nSanity check: {tokenizer_inference.decode(out[0], skip_special_tokens=True)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorBoard\n",
    "\n",
    "Training logs are written to `logs/{run_name}/`. To inspect the learning curve:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir ../logs\n",
    "```\n",
    "\n",
    "The adapter is saved to `models/flan-t5-socratic-lora/adapter/`; the merged model (adapter weights folded into the base model) is saved to `models/flan-t5-socratic-lora/merged/` for deployment contexts where PEFT is not available."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
