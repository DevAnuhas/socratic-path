{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 04 \u2014 Evaluation\n\nThe trained adapter is evaluated on the SocratiQ held-out test set (10,573 examples) using three complementary automatic metrics: ROUGE-1/2/L (Lin, 2004) for n-gram overlap; BLEU-4 via sacrebleu (Post, 2018) for n-gram precision; and BERTScore F1 (Zhang et al., 2020) for contextual semantic similarity. All metrics are computed at corpus level with deterministic beam search (num_beams = 4, do_sample = False) to ensure reproducibility and direct comparability with Ang et al. (2023, Table 3). A per-question-type ROUGE breakdown is also reported \u2014 an analysis not presented in the original paper.",
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# MODEL_PATH points to the LoRA adapter produced by 03_training.\n# MODEL_NAME must match the base model used during training (flan-t5-small).\nfrom pathlib import Path\nimport torch\n\nMODEL_PATH = Path(\"../models/flan-t5-socratic-lora/adapter\")\nDATA_PATH  = Path(\"../datasets/processed\")\nOUTPUT_PATH = Path(\"../evaluation_results\")\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\n# Must match the base model used during training (flan-t5-small was trained, not base)\nMODEL_NAME = \"google/flan-t5-small\"\nprint(f\"Using device: {DEVICE}\")\n\n# \u2500\u2500 Generation config \u2014 EVALUATION (deterministic) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# Deterministic beam search is used for evaluation to ensure reproducible\n# ROUGE scores comparable to Ang et al. (2023, Table 3).\nEVAL_GENERATION_CONFIG = {\n    \"max_length\": 80,\n    \"num_beams\": 4,\n    \"do_sample\": False,    # deterministic \u2014 required for reproducible ROUGE\n    \"early_stopping\": True,\n}\n\n# \u2500\u2500 Generation config \u2014 DEMO / FRONTEND (diverse outputs) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDEMO_GENERATION_CONFIG = {\n    \"max_length\": 80,\n    \"num_beams\": 2,\n    \"do_sample\": True,\n    \"temperature\": 0.8,\n    \"top_p\": 0.9,\n    \"repetition_penalty\": 1.2,\n    \"no_repeat_ngram_size\": 3,\n}\n\nprint(\"Eval generation config (deterministic):\", EVAL_GENERATION_CONFIG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# Load sequence: tokenizer \u2192 base model \u2192 resize \u2192 adapter.\n# The base model must be resized to the extended vocabulary (32,101 tokens)\n# before the adapter is loaded to avoid a shape mismatch error.\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\n\nMODEL_NAME = \"google/flan-t5-small\"   # must match what was used in training\n\nprint(\"Step 1: Loading tokenizer from adapter directory...\")\ntokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\nprint(f\"  Vocabulary size: {len(tokenizer)}\")\n\nprint(f\"\\nStep 2: Loading base model ({MODEL_NAME})...\")\nbase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\nprint(f\"\\nStep 3: Resizing base model embeddings to {len(tokenizer)}...\")\nbase_model.resize_token_embeddings(len(tokenizer))\n\nprint(\"\\nStep 4: Loading LoRA adapter on top of resized base model...\")\nmodel = PeftModel.from_pretrained(base_model, str(MODEL_PATH))\nmodel = model.to(DEVICE)\nmodel.eval()\n\nprint(f\"\\n\u2713 Model loaded: {model.num_parameters():,} total parameters\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_formatted = pd.read_parquet(DATA_PATH / \"test_formatted.parquet\")\n",
    "print(f\"Test samples: {len(test_formatted)}\")\n",
    "test_formatted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# \u2500\u2500 Generate Predictions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The input prompt at evaluation must match the training format exactly:\n#   \"Generate a Socratic question for this context: {question_type}: {context}\"\n#\n# test_formatted already has input_text in this format (set in 02_preprocessing).\n# We use EVAL_GENERATION_CONFIG (beam search, no sampling) throughout.\n\ndef generate_question(input_text: str) -> str:\n    \"\"\"Generate a Socratic question; strip the [Question] prefix from output.\"\"\"\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=400,\n        truncation=True,\n    )\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **EVAL_GENERATION_CONFIG)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated.replace(\"[Question]\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\nreferences = []\n\nprint(\"Generating predictions...\")\nfor idx, row in tqdm(test_formatted.iterrows(), total=len(test_formatted)):\n    pred = generate_question(row['original_input'])\n    predictions.append(pred)\n    ref = row['original_target'] if 'original_target' in row else row['target_text'].replace(\"[Question] \", \"\")\n    references.append(ref)\n\ntest_formatted['prediction'] = predictions\ntest_formatted['reference'] = references\nprint(f\"Generated {len(predictions)} predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# ROUGE is computed corpus-level (one call over all predictions), not as a\n# per-sentence average. This is the academic standard (Lin, 2004) and matches\n# the evaluation setup in Ang et al. (2023, Table 3).\nimport evaluate as hf_evaluate\n\nrouge_metric = hf_evaluate.load(\"rouge\")\n\n# Clean reference strings (strip [Question] prefix added during preprocessing)\nclean_preds = [p.replace(\"[Question]\", \"\").strip() for p in predictions]\nclean_refs  = [r.replace(\"[Question]\", \"\").strip() for r in references]\n\n# Single corpus-level call\nrouge_results = rouge_metric.compute(\n    predictions=clean_preds,\n    references=clean_refs,\n    use_stemmer=True,\n)\n\nprint(\"Corpus-level ROUGE Scores\")\nprint(\"=\" * 40)\nfor k in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n    print(f\"  {k.upper():<10}: {rouge_results[k]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# sacrebleu is used for BLEU-4 (corpus-level, tokenisation-consistent).\n# The score is normalised from 0\u2013100 to 0\u20131 for consistency with ROUGE.\nbleu_metric = hf_evaluate.load(\"sacrebleu\")\n\nbleu_result = bleu_metric.compute(\n    predictions=clean_preds,\n    references=[[ref] for ref in clean_refs],   # sacrebleu expects list-of-lists\n)\n\nbleu4 = bleu_result[\"score\"] / 100.0   # normalise 0-100 \u2192 0-1\n\nprint(f\"BLEU-4 (sacrebleu, normalised): {bleu4:.4f}\")\nprint(f\"BLEU-4 (sacrebleu, raw 0-100) : {bleu_result['score']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERTScore (Zhang et al., 2020) computes token-level similarity using contextual embeddings from a pre-trained language model, making it more sensitive to semantic equivalence than n-gram metrics. For Socratic question generation \u2014 where many valid paraphrases exist \u2014 BERTScore F1 is a more meaningful signal than BLEU or ROUGE alone."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing BERTScore (this may take a few minutes)...\")\n",
    "\n",
    "P, R, F1 = bert_score(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang=\"en\",\n",
    "    verbose=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "bertscore_results = {\n",
    "    'Precision': P.mean().item(),\n",
    "    'Recall': R.mean().item(),\n",
    "    'F1': F1.mean().item()\n",
    "}\n",
    "\n",
    "print(\"\\nBERTScore Results:\")\n",
    "for metric, score in bertscore_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# Paper reference scores are from Ang et al. (2023, Table 3), evaluated on\n# the SocratiQ test set with beam search. Verify these values against your\n# copy of the paper before citing them in the dissertation.\n#\n# NOTE: The paper's T5-base and FLAN-T5-base numbers reflect full fine-tuning\n# (100% of parameters). This implementation uses LoRA (~1.4% of parameters),\n# so a moderate gap is expected and is itself a contribution.\npaper_models = [\n    (\"GPT-3 (zero-shot, paper)\",   0.2100, 0.0420, 0.1980, 0.0120),\n    (\"T5-base (paper, full FT)\",   0.3876, 0.1712, 0.3657, 0.0721),\n    (\"T5-large (paper, full FT)\",  0.4051, 0.1832, 0.3818, 0.0798),\n    (\"FLAN-T5-base (paper, full FT)\", 0.4143, 0.1897, 0.3901, 0.0831),\n]\n\nour_scores = (\n    \"FLAN-T5-small + LoRA (ours)\",\n    rouge_results[\"rouge1\"],\n    rouge_results[\"rouge2\"],\n    rouge_results[\"rougeL\"],\n    bleu4,\n)\n\nprint(\"=\" * 80)\nprint(f\"{'Model':<40} {'ROUGE-1':>8} {'ROUGE-2':>8} {'ROUGE-L':>8} {'BLEU-4':>8}\")\nprint(\"=\" * 80)\nfor name, r1, r2, rl, b4 in paper_models:\n    print(f\"{name:<40} {r1:>8.4f} {r2:>8.4f} {rl:>8.4f} {b4:>8.4f}\")\nprint(\"-\" * 80)\n# Highlight our model\nname, r1, r2, rl, b4 = our_scores\nprint(f\"{name:<40} {r1:>8.4f} {r2:>8.4f} {rl:>8.4f} {b4:>8.4f}  \u2190 SocraticPath\")\nprint(\"=\" * 80)\n\n# Gap analysis\nflan_r1, flan_r2, flan_rl = 0.4143, 0.1897, 0.3901\nprint(f\"\\nGap vs FLAN-T5-base (paper):\")\nprint(f\"  ROUGE-1: {rouge_results['rouge1'] - flan_r1:+.4f}\")\nprint(f\"  ROUGE-2: {rouge_results['rouge2'] - flan_r2:+.4f}\")\nprint(f\"  ROUGE-L: {rouge_results['rougeL'] - flan_rl:+.4f}\")\nprint(\"\\nNote: LoRA uses ~1.4% of parameters vs 100% for full fine-tuning.\")\nprint(\"A small gap is expected and is itself a contribution (parameter efficiency).\")\n"
  },
  {
   "cell_type": "code",
   "source": "\n# \u2500\u2500 Per-Question-Type ROUGE Breakdown \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# SocratiQ has 5 question types.  Breaking ROUGE down by type shows which\n# categories the model handles well and which need improvement \u2014 a stronger\n# analysis than aggregate scores alone, and a clear dissertation finding.\n#\n# This breakdown is novel \u2014 the SOQG paper only reports aggregate scores.\n\nimport pandas as pd\n\ndef extract_question_type(input_text: str) -> str:\n    \"\"\"Extract question type prefix from dataset input format.\"\"\"\n    # Format: \"Generate a Socratic question for this context: {type}: {text}\"\n    # or just the raw column: \"{type}: {text}\"\n    parts = input_text.split(\":\")\n    if len(parts) >= 3:\n        return parts[1].strip()   # type sits between 1st and 2nd colon\n    elif len(parts) == 2:\n        return parts[0].strip()\n    return \"unknown\"\n\ntest_formatted[\"question_type\"] = test_formatted[\"input_text\"].apply(\n    extract_question_type\n)\n\nprint(\"Per-Question-Type ROUGE-L Breakdown\")\nprint(\"=\" * 65)\nprint(f\"{'Question Type':<35} {'Count':>6} {'ROUGE-1':>8} {'ROUGE-2':>8} {'ROUGE-L':>8}\")\nprint(\"=\" * 65)\n\ntype_results = {}\nfor q_type, group in test_formatted.groupby(\"question_type\"):\n    type_preds = [\n        p.replace(\"[Question]\", \"\").strip()\n        for p in group[\"prediction\"].tolist()\n    ]\n    type_refs = [\n        r.replace(\"[Question]\", \"\").strip()\n        for r in group[\"reference\"].tolist()\n    ]\n    if not type_preds:\n        continue\n    res = rouge_metric.compute(\n        predictions=type_preds,\n        references=type_refs,\n        use_stemmer=True,\n    )\n    type_results[q_type] = res\n    print(\n        f\"{q_type:<35} {len(group):>6} \"\n        f\"{res['rouge1']:>8.4f} {res['rouge2']:>8.4f} {res['rougeL']:>8.4f}\"\n    )\n\nprint(\"=\" * 65)\nprint(\n    f\"{'Overall (corpus)':<35} {len(test_formatted):>6} \"\n    f\"{rouge_results['rouge1']:>8.4f} {rouge_results['rouge2']:>8.4f} \"\n    f\"{rouge_results['rougeL']:>8.4f}\"\n)\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2500\u2500 Per-Sample Score Distributions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The corpus-level rouge_results dict (computed in cell 12) gives a single\n# aggregate value.  For distribution plots we compute per-sample ROUGE-L using\n# rouge_score directly.  This is intentionally separate from the corpus-level\n# computation so the two methods remain independent.\n\nfrom rouge_score import rouge_scorer as rs_lib\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\nscorer = rs_lib.RougeScorer([\"rougeL\"], use_stemmer=True)\nsf = SmoothingFunction().method1\n\nper_sample_rougeL = []\nper_sample_bleu = []\n\nfor pred, ref in zip(clean_preds, clean_refs):\n    score = scorer.score(ref, pred)\n    per_sample_rougeL.append(score[\"rougeL\"].fmeasure)\n    pred_toks = pred.split()\n    ref_toks = ref.split()\n    b = sentence_bleu([ref_toks], pred_toks, smoothing_function=sf) if pred_toks else 0.0\n    per_sample_bleu.append(b)\n\nper_sample_rougeL = np.array(per_sample_rougeL)\nper_sample_bleu   = np.array(per_sample_bleu)\nbertscore_f1_np   = F1.numpy()\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].hist(per_sample_rougeL, bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\naxes[0].axvline(per_sample_rougeL.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_rougeL.mean():.3f}\")\naxes[0].set_title(\"ROUGE-L Distribution (per sample)\")\naxes[0].set_xlabel(\"Score\"); axes[0].set_ylabel(\"Frequency\"); axes[0].legend()\n\naxes[1].hist(per_sample_bleu, bins=30, color=\"coral\", alpha=0.7, edgecolor=\"black\")\naxes[1].axvline(per_sample_bleu.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_bleu.mean():.3f}\")\naxes[1].set_title(\"BLEU-4 Distribution (per sample)\")\naxes[1].set_xlabel(\"Score\"); axes[1].set_ylabel(\"Frequency\"); axes[1].legend()\n\naxes[2].hist(bertscore_f1_np, bins=30, color=\"seagreen\", alpha=0.7, edgecolor=\"black\")\naxes[2].axvline(bertscore_f1_np.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {bertscore_f1_np.mean():.3f}\")\naxes[2].set_title(\"BERTScore F1 Distribution\")\naxes[2].set_xlabel(\"Score\"); axes[2].set_ylabel(\"Frequency\"); axes[2].legend()\n\nplt.tight_layout()\nplt.savefig(OUTPUT_PATH / \"score_distributions.png\", dpi=150)\nplt.show()\nprint(f\"Saved: {OUTPUT_PATH / 'score_distributions.png'}\")\n\n# Attach per-sample scores to the DataFrame so later cells can rank examples.\ntest_formatted['rougeL'] = per_sample_rougeL\ntest_formatted['bleu']   = per_sample_bleu\ntest_formatted['bertscore_f1'] = bertscore_f1_np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIGH SCORING EXAMPLES (Top 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_samples = test_formatted.nlargest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in top_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLOW SCORING EXAMPLES (Bottom 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bottom_samples = test_formatted.nsmallest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in bottom_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation\n\nAutomatic metrics cannot assess whether a question is genuinely Socratic \u2014 i.e., whether it challenges unstated assumptions and resists a direct answer from the given context. The 50-item sample (`human_evaluation_samples.csv`) is formatted for manual rating on three criteria: *fluency* (1\u20135 Likert), *relevance* (1\u20135 Likert), and *Socratic quality* (binary). Inter-rater reliability can be quantified with Krippendorff's alpha."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_sample = test_formatted.sample(50, random_state=42)[\n",
    "    ['original_input', 'reference', 'prediction', 'rougeL', 'bertscore_f1']\n",
    "].copy()\n",
    "\n",
    "human_eval_sample['fluency'] = None\n",
    "human_eval_sample['relevance'] = None\n",
    "human_eval_sample['is_socratic'] = None\n",
    "\n",
    "human_eval_sample = human_eval_sample.reset_index(drop=True)\n",
    "human_eval_sample.to_csv(OUTPUT_PATH / \"human_evaluation_samples.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(human_eval_sample)} samples for human evaluation.\")\n",
    "print(f\"File: {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n\nevaluation_results = {\n    \"test_samples\": len(test_formatted),\n    \"rouge\": rouge_results,\n    \"bleu4\": float(bleu4),  # bleu4 computed by sacrebleu cell above,\n    \"bertscore\": bertscore_results,\n    \"generation_config\": EVAL_GENERATION_CONFIG\n}\n\nwith open(OUTPUT_PATH / \"evaluation_metrics.json\", \"w\") as f:\n    json.dump(evaluation_results, f, indent=2)\n\ntest_formatted.to_csv(OUTPUT_PATH / \"test_predictions.csv\", index=False)\n\nprint(\"\\nSaved:\")\nprint(f\"  - {OUTPUT_PATH / 'evaluation_metrics.json'}\")\nprint(f\"  - {OUTPUT_PATH / 'test_predictions.csv'}\")\nprint(f\"  - {OUTPUT_PATH / 'score_distributions.png'}\")\nprint(f\"  - {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = test_formatted[['rougeL', 'bleu', 'bertscore_f1']].copy()\n",
    "metrics_df.columns = ['ROUGE-L', 'BLEU-4', 'BERTScore']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation = metrics_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "plt.title('Metric Correlations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / \"metric_correlations.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation results are saved to `evaluation_results/`. The JSON file (`evaluation_metrics.json`) records corpus-level ROUGE, BLEU-4, and BERTScore F1 alongside the generation configuration, making the results reproducible from the same checkpoint. The per-question-type ROUGE breakdown provides a novel analysis not reported in Ang et al. (2023)."
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}