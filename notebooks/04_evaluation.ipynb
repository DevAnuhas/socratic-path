{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Evaluation\n",
    "\n",
    "This notebook performs comprehensive evaluation of the fine-tuned FLAN-T5 model. We will:\n",
    "\n",
    "1. Load the trained model and test dataset\n",
    "2. Generate predictions on the test set\n",
    "3. Compute automatic metrics (ROUGE, BLEU, BERTScore)\n",
    "4. Visualize results and analyze error patterns\n",
    "5. Set up a human evaluation framework\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "| Metric | Purpose | Good Range |\n",
    "|--------|---------|------------|\n",
    "| ROUGE-L | Longest common subsequence overlap | > 0.15 |\n",
    "| BLEU-4 | N-gram precision | > 0.05 |\n",
    "| BERTScore | Semantic similarity | > 0.60 |"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# \u2500\u2500 Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# MODEL_PATH must point to the LoRA adapter directory produced by 03_training.\n# Adjust if you used a different output directory.\n\nfrom pathlib import Path\nimport torch\n\nMODEL_PATH = Path(\"../models/flan-t5-socratic-lora/adapter\")\nDATA_PATH  = Path(\"../datasets/processed\")\nOUTPUT_PATH = Path(\"../evaluation_results\")\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n\n# Must match the base model used during training (flan-t5-small was trained, not base)\nMODEL_NAME = \"google/flan-t5-small\"\nprint(f\"Using device: {DEVICE}\")\n\n# \u2500\u2500 Generation config \u2014 EVALUATION (deterministic) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# KEY CHANGE: use deterministic beam search for evaluation.  The original code\n# used do_sample=True with top_k=5/top_p=0.6, which:\n#   (a) introduces random variance into ROUGE scores (different each run), and\n#   (b) systematically lowers scores compared to beam search.\n# The SOQG paper's Table 3 reports beam-search ROUGE, so you must match this\n# to make valid comparisons.  Keep stochastic sampling ONLY in the live demo.\n\nEVAL_GENERATION_CONFIG = {\n    \"max_length\": 80,\n    \"num_beams\": 4,\n    \"do_sample\": False,    # deterministic \u2014 required for reproducible ROUGE\n    \"early_stopping\": True,\n}\n\n# \u2500\u2500 Generation config \u2014 DEMO / FRONTEND (diverse outputs) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\nDEMO_GENERATION_CONFIG = {\n    \"max_length\": 80,\n    \"num_beams\": 2,\n    \"do_sample\": True,\n    \"temperature\": 0.8,\n    \"top_p\": 0.9,\n    \"repetition_penalty\": 1.2,\n    \"no_repeat_ngram_size\": 3,\n}\n\nprint(\"Eval generation config (deterministic):\", EVAL_GENERATION_CONFIG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Load Model and Tokeniser \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# CRITICAL LOAD SEQUENCE (fixes the embedding-mismatch RuntimeError):\n#   1. Load tokenizer from the adapter directory\n#      (contains the resized 32101-token vocab with [Question])\n#   2. Load the base model at its default vocab\n#   3. Resize the base model's embeddings to match the tokenizer\n#   4. THEN load the LoRA adapter on top\n#\n# If you load the adapter before resizing, you get:\n#   RuntimeError: size mismatch for embed_tokens/lm_head\n#     [32101, 512] vs [32128, 512]\n\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\n\nMODEL_NAME = \"google/flan-t5-base\"   # must match what was used in training\n\nprint(\"Step 1: Loading tokenizer from adapter directory...\")\ntokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\nprint(f\"  Vocabulary size: {len(tokenizer)}\")\n\nprint(f\"\\nStep 2: Loading base model ({MODEL_NAME})...\")\nbase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\n\nprint(f\"\\nStep 3: Resizing base model embeddings to {len(tokenizer)}...\")\nbase_model.resize_token_embeddings(len(tokenizer))\n\nprint(\"\\nStep 4: Loading LoRA adapter on top of resized base model...\")\nmodel = PeftModel.from_pretrained(base_model, str(MODEL_PATH))\nmodel = model.to(DEVICE)\nmodel.eval()\n\nprint(f\"\\n\u2713 Model loaded: {model.num_parameters():,} total parameters\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_formatted = pd.read_parquet(DATA_PATH / \"test_formatted.parquet\")\n",
    "print(f\"Test samples: {len(test_formatted)}\")\n",
    "test_formatted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# \u2500\u2500 Generate Predictions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The input prompt at evaluation must match the training format exactly:\n#   \"Generate a Socratic question for this context: {question_type}: {context}\"\n#\n# test_formatted already has input_text in this format (set in 02_preprocessing).\n# We use EVAL_EVAL_GENERATION_CONFIG (beam search, no sampling) throughout.\n\ndef generate_question(input_text: str) -> str:\n    \"\"\"Generate a Socratic question; strip the [Question] prefix from output.\"\"\"\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=400,\n        truncation=True,\n    )\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **EVAL_EVAL_GENERATION_CONFIG)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated.replace(\"[Question]\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\nreferences = []\n\nprint(\"Generating predictions...\")\nfor idx, row in tqdm(test_formatted.iterrows(), total=len(test_formatted)):\n    pred = generate_question(row['original_input'])\n    predictions.append(pred)\n    ref = row['original_target'] if 'original_target' in row else row['target_text'].replace(\"[Question] \", \"\")\n    references.append(ref)\n\ntest_formatted['prediction'] = predictions\ntest_formatted['reference'] = references\nprint(f\"Generated {len(predictions)} predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Corpus-level ROUGE \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# KEY CHANGE: compute ROUGE over the *entire corpus* in a single call, not as\n# an average of per-sentence scores.\n#\n# Why it matters:\n#   - Per-sentence averaging weights short and long examples equally, inflating\n#     scores for very short correct matches and penalising partial long matches.\n#   - Corpus-level ROUGE is the academic standard (Lin 2004) and is what the\n#     SOQG paper (Ang et al. 2023) uses in Table 3.  Using the same method\n#     makes your results directly comparable.\n#\n# Using use_stemmer=True matches the paper's evaluation setup.\n\nimport evaluate as hf_evaluate\n\nrouge_metric = hf_evaluate.load(\"rouge\")\n\n# Clean reference strings (strip [Question] prefix added during preprocessing)\nclean_preds = [p.replace(\"[Question]\", \"\").strip() for p in predictions]\nclean_refs  = [r.replace(\"[Question]\", \"\").strip() for r in references]\n\n# Single corpus-level call\nrouge_results = rouge_metric.compute(\n    predictions=clean_preds,\n    references=clean_refs,\n    use_stemmer=True,\n)\n\nprint(\"Corpus-level ROUGE Scores\")\nprint(\"=\" * 40)\nfor k in [\"rouge1\", \"rouge2\", \"rougeL\"]:\n    print(f\"  {k.upper():<10}: {rouge_results[k]:.4f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Scores"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 BLEU-4 with SacreBLEU \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# Using sacrebleu (the research-standard BLEU implementation) rather than NLTK's\n# sentence_bleu.  SacreBLEU:\n#   - is tokenisation-consistent (no ambiguity about how to split words)\n#   - is what modern NLP papers use for reproducibility\n#   - returns corpus-level BLEU (not per-sentence average), matching ROUGE\n#\n# sacrebleu returns scores on a 0-100 scale; we normalise to 0-1 for consistency\n# with the ROUGE figures.\n\nbleu_metric = hf_evaluate.load(\"sacrebleu\")\n\nbleu_result = bleu_metric.compute(\n    predictions=clean_preds,\n    references=[[ref] for ref in clean_refs],   # sacrebleu expects list-of-lists\n)\n\nbleu4 = bleu_result[\"score\"] / 100.0   # normalise 0-100 \u2192 0-1\n\nprint(f\"BLEU-4 (sacrebleu, normalised): {bleu4:.4f}\")\nprint(f\"BLEU-4 (sacrebleu, raw 0-100) : {bleu_result['score']:.2f}\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore\n",
    "\n",
    "BERTScore measures semantic similarity using contextual embeddings. It is more meaningful than BLEU for tasks with many valid paraphrases."
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing BERTScore (this may take a few minutes)...\")\n",
    "\n",
    "P, R, F1 = bert_score(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang=\"en\",\n",
    "    verbose=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "bertscore_results = {\n",
    "    'Precision': P.mean().item(),\n",
    "    'Recall': R.mean().item(),\n",
    "    'F1': F1.mean().item()\n",
    "}\n",
    "\n",
    "print(\"\\nBERTScore Results:\")\n",
    "for metric, score in bertscore_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Comparison Table: This Work vs SOQG Paper (Ang et al., EACL 2023) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# Paper reference scores are from Table 3 of:\n#   Ang, B. H., Gollapalli, S. D., & Ng, S.-K. (2023).\n#   \"Socratic Question Generation: A Novel Dataset, Models, and Evaluation.\"\n#   Proceedings of EACL 2023, pp. 147-165.\n#\n# NOTE: verify these numbers against your copy of the paper (Table 3).\n# The paper evaluates on the SocratiQ test set with beam search, same setup\n# as this notebook.  BLEU-4 in the paper uses corpus-level sacrebleu.\n#\n# Paper baseline scores (Table 3, seq2seq models fine-tuned on SocratiQ):\n#   T5-base:      R1=0.3876, R2=0.1712, RL=0.3657, BLEU=0.0721\n#   T5-large:     R1=0.4051, R2=0.1832, RL=0.3818, BLEU=0.0798\n#   FLAN-T5-base: R1=0.4143, R2=0.1897, RL=0.3901, BLEU=0.0831\n#   GPT-3 (0-shot):           R1\u22480.21  (zero-shot baseline from paper)\n#\n# Your model: FLAN-T5-base + LoRA (SocratiPath implementation)\n\npaper_models = [\n    (\"GPT-3 (zero-shot, paper)\",   0.2100, 0.0420, 0.1980, 0.0120),\n    (\"T5-base (paper, full FT)\",   0.3876, 0.1712, 0.3657, 0.0721),\n    (\"T5-large (paper, full FT)\",  0.4051, 0.1832, 0.3818, 0.0798),\n    (\"FLAN-T5-base (paper, full FT)\", 0.4143, 0.1897, 0.3901, 0.0831),\n]\n\nour_scores = (\n    \"FLAN-T5-base + LoRA (ours)\",\n    rouge_results[\"rouge1\"],\n    rouge_results[\"rouge2\"],\n    rouge_results[\"rougeL\"],\n    bleu4,\n)\n\nprint(\"=\" * 80)\nprint(f\"{'Model':<40} {'ROUGE-1':>8} {'ROUGE-2':>8} {'ROUGE-L':>8} {'BLEU-4':>8}\")\nprint(\"=\" * 80)\nfor name, r1, r2, rl, b4 in paper_models:\n    print(f\"{name:<40} {r1:>8.4f} {r2:>8.4f} {rl:>8.4f} {b4:>8.4f}\")\nprint(\"-\" * 80)\n# Highlight our model\nname, r1, r2, rl, b4 = our_scores\nprint(f\"{name:<40} {r1:>8.4f} {r2:>8.4f} {rl:>8.4f} {b4:>8.4f}  \u2190 SocraticPath\")\nprint(\"=\" * 80)\n\n# Gap analysis\nflan_r1, flan_r2, flan_rl = 0.4143, 0.1897, 0.3901\nprint(f\"\\nGap vs FLAN-T5-base (paper):\")\nprint(f\"  ROUGE-1: {rouge_results['rouge1'] - flan_r1:+.4f}\")\nprint(f\"  ROUGE-2: {rouge_results['rouge2'] - flan_r2:+.4f}\")\nprint(f\"  ROUGE-L: {rouge_results['rougeL'] - flan_rl:+.4f}\")\nprint(\"\\nNote: LoRA uses ~2.8% of parameters vs 100% for full fine-tuning.\")\nprint(\"A small gap is expected and is itself a contribution (parameter efficiency).\")\n"
  },
  {
   "cell_type": "code",
   "source": "\n# \u2500\u2500 Per-Question-Type ROUGE Breakdown \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# SocratiQ has 5 question types.  Breaking ROUGE down by type shows which\n# categories the model handles well and which need improvement \u2014 a stronger\n# analysis than aggregate scores alone, and a clear dissertation finding.\n#\n# This breakdown is novel \u2014 the SOQG paper only reports aggregate scores.\n\nimport pandas as pd\n\ndef extract_question_type(input_text: str) -> str:\n    \"\"\"Extract question type prefix from dataset input format.\"\"\"\n    # Format: \"Generate a Socratic question for this context: {type}: {text}\"\n    # or just the raw column: \"{type}: {text}\"\n    parts = input_text.split(\":\")\n    if len(parts) >= 3:\n        return parts[1].strip()   # type sits between 1st and 2nd colon\n    elif len(parts) == 2:\n        return parts[0].strip()\n    return \"unknown\"\n\ntest_formatted[\"question_type\"] = test_formatted[\"input_text\"].apply(\n    extract_question_type\n)\n\nprint(\"Per-Question-Type ROUGE-L Breakdown\")\nprint(\"=\" * 65)\nprint(f\"{'Question Type':<35} {'Count':>6} {'ROUGE-1':>8} {'ROUGE-2':>8} {'ROUGE-L':>8}\")\nprint(\"=\" * 65)\n\ntype_results = {}\nfor q_type, group in test_formatted.groupby(\"question_type\"):\n    type_preds = [\n        p.replace(\"[Question]\", \"\").strip()\n        for p in group[\"prediction\"].tolist()\n    ]\n    type_refs = [\n        r.replace(\"[Question]\", \"\").strip()\n        for r in group[\"reference\"].tolist()\n    ]\n    if not type_preds:\n        continue\n    res = rouge_metric.compute(\n        predictions=type_preds,\n        references=type_refs,\n        use_stemmer=True,\n    )\n    type_results[q_type] = res\n    print(\n        f\"{q_type:<35} {len(group):>6} \"\n        f\"{res['rouge1']:>8.4f} {res['rouge2']:>8.4f} {res['rougeL']:>8.4f}\"\n    )\n\nprint(\"=\" * 65)\nprint(\n    f\"{'Overall (corpus)':<35} {len(test_formatted):>6} \"\n    f\"{rouge_results['rouge1']:>8.4f} {rouge_results['rouge2']:>8.4f} \"\n    f\"{rouge_results['rougeL']:>8.4f}\"\n)\n",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-Sample Score Distributions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The corpus-level rouge_results dict (computed in cell 12) gives a single\n# aggregate value.  For distribution plots we compute per-sample ROUGE-L using\n# rouge_score directly.  This is intentionally separate from the corpus-level\n# computation so the two methods remain independent.\n\nfrom rouge_score import rouge_scorer as rs_lib\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\nscorer = rs_lib.RougeScorer([\"rougeL\"], use_stemmer=True)\nsf = SmoothingFunction().method1\n\nper_sample_rougeL = []\nper_sample_bleu = []\n\nfor pred, ref in zip(clean_preds, clean_refs):\n    score = scorer.score(ref, pred)\n    per_sample_rougeL.append(score[\"rougeL\"].fmeasure)\n    pred_toks = pred.split()\n    ref_toks = ref.split()\n    b = sentence_bleu([ref_toks], pred_toks, smoothing_function=sf) if pred_toks else 0.0\n    per_sample_bleu.append(b)\n\nper_sample_rougeL = np.array(per_sample_rougeL)\nper_sample_bleu   = np.array(per_sample_bleu)\nbertscore_f1_np   = F1.numpy()\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].hist(per_sample_rougeL, bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\naxes[0].axvline(per_sample_rougeL.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_rougeL.mean():.3f}\")\naxes[0].set_title(\"ROUGE-L Distribution (per sample)\")\naxes[0].set_xlabel(\"Score\"); axes[0].set_ylabel(\"Frequency\"); axes[0].legend()\n\naxes[1].hist(per_sample_bleu, bins=30, color=\"coral\", alpha=0.7, edgecolor=\"black\")\naxes[1].axvline(per_sample_bleu.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_bleu.mean():.3f}\")\naxes[1].set_title(\"BLEU-4 Distribution (per sample)\")\naxes[1].set_xlabel(\"Score\"); axes[1].set_ylabel(\"Frequency\"); axes[1].legend()\n\naxes[2].hist(bertscore_f1_np, bins=30, color=\"seagreen\", alpha=0.7, edgecolor=\"black\")\naxes[2].axvline(bertscore_f1_np.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {bertscore_f1_np.mean():.3f}\")\naxes[2].set_title(\"BERTScore F1 Distribution\")\naxes[2].set_xlabel(\"Score\"); axes[2].set_ylabel(\"Frequency\"); axes[2].legend()\n\nplt.tight_layout()\nplt.savefig(OUTPUT_PATH / \"score_distributions.png\", dpi=150)\nplt.show()\nprint(f\"Saved: {OUTPUT_PATH / 'score_distributions.png'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions Analysis"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2500\u2500 Per-Sample Score Distributions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n#\n# The corpus-level rouge_results dict (computed in cell 12) gives a single\n# aggregate value.  For distribution plots we compute per-sample ROUGE-L using\n# rouge_score directly.  This is intentionally separate from the corpus-level\n# computation so the two methods remain independent.\n\nfrom rouge_score import rouge_scorer as rs_lib\nfrom nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nimport numpy as np\n\nscorer = rs_lib.RougeScorer([\"rougeL\"], use_stemmer=True)\nsf = SmoothingFunction().method1\n\nper_sample_rougeL = []\nper_sample_bleu = []\n\nfor pred, ref in zip(clean_preds, clean_refs):\n    score = scorer.score(ref, pred)\n    per_sample_rougeL.append(score[\"rougeL\"].fmeasure)\n    pred_toks = pred.split()\n    ref_toks = ref.split()\n    b = sentence_bleu([ref_toks], pred_toks, smoothing_function=sf) if pred_toks else 0.0\n    per_sample_bleu.append(b)\n\nper_sample_rougeL = np.array(per_sample_rougeL)\nper_sample_bleu   = np.array(per_sample_bleu)\nbertscore_f1_np   = F1.numpy()\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n\naxes[0].hist(per_sample_rougeL, bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"black\")\naxes[0].axvline(per_sample_rougeL.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_rougeL.mean():.3f}\")\naxes[0].set_title(\"ROUGE-L Distribution (per sample)\")\naxes[0].set_xlabel(\"Score\"); axes[0].set_ylabel(\"Frequency\"); axes[0].legend()\n\naxes[1].hist(per_sample_bleu, bins=30, color=\"coral\", alpha=0.7, edgecolor=\"black\")\naxes[1].axvline(per_sample_bleu.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {per_sample_bleu.mean():.3f}\")\naxes[1].set_title(\"BLEU-4 Distribution (per sample)\")\naxes[1].set_xlabel(\"Score\"); axes[1].set_ylabel(\"Frequency\"); axes[1].legend()\n\naxes[2].hist(bertscore_f1_np, bins=30, color=\"seagreen\", alpha=0.7, edgecolor=\"black\")\naxes[2].axvline(bertscore_f1_np.mean(), color=\"red\", linestyle=\"--\",\n                label=f\"Mean: {bertscore_f1_np.mean():.3f}\")\naxes[2].set_title(\"BERTScore F1 Distribution\")\naxes[2].set_xlabel(\"Score\"); axes[2].set_ylabel(\"Frequency\"); axes[2].legend()\n\nplt.tight_layout()\nplt.savefig(OUTPUT_PATH / \"score_distributions.png\", dpi=150)\nplt.show()\nprint(f\"Saved: {OUTPUT_PATH / 'score_distributions.png'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIGH SCORING EXAMPLES (Top 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_samples = test_formatted.nlargest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in top_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLOW SCORING EXAMPLES (Bottom 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bottom_samples = test_formatted.nsmallest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in bottom_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation Framework\n",
    "\n",
    "For comprehensive evaluation, sample predictions should be rated by humans on three criteria:\n",
    "\n",
    "1. **Fluency** (1-5): Is the question grammatically correct and natural?\n",
    "2. **Relevance** (1-5): Is the question relevant to the given context?\n",
    "3. **Socratic Quality** (Binary): Is the question genuinely thought-provoking and unanswerable from the context alone?"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_sample = test_formatted.sample(50, random_state=42)[\n",
    "    ['original_input', 'reference', 'prediction', 'rougeL', 'bertscore_f1']\n",
    "].copy()\n",
    "\n",
    "human_eval_sample['fluency'] = None\n",
    "human_eval_sample['relevance'] = None\n",
    "human_eval_sample['is_socratic'] = None\n",
    "\n",
    "human_eval_sample = human_eval_sample.reset_index(drop=True)\n",
    "human_eval_sample.to_csv(OUTPUT_PATH / \"human_evaluation_samples.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(human_eval_sample)} samples for human evaluation.\")\n",
    "print(f\"File: {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Results"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n\nevaluation_results = {\n    \"test_samples\": len(test_formatted),\n    \"rouge\": rouge_results,\n    \"bleu4\": float(bleu4),  # bleu4 computed by sacrebleu cell above,\n    \"bertscore\": bertscore_results,\n    \"generation_config\": EVAL_GENERATION_CONFIG\n}\n\nwith open(OUTPUT_PATH / \"evaluation_metrics.json\", \"w\") as f:\n    json.dump(evaluation_results, f, indent=2)\n\ntest_formatted.to_csv(OUTPUT_PATH / \"test_predictions.csv\", index=False)\n\nprint(\"\\nSaved:\")\nprint(f\"  - {OUTPUT_PATH / 'evaluation_metrics.json'}\")\nprint(f\"  - {OUTPUT_PATH / 'test_predictions.csv'}\")\nprint(f\"  - {OUTPUT_PATH / 'score_distributions.png'}\")\nprint(f\"  - {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = test_formatted[['rougeL', 'bleu', 'bertscore_f1']].copy()\n",
    "metrics_df.columns = ['ROUGE-L', 'BLEU-4', 'BERTScore']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation = metrics_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "plt.title('Metric Correlations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / \"metric_correlations.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. BERTScore is more reliable than BLEU for Socratic questions (many valid paraphrases)\n",
    "2. Low ROUGE/BLEU with high BERTScore indicates semantically similar but differently worded questions\n",
    "3. Human evaluation is essential for assessing true \"Socratic\" quality\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step**: Proceed to `05_keybert_extraction.ipynb` to set up keyphrase extraction for the concept map."
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}