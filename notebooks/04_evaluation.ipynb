{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Evaluation\n",
    "\n",
    "This notebook evaluates the fine-tuned FLAN-T5 model using automated metrics and manual evaluation.\n",
    "\n",
    "## Objectives\n",
    "- Generate questions on the test set\n",
    "- Calculate BLEU, ROUGE, and BERTScore metrics\n",
    "- Compare to EACL 2023 paper baselines\n",
    "- Perform manual evaluation on 50 samples\n",
    "- Analyze errors by question type\n",
    "\n",
    "## Paper Baselines (FLAN-T5-base)\n",
    "| Metric | Paper Score | Target |\n",
    "|--------|-------------|--------|\n",
    "| BLEU-1 | 0.172 | > 0.15 |\n",
    "| ROUGE-L | 0.211 | > 0.18 |\n",
    "| BERTScore | 0.632 | > 0.60 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"../backend/model_artifacts/soqg_flan_t5/final\")\n",
    "DATASET_PATH = Path(\"../datasets/processed/soqg_tokenized\")\n",
    "CLEAN_DATA_PATH = Path(\"../datasets/processed/test_clean.csv\")\n",
    "OUTPUT_DIR = Path(\"../experiments/evaluation\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(device)\n",
    "tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded from {MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(DATASET_PATH)\n",
    "test_clean = pd.read_csv(CLEAN_DATA_PATH)\n",
    "\n",
    "print(f\"Test set size: {len(dataset['test'])}\")\n",
    "print(f\"Clean test CSV size: {len(test_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(input_text, max_length=128, num_beams=4):\n",
    "    \"\"\"Generate a Socratic question from input.\"\"\"\n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            num_beams=num_beams,\n",
    "            early_stopping=True,\n",
    "            no_repeat_ngram_size=2\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Generate on Test Set\n",
    "\n",
    "This may take 30-60 minutes depending on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "question_types = []\n",
    "inputs_list = []\n",
    "\n",
    "for i, row in tqdm(test_clean.iterrows(), total=len(test_clean), desc=\"Generating\"):\n",
    "    input_text = row['formatted_input']\n",
    "    reference = row['target']\n",
    "    q_type = row['question_type']\n",
    "    \n",
    "    prediction = generate_question(input_text)\n",
    "    \n",
    "    predictions.append(prediction)\n",
    "    references.append(reference)\n",
    "    question_types.append(q_type)\n",
    "    inputs_list.append(input_text)\n",
    "\n",
    "print(f\"\\nGenerated {len(predictions)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame({\n",
    "    'input': inputs_list,\n",
    "    'reference': references,\n",
    "    'prediction': predictions,\n",
    "    'question_type': question_types\n",
    "})\n",
    "\n",
    "results_df.to_csv(OUTPUT_DIR / \"generation_results.csv\", index=False)\n",
    "print(f\"Results saved to {OUTPUT_DIR / 'generation_results.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Calculate BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoother = SmoothingFunction()\n",
    "\n",
    "bleu_1_scores = []\n",
    "bleu_2_scores = []\n",
    "bleu_4_scores = []\n",
    "\n",
    "for pred, ref in tqdm(zip(predictions, references), total=len(predictions), desc=\"BLEU\"):\n",
    "    pred_tokens = nltk.word_tokenize(pred.lower())\n",
    "    ref_tokens = [nltk.word_tokenize(ref.lower())]\n",
    "    \n",
    "    bleu_1 = sentence_bleu(ref_tokens, pred_tokens, weights=(1, 0, 0, 0), smoothing_function=smoother.method1)\n",
    "    bleu_2 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.5, 0.5, 0, 0), smoothing_function=smoother.method1)\n",
    "    bleu_4 = sentence_bleu(ref_tokens, pred_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoother.method1)\n",
    "    \n",
    "    bleu_1_scores.append(bleu_1)\n",
    "    bleu_2_scores.append(bleu_2)\n",
    "    bleu_4_scores.append(bleu_4)\n",
    "\n",
    "results_df['bleu_1'] = bleu_1_scores\n",
    "results_df['bleu_2'] = bleu_2_scores\n",
    "results_df['bleu_4'] = bleu_4_scores\n",
    "\n",
    "print(f\"\\nBLEU Scores:\")\n",
    "print(f\"  BLEU-1: {np.mean(bleu_1_scores):.4f} (Paper: 0.172)\")\n",
    "print(f\"  BLEU-2: {np.mean(bleu_2_scores):.4f}\")\n",
    "print(f\"  BLEU-4: {np.mean(bleu_4_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Calculate ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "for pred, ref in tqdm(zip(predictions, references), total=len(predictions), desc=\"ROUGE\"):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "\n",
    "results_df['rouge_1'] = rouge1_scores\n",
    "results_df['rouge_2'] = rouge2_scores\n",
    "results_df['rouge_L'] = rougeL_scores\n",
    "\n",
    "print(f\"\\nROUGE Scores:\")\n",
    "print(f\"  ROUGE-1: {np.mean(rouge1_scores):.4f}\")\n",
    "print(f\"  ROUGE-2: {np.mean(rouge2_scores):.4f}\")\n",
    "print(f\"  ROUGE-L: {np.mean(rougeL_scores):.4f} (Paper: 0.211)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Calculate BERTScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P, R, F1 = bert_score(predictions, references, lang=\"en\", verbose=True)\n",
    "\n",
    "results_df['bertscore_p'] = P.numpy()\n",
    "results_df['bertscore_r'] = R.numpy()\n",
    "results_df['bertscore_f1'] = F1.numpy()\n",
    "\n",
    "print(f\"\\nBERTScore:\")\n",
    "print(f\"  Precision: {P.mean():.4f}\")\n",
    "print(f\"  Recall: {R.mean():.4f}\")\n",
    "print(f\"  F1: {F1.mean():.4f} (Paper: 0.632)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Metrics by Question Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_metrics = results_df.groupby('question_type').agg({\n",
    "    'bleu_1': 'mean',\n",
    "    'rouge_L': 'mean',\n",
    "    'bertscore_f1': 'mean',\n",
    "    'input': 'count'\n",
    "}).rename(columns={'input': 'count'})\n",
    "\n",
    "print(\"\\nMetrics by Question Type:\")\n",
    "print(type_metrics.round(4).to_markdown())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "x = np.arange(len(type_metrics))\n",
    "width = 0.25\n",
    "\n",
    "bars1 = ax.bar(x - width, type_metrics['bleu_1'], width, label='BLEU-1', color='#3B82F6')\n",
    "bars2 = ax.bar(x, type_metrics['rouge_L'], width, label='ROUGE-L', color='#10B981')\n",
    "bars3 = ax.bar(x + width, type_metrics['bertscore_f1'], width, label='BERTScore', color='#8B5CF6')\n",
    "\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Evaluation Metrics by Question Type')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(type_metrics.index, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_DIR / \"metrics_by_type.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Sample Outputs for Manual Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Outputs (10 per type):\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for q_type in results_df['question_type'].unique():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"TYPE: {q_type.upper()}\")\n",
    "    print('='*80)\n",
    "    \n",
    "    type_samples = results_df[results_df['question_type'] == q_type].sample(\n",
    "        min(10, len(results_df[results_df['question_type'] == q_type])), \n",
    "        random_state=SEED\n",
    "    )\n",
    "    \n",
    "    for idx, row in type_samples.head(3).iterrows():\n",
    "        context = row['input'].split(':', 2)[-1][:150]\n",
    "        print(f\"\\nContext: {context}...\")\n",
    "        print(f\"Reference: {row['reference']}\")\n",
    "        print(f\"Generated: {row['prediction']}\")\n",
    "        print(f\"BLEU-1: {row['bleu_1']:.3f}, ROUGE-L: {row['rouge_L']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Manual Evaluation Form\n",
    "\n",
    "Export 50 samples for manual evaluation following the paper's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_per_type = 10\n",
    "manual_eval_samples = []\n",
    "\n",
    "for q_type in results_df['question_type'].unique():\n",
    "    type_data = results_df[results_df['question_type'] == q_type]\n",
    "    samples = type_data.sample(min(samples_per_type, len(type_data)), random_state=SEED)\n",
    "    manual_eval_samples.append(samples)\n",
    "\n",
    "manual_eval_df = pd.concat(manual_eval_samples, ignore_index=True)\n",
    "\n",
    "manual_eval_df['fluency'] = \"\"\n",
    "manual_eval_df['relevance'] = \"\"\n",
    "manual_eval_df['answerability'] = \"\"\n",
    "manual_eval_df['notes'] = \"\"\n",
    "\n",
    "eval_columns = ['question_type', 'input', 'reference', 'prediction', \n",
    "                'fluency', 'relevance', 'answerability', 'notes']\n",
    "manual_eval_df[eval_columns].to_csv(OUTPUT_DIR / \"manual_evaluation_form.csv\", index=False)\n",
    "\n",
    "print(f\"Manual evaluation form saved with {len(manual_eval_df)} samples\")\n",
    "print(f\"\\nEvaluation Criteria (from paper):\")\n",
    "print(\"  Fluency (1-5): Is the question grammatically correct and natural?\")\n",
    "print(\"  Relevance (1-5): Is the question relevant to the context?\")\n",
    "print(\"  Answerability (0/1): Can the question be answered from the context? (0 = good for Socratic)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_bleu = results_df[results_df['bleu_1'] < 0.05].sample(min(5, len(results_df[results_df['bleu_1'] < 0.05])), random_state=SEED)\n",
    "\n",
    "print(\"Low BLEU Examples (potential issues):\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in low_bleu.iterrows():\n",
    "    print(f\"\\nType: {row['question_type']}\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Generated: {row['prediction']}\")\n",
    "    print(f\"BLEU-1: {row['bleu_1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df['pred_length'] = results_df['prediction'].apply(lambda x: len(x.split()))\n",
    "results_df['ref_length'] = results_df['reference'].apply(lambda x: len(x.split()))\n",
    "\n",
    "short_preds = results_df[results_df['pred_length'] < 3]\n",
    "long_preds = results_df[results_df['pred_length'] > 50]\n",
    "\n",
    "print(f\"\\nLength Analysis:\")\n",
    "print(f\"  Very short predictions (< 3 words): {len(short_preds)}\")\n",
    "print(f\"  Very long predictions (> 50 words): {len(long_preds)}\")\n",
    "print(f\"  Avg prediction length: {results_df['pred_length'].mean():.1f} words\")\n",
    "print(f\"  Avg reference length: {results_df['ref_length'].mean():.1f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = {\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"test_samples\": len(results_df),\n",
    "    \"metrics\": {\n",
    "        \"bleu_1\": float(np.mean(bleu_1_scores)),\n",
    "        \"bleu_2\": float(np.mean(bleu_2_scores)),\n",
    "        \"bleu_4\": float(np.mean(bleu_4_scores)),\n",
    "        \"rouge_1\": float(np.mean(rouge1_scores)),\n",
    "        \"rouge_2\": float(np.mean(rouge2_scores)),\n",
    "        \"rouge_L\": float(np.mean(rougeL_scores)),\n",
    "        \"bertscore_f1\": float(F1.mean())\n",
    "    },\n",
    "    \"paper_baselines\": {\n",
    "        \"bleu_1\": 0.172,\n",
    "        \"rouge_L\": 0.211,\n",
    "        \"bertscore_f1\": 0.632\n",
    "    },\n",
    "    \"metrics_by_type\": type_metrics.to_dict()\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"evaluation_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTest samples: {len(results_df)}\")\n",
    "print(f\"\\nOverall Metrics vs Paper Baselines:\")\n",
    "print(f\"  BLEU-1:     {np.mean(bleu_1_scores):.4f} (Paper: 0.172) {'✓' if np.mean(bleu_1_scores) >= 0.15 else '✗'}\")\n",
    "print(f\"  ROUGE-L:    {np.mean(rougeL_scores):.4f} (Paper: 0.211) {'✓' if np.mean(rougeL_scores) >= 0.18 else '✗'}\")\n",
    "print(f\"  BERTScore:  {F1.mean():.4f} (Paper: 0.632) {'✓' if F1.mean() >= 0.60 else '✗'}\")\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Next Steps\n",
    "\n",
    "1. **Complete manual evaluation** using `manual_evaluation_form.csv`\n",
    "2. **Calculate inter-annotator agreement** if multiple evaluators\n",
    "3. **Analyze failure cases** to identify improvement areas\n",
    "4. **Proceed to vector store setup** (05_keyphrase_extraction.ipynb)\n",
    "\n",
    "### For Report\n",
    "Include:\n",
    "- Table comparing your metrics to paper baselines\n",
    "- Metrics breakdown by question type chart\n",
    "- Error analysis examples\n",
    "- Manual evaluation results (after completion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
