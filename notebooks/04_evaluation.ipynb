{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Model Evaluation\n",
    "\n",
    "This notebook performs comprehensive evaluation of the fine-tuned FLAN-T5 model. We will:\n",
    "\n",
    "1. Load the trained model and test dataset\n",
    "2. Generate predictions on the test set\n",
    "3. Compute automatic metrics (ROUGE, BLEU, BERTScore)\n",
    "4. Visualize results and analyze error patterns\n",
    "5. Set up a human evaluation framework\n",
    "\n",
    "---\n",
    "\n",
    "## Evaluation Metrics\n",
    "\n",
    "| Metric | Purpose | Good Range |\n",
    "|--------|---------|------------|\n",
    "| ROUGE-L | Longest common subsequence overlap | > 0.15 |\n",
    "| BLEU-4 | N-gram precision | > 0.05 |\n",
    "| BERTScore | Semantic similarity | > 0.60 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tabulate import tabulate\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('punkt_tab', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"../models/flan-t5-socratic/final\")\n",
    "DATA_PATH = Path(\"../datasets/processed\")\n",
    "OUTPUT_PATH = Path(\"../evaluation_results\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "GENERATION_CONFIG = {\n",
    "    \"max_length\": 80,\n",
    "    \"num_beams\": 4,\n",
    "    \"do_sample\": True,\n",
    "    \"top_k\": 5,\n",
    "    \"top_p\": 0.6,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"no_repeat_ngram_size\": 3\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\n",
    "model = T5ForConditionalGeneration.from_pretrained(str(MODEL_PATH))\n",
    "model = model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.num_parameters():,} parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_formatted = pd.read_parquet(DATA_PATH / \"test_formatted.parquet\")\n",
    "print(f\"Test samples: {len(test_formatted)}\")\n",
    "test_formatted.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question(context, model, tokenizer, device, config):\n",
    "    \"\"\"Generate a Socratic question for the given context.\"\"\"\n",
    "    input_text = f\"Generate a Socratic question for this context: {context}\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=400, truncation=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, **config)\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated = generated.replace(\"[Question]\", \"\").strip()\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(\"Generating predictions...\")\n",
    "for idx, row in tqdm(test_formatted.iterrows(), total=len(test_formatted)):\n",
    "    pred = generate_question(\n",
    "        row['original_input'],\n",
    "        model,\n",
    "        tokenizer,\n",
    "        DEVICE,\n",
    "        GENERATION_CONFIG\n",
    "    )\n",
    "    predictions.append(pred)\n",
    "    ref = row['original_target'] if 'original_target' in row else row['target_text'].replace(\"[Question] \", \"\")\n",
    "    references.append(ref)\n",
    "\n",
    "test_formatted['prediction'] = predictions\n",
    "test_formatted['reference'] = references\n",
    "print(f\"Generated {len(predictions)} predictions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    scores = scorer.score(ref, pred)\n",
    "    for key in rouge_scores:\n",
    "        rouge_scores[key].append(scores[key].fmeasure)\n",
    "\n",
    "rouge_results = {\n",
    "    'ROUGE-1': np.mean(rouge_scores['rouge1']),\n",
    "    'ROUGE-2': np.mean(rouge_scores['rouge2']),\n",
    "    'ROUGE-L': np.mean(rouge_scores['rougeL'])\n",
    "}\n",
    "\n",
    "print(\"ROUGE Scores:\")\n",
    "for metric, score in rouge_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing = SmoothingFunction().method1\n",
    "bleu_scores = []\n",
    "\n",
    "for pred, ref in zip(predictions, references):\n",
    "    ref_tokens = nltk.word_tokenize(ref.lower())\n",
    "    pred_tokens = nltk.word_tokenize(pred.lower())\n",
    "    \n",
    "    bleu = sentence_bleu(\n",
    "        [ref_tokens],\n",
    "        pred_tokens,\n",
    "        weights=(0.25, 0.25, 0.25, 0.25),\n",
    "        smoothing_function=smoothing\n",
    "    )\n",
    "    bleu_scores.append(bleu)\n",
    "\n",
    "bleu_mean = np.mean(bleu_scores)\n",
    "print(f\"BLEU-4 Score: {bleu_mean:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTScore\n",
    "\n",
    "BERTScore measures semantic similarity using contextual embeddings. It is more meaningful than BLEU for tasks with many valid paraphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Computing BERTScore (this may take a few minutes)...\")\n",
    "\n",
    "P, R, F1 = bert_score(\n",
    "    predictions,\n",
    "    references,\n",
    "    lang=\"en\",\n",
    "    verbose=True,\n",
    "    device=DEVICE\n",
    ")\n",
    "\n",
    "bertscore_results = {\n",
    "    'Precision': P.mean().item(),\n",
    "    'Recall': R.mean().item(),\n",
    "    'F1': F1.mean().item()\n",
    "}\n",
    "\n",
    "print(\"\\nBERTScore Results:\")\n",
    "for metric, score in bertscore_results.items():\n",
    "    print(f\"  {metric}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = [\n",
    "    [\"ROUGE-1\", f\"{rouge_results['ROUGE-1']:.4f}\", \"> 0.20\"],\n",
    "    [\"ROUGE-2\", f\"{rouge_results['ROUGE-2']:.4f}\", \"> 0.05\"],\n",
    "    [\"ROUGE-L\", f\"{rouge_results['ROUGE-L']:.4f}\", \"> 0.15\"],\n",
    "    [\"BLEU-4\", f\"{bleu_mean:.4f}\", \"> 0.05\"],\n",
    "    [\"BERTScore F1\", f\"{bertscore_results['F1']:.4f}\", \"> 0.60\"]\n",
    "]\n",
    "\n",
    "print(\"\\nEvaluation Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(tabulate(summary_data, headers=[\"Metric\", \"Score\", \"Target\"], tablefmt=\"grid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "axes[0].hist(rouge_scores['rougeL'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].axvline(np.mean(rouge_scores['rougeL']), color='red', linestyle='--', label=f\"Mean: {np.mean(rouge_scores['rougeL']):.3f}\")\n",
    "axes[0].set_title('ROUGE-L Distribution')\n",
    "axes[0].set_xlabel('Score')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].hist(bleu_scores, bins=30, color='coral', alpha=0.7, edgecolor='black')\n",
    "axes[1].axvline(np.mean(bleu_scores), color='red', linestyle='--', label=f\"Mean: {np.mean(bleu_scores):.3f}\")\n",
    "axes[1].set_title('BLEU-4 Distribution')\n",
    "axes[1].set_xlabel('Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "\n",
    "axes[2].hist(F1.numpy(), bins=30, color='seagreen', alpha=0.7, edgecolor='black')\n",
    "axes[2].axvline(F1.mean().item(), color='red', linestyle='--', label=f\"Mean: {F1.mean().item():.3f}\")\n",
    "axes[2].set_title('BERTScore F1 Distribution')\n",
    "axes[2].set_xlabel('Score')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / \"score_distributions.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Predictions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_formatted['rougeL'] = rouge_scores['rougeL']\n",
    "test_formatted['bleu'] = bleu_scores\n",
    "test_formatted['bertscore_f1'] = F1.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HIGH SCORING EXAMPLES (Top 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "top_samples = test_formatted.nlargest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in top_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nLOW SCORING EXAMPLES (Bottom 5 by BERTScore)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "bottom_samples = test_formatted.nsmallest(5, 'bertscore_f1')\n",
    "\n",
    "for idx, row in bottom_samples.iterrows():\n",
    "    print(f\"\\nContext: {row['original_input'][:150]}...\")\n",
    "    print(f\"Reference: {row['reference']}\")\n",
    "    print(f\"Prediction: {row['prediction']}\")\n",
    "    print(f\"Scores - ROUGE-L: {row['rougeL']:.3f}, BERTScore: {row['bertscore_f1']:.3f}\")\n",
    "    print(\"-\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Human Evaluation Framework\n",
    "\n",
    "For comprehensive evaluation, sample predictions should be rated by humans on three criteria:\n",
    "\n",
    "1. **Fluency** (1-5): Is the question grammatically correct and natural?\n",
    "2. **Relevance** (1-5): Is the question relevant to the given context?\n",
    "3. **Socratic Quality** (Binary): Is the question genuinely thought-provoking and unanswerable from the context alone?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_sample = test_formatted.sample(50, random_state=42)[\n",
    "    ['original_input', 'reference', 'prediction', 'rougeL', 'bertscore_f1']\n",
    "].copy()\n",
    "\n",
    "human_eval_sample['fluency'] = None\n",
    "human_eval_sample['relevance'] = None\n",
    "human_eval_sample['is_socratic'] = None\n",
    "\n",
    "human_eval_sample = human_eval_sample.reset_index(drop=True)\n",
    "human_eval_sample.to_csv(OUTPUT_PATH / \"human_evaluation_samples.csv\", index=False)\n",
    "\n",
    "print(f\"Saved {len(human_eval_sample)} samples for human evaluation.\")\n",
    "print(f\"File: {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Full Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "evaluation_results = {\n",
    "    \"test_samples\": len(test_formatted),\n",
    "    \"rouge\": rouge_results,\n",
    "    \"bleu4\": float(bleu_mean),\n",
    "    \"bertscore\": bertscore_results,\n",
    "    \"generation_config\": GENERATION_CONFIG\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / \"evaluation_metrics.json\", \"w\") as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "test_formatted.to_csv(OUTPUT_PATH / \"test_predictions.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(f\"  - {OUTPUT_PATH / 'evaluation_metrics.json'}\")\n",
    "print(f\"  - {OUTPUT_PATH / 'test_predictions.csv'}\")\n",
    "print(f\"  - {OUTPUT_PATH / 'score_distributions.png'}\")\n",
    "print(f\"  - {OUTPUT_PATH / 'human_evaluation_samples.csv'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df = test_formatted[['rougeL', 'bleu', 'bertscore_f1']].copy()\n",
    "metrics_df.columns = ['ROUGE-L', 'BLEU-4', 'BERTScore']\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "correlation = metrics_df.corr()\n",
    "sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0, fmt='.3f')\n",
    "plt.title('Metric Correlations')\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUTPUT_PATH / \"metric_correlations.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Complete!\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. BERTScore is more reliable than BLEU for Socratic questions (many valid paraphrases)\n",
    "2. Low ROUGE/BLEU with high BERTScore indicates semantically similar but differently worded questions\n",
    "3. Human evaluation is essential for assessing true \"Socratic\" quality\n",
    "\n",
    "---\n",
    "\n",
    "**Next Step**: Proceed to `05_keybert_extraction.ipynb` to set up keyphrase extraction for the concept map."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
