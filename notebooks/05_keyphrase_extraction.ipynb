{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Keyphrase Extraction\n",
    "\n",
    "This notebook implements keyphrase extraction for the concept map visualization and context retrieval.\n",
    "\n",
    "## Objectives\n",
    "- Implement TF-IDF based keyphrase extraction (baseline)\n",
    "- Fine-tune a KeyBERT model on KP20k (optional, advanced)\n",
    "- Test extraction on sample contexts\n",
    "- Integrate with the retrieval pipeline\n",
    "\n",
    "## Why Keyphrase Extraction?\n",
    "Keyphrases are used for:\n",
    "1. **Concept Map Visualization** - Nodes in React Flow represent extracted concepts\n",
    "2. **Wikipedia/Gemini Retrieval** - Query terms for context augmentation\n",
    "3. **Question Focusing** - Generate questions about specific concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "OUTPUT_DIR = Path(\"../backend/keyphrase_artifacts\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_contexts = [\n",
    "    \"Climate change is causing significant shifts in global weather patterns. Rising temperatures lead to melting ice caps, rising sea levels, and more frequent extreme weather events. Scientists argue that human activities, particularly the burning of fossil fuels, are the primary drivers of these changes.\",\n",
    "    \n",
    "    \"Machine learning algorithms can be broadly categorized into supervised learning, unsupervised learning, and reinforcement learning. Neural networks, a subset of machine learning, have revolutionized fields like computer vision and natural language processing.\",\n",
    "    \n",
    "    \"The French Revolution of 1789 fundamentally transformed French society. The abolition of feudalism, the Declaration of the Rights of Man, and the rise of Napoleon Bonaparte marked significant turning points in European history.\",\n",
    "    \n",
    "    \"Photosynthesis is the process by which plants convert sunlight, water, and carbon dioxide into glucose and oxygen. This process occurs primarily in the chloroplasts of plant cells and is essential for life on Earth.\",\n",
    "    \n",
    "    \"Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to perform computations. Unlike classical bits that are either 0 or 1, quantum bits or qubits can exist in multiple states simultaneously.\"\n",
    "]\n",
    "\n",
    "print(f\"Loaded {len(test_contexts)} sample contexts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. TF-IDF Keyphrase Extractor (Baseline)\n",
    "\n",
    "A simple but effective approach using TF-IDF scores and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDFKeyphraseExtractor:\n",
    "    \"\"\"Extract keyphrases using TF-IDF scoring.\"\"\"\n",
    "    \n",
    "    def __init__(self, ngram_range=(1, 3), top_n=10):\n",
    "        self.ngram_range = ngram_range\n",
    "        self.top_n = top_n\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.vectorizer = TfidfVectorizer(\n",
    "            ngram_range=ngram_range,\n",
    "            stop_words='english',\n",
    "            max_features=1000\n",
    "        )\n",
    "        self.stemmer = PorterStemmer()\n",
    "    \n",
    "    def preprocess(self, text):\n",
    "        \"\"\"Clean and normalize text.\"\"\"\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        return text\n",
    "    \n",
    "    def extract(self, text, top_n=None):\n",
    "        \"\"\"Extract top keyphrases from text.\"\"\"\n",
    "        if top_n is None:\n",
    "            top_n = self.top_n\n",
    "        \n",
    "        processed = self.preprocess(text)\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = self.vectorizer.fit_transform([processed])\n",
    "            feature_names = self.vectorizer.get_feature_names_out()\n",
    "            scores = tfidf_matrix.toarray()[0]\n",
    "            \n",
    "            scored_phrases = list(zip(feature_names, scores))\n",
    "            scored_phrases.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            keyphrases = []\n",
    "            seen_stems = set()\n",
    "            \n",
    "            for phrase, score in scored_phrases:\n",
    "                stem = ' '.join([self.stemmer.stem(w) for w in phrase.split()])\n",
    "                if stem not in seen_stems and score > 0:\n",
    "                    keyphrases.append({'text': phrase, 'score': float(score)})\n",
    "                    seen_stems.add(stem)\n",
    "                if len(keyphrases) >= top_n:\n",
    "                    break\n",
    "            \n",
    "            return keyphrases\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting keyphrases: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_extractor = TFIDFKeyphraseExtractor(ngram_range=(1, 2), top_n=5)\n",
    "\n",
    "print(\"TF-IDF Keyphrase Extraction Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, context in enumerate(test_contexts):\n",
    "    keyphrases = tfidf_extractor.extract(context)\n",
    "    print(f\"\\nContext {i+1}: {context[:80]}...\")\n",
    "    print(\"Keyphrases:\")\n",
    "    for kp in keyphrases:\n",
    "        print(f\"  - {kp['text']} (score: {kp['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. KeyBERT-based Extractor (Advanced)\n",
    "\n",
    "Uses sentence embeddings for better semantic understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class KeyBERTExtractor:\n",
    "    \"\"\"Extract keyphrases using sentence embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', ngram_range=(1, 2), top_n=5):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.top_n = top_n\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def get_candidates(self, text):\n",
    "        \"\"\"Extract candidate phrases from text.\"\"\"\n",
    "        words = word_tokenize(text.lower())\n",
    "        candidates = []\n",
    "        \n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                phrase = ' '.join(words[i:i+n])\n",
    "                phrase_words = phrase.split()\n",
    "                if not any(w in self.stop_words for w in phrase_words):\n",
    "                    if all(w.isalpha() for w in phrase_words):\n",
    "                        candidates.append(phrase)\n",
    "        \n",
    "        return list(set(candidates))\n",
    "    \n",
    "    def extract(self, text, top_n=None):\n",
    "        \"\"\"Extract keyphrases using embedding similarity.\"\"\"\n",
    "        if top_n is None:\n",
    "            top_n = self.top_n\n",
    "        \n",
    "        candidates = self.get_candidates(text)\n",
    "        if not candidates:\n",
    "            return []\n",
    "        \n",
    "        doc_embedding = self.model.encode([text])\n",
    "        candidate_embeddings = self.model.encode(candidates)\n",
    "        \n",
    "        similarities = cosine_similarity(doc_embedding, candidate_embeddings)[0]\n",
    "        \n",
    "        scored = list(zip(candidates, similarities))\n",
    "        scored.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        keyphrases = []\n",
    "        for phrase, score in scored[:top_n]:\n",
    "            keyphrases.append({'text': phrase, 'score': float(score)})\n",
    "        \n",
    "        return keyphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keybert_extractor = KeyBERTExtractor(ngram_range=(1, 2), top_n=5)\n",
    "\n",
    "print(\"KeyBERT Keyphrase Extraction Results:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for i, context in enumerate(test_contexts):\n",
    "    keyphrases = keybert_extractor.extract(context)\n",
    "    print(f\"\\nContext {i+1}: {context[:80]}...\")\n",
    "    print(\"Keyphrases:\")\n",
    "    for kp in keyphrases:\n",
    "        print(f\"  - {kp['text']} (score: {kp['score']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load and Explore KP20k Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "kp20k = load_dataset(\"midas/kp20k\", trust_remote_code=True)\n",
    "\n",
    "print(f\"KP20k dataset:\")\n",
    "print(f\"  Train: {len(kp20k['train'])} samples\")\n",
    "print(f\"  Validation: {len(kp20k['validation'])} samples\")\n",
    "print(f\"  Test: {len(kp20k['test'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = kp20k['train'][0]\n",
    "print(\"\\nSample entry:\")\n",
    "print(f\"Document: {sample['document'][:300]}...\")\n",
    "print(f\"\\nExtractive keyphrases: {sample['extractive_keyphrases']}\")\n",
    "print(f\"Abstractive keyphrases: {sample['abstractive_keyphrases']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate Extractors on KP20k\n",
    "\n",
    "Compare TF-IDF and KeyBERT against ground truth keyphrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_extraction(extractor, samples, k=5):\n",
    "    \"\"\"Evaluate keyphrase extraction precision and recall.\"\"\"\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1s = []\n",
    "    \n",
    "    for sample in tqdm(samples, desc=\"Evaluating\"):\n",
    "        doc = sample['document']\n",
    "        true_kps = set([kp.lower() for kp in sample['extractive_keyphrases']])\n",
    "        \n",
    "        if not true_kps or not doc:\n",
    "            continue\n",
    "        \n",
    "        predicted = extractor.extract(doc, top_n=k)\n",
    "        pred_kps = set([kp['text'].lower() for kp in predicted])\n",
    "        \n",
    "        if not pred_kps:\n",
    "            continue\n",
    "        \n",
    "        overlap = len(pred_kps & true_kps)\n",
    "        precision = overlap / len(pred_kps) if pred_kps else 0\n",
    "        recall = overlap / len(true_kps) if true_kps else 0\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1s.append(f1)\n",
    "    \n",
    "    return {\n",
    "        'precision': np.mean(precisions),\n",
    "        'recall': np.mean(recalls),\n",
    "        'f1': np.mean(f1s)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_samples = kp20k['test'].select(range(min(500, len(kp20k['test']))))\n",
    "\n",
    "print(\"Evaluating TF-IDF extractor...\")\n",
    "tfidf_results = evaluate_extraction(tfidf_extractor, eval_samples, k=5)\n",
    "print(f\"TF-IDF Results:\")\n",
    "print(f\"  Precision@5: {tfidf_results['precision']:.4f}\")\n",
    "print(f\"  Recall@5: {tfidf_results['recall']:.4f}\")\n",
    "print(f\"  F1@5: {tfidf_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluating KeyBERT extractor...\")\n",
    "keybert_results = evaluate_extraction(keybert_extractor, eval_samples, k=5)\n",
    "print(f\"KeyBERT Results:\")\n",
    "print(f\"  Precision@5: {keybert_results['precision']:.4f}\")\n",
    "print(f\"  Recall@5: {keybert_results['recall']:.4f}\")\n",
    "print(f\"  F1@5: {keybert_results['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Production Extractor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KeyphraseExtractor:\n",
    "    \"\"\"Production keyphrase extractor with fallback.\"\"\"\n",
    "    \n",
    "    def __init__(self, use_keybert=True, model_name='all-MiniLM-L6-v2'):\n",
    "        self.use_keybert = use_keybert\n",
    "        \n",
    "        if use_keybert:\n",
    "            try:\n",
    "                self.extractor = KeyBERTExtractor(model_name=model_name)\n",
    "                print(\"Using KeyBERT extractor\")\n",
    "            except Exception as e:\n",
    "                print(f\"KeyBERT failed, falling back to TF-IDF: {e}\")\n",
    "                self.extractor = TFIDFKeyphraseExtractor()\n",
    "                self.use_keybert = False\n",
    "        else:\n",
    "            self.extractor = TFIDFKeyphraseExtractor()\n",
    "            print(\"Using TF-IDF extractor\")\n",
    "    \n",
    "    def extract(self, text, top_n=5):\n",
    "        \"\"\"Extract keyphrases from text.\"\"\"\n",
    "        try:\n",
    "            return self.extractor.extract(text, top_n=top_n)\n",
    "        except Exception as e:\n",
    "            print(f\"Extraction error: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def batch_extract(self, texts, top_n=5):\n",
    "        \"\"\"Extract keyphrases from multiple texts.\"\"\"\n",
    "        results = []\n",
    "        for text in tqdm(texts, desc=\"Extracting\"):\n",
    "            results.append(self.extract(text, top_n=top_n))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = KeyphraseExtractor(use_keybert=True)\n",
    "\n",
    "test_text = \"Artificial intelligence and machine learning are transforming healthcare through improved diagnostics and personalized treatment.\"\n",
    "keyphrases = extractor.extract(test_text, top_n=5)\n",
    "\n",
    "print(f\"\\nTest extraction:\")\n",
    "print(f\"Input: {test_text}\")\n",
    "print(f\"Keyphrases: {[kp['text'] for kp in keyphrases]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Extractor for Backend Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "\n",
    "extractor_config = {\n",
    "    \"use_keybert\": True,\n",
    "    \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "    \"default_top_n\": 5,\n",
    "    \"ngram_range\": [1, 2],\n",
    "    \"tfidf_evaluation\": tfidf_results,\n",
    "    \"keybert_evaluation\": keybert_results\n",
    "}\n",
    "\n",
    "with open(OUTPUT_DIR / \"extractor_config.json\", \"w\") as f:\n",
    "    json.dump(extractor_config, f, indent=2)\n",
    "\n",
    "print(f\"Configuration saved to {OUTPUT_DIR / 'extractor_config.json'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor_code = '''\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class KeyphraseExtractor:\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2', ngram_range=(1, 2), top_n=5):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "        self.ngram_range = ngram_range\n",
    "        self.top_n = top_n\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def get_candidates(self, text):\n",
    "        words = word_tokenize(text.lower())\n",
    "        candidates = []\n",
    "        for n in range(self.ngram_range[0], self.ngram_range[1] + 1):\n",
    "            for i in range(len(words) - n + 1):\n",
    "                phrase = ' '.join(words[i:i+n])\n",
    "                phrase_words = phrase.split()\n",
    "                if not any(w in self.stop_words for w in phrase_words):\n",
    "                    if all(w.isalpha() for w in phrase_words):\n",
    "                        candidates.append(phrase)\n",
    "        return list(set(candidates))\n",
    "    \n",
    "    def extract(self, text, top_n=None):\n",
    "        if top_n is None:\n",
    "            top_n = self.top_n\n",
    "        candidates = self.get_candidates(text)\n",
    "        if not candidates:\n",
    "            return []\n",
    "        doc_embedding = self.model.encode([text])\n",
    "        candidate_embeddings = self.model.encode(candidates)\n",
    "        similarities = cosine_similarity(doc_embedding, candidate_embeddings)[0]\n",
    "        scored = sorted(zip(candidates, similarities), key=lambda x: x[1], reverse=True)\n",
    "        return [{\"text\": phrase, \"score\": float(score)} for phrase, score in scored[:top_n]]\n",
    "'''\n",
    "\n",
    "with open(OUTPUT_DIR / \"keyphrase_extractor.py\", \"w\") as f:\n",
    "    f.write(extractor_code)\n",
    "\n",
    "print(f\"Extractor module saved to {OUTPUT_DIR / 'keyphrase_extractor.py'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Integration Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_concept_map(text, extractor, top_n=8):\n",
    "    \"\"\"Prepare keyphrase data for React Flow concept map.\"\"\"\n",
    "    keyphrases = extractor.extract(text, top_n=top_n)\n",
    "    \n",
    "    nodes = [\n",
    "        {\n",
    "            \"id\": \"main\",\n",
    "            \"type\": \"topic\",\n",
    "            \"data\": {\"label\": \"Main Topic\"},\n",
    "            \"position\": {\"x\": 400, \"y\": 300}\n",
    "        }\n",
    "    ]\n",
    "    edges = []\n",
    "    \n",
    "    for i, kp in enumerate(keyphrases):\n",
    "        angle = (2 * np.pi * i) / len(keyphrases)\n",
    "        radius = 200\n",
    "        x = 400 + radius * np.cos(angle)\n",
    "        y = 300 + radius * np.sin(angle)\n",
    "        \n",
    "        node_id = f\"kp_{i}\"\n",
    "        nodes.append({\n",
    "            \"id\": node_id,\n",
    "            \"type\": \"keyphrase\",\n",
    "            \"data\": {\n",
    "                \"label\": kp['text'],\n",
    "                \"score\": kp['score']\n",
    "            },\n",
    "            \"position\": {\"x\": x, \"y\": y}\n",
    "        })\n",
    "        \n",
    "        edges.append({\n",
    "            \"id\": f\"edge_{i}\",\n",
    "            \"source\": \"main\",\n",
    "            \"target\": node_id,\n",
    "            \"animated\": kp['score'] > 0.5\n",
    "        })\n",
    "    \n",
    "    return {\"nodes\": nodes, \"edges\": edges}\n",
    "\n",
    "concept_data = prepare_for_concept_map(test_contexts[0], extractor)\n",
    "print(\"Concept map data structure:\")\n",
    "print(json.dumps(concept_data, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Summary\n",
    "\n",
    "### Implemented\n",
    "- TF-IDF based keyphrase extraction (baseline)\n",
    "- KeyBERT embedding-based extraction (advanced)\n",
    "- Evaluation on KP20k dataset\n",
    "- Production-ready extractor class\n",
    "- Concept map data preparation\n",
    "\n",
    "### Performance Comparison\n",
    "| Method | Precision@5 | Recall@5 | F1@5 |\n",
    "|--------|------------|----------|------|\n",
    "| TF-IDF | See above | See above | See above |\n",
    "| KeyBERT | See above | See above | See above |\n",
    "\n",
    "### Next Steps\n",
    "1. **06_vector_store_setup.ipynb** - Build ChromaDB index with extracted keyphrases\n",
    "2. Integrate extractor with FastAPI backend\n",
    "3. Connect to React Flow visualization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
