{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "\n# 07 \u2014 Complete Inference Pipeline\n#\n# This notebook integrates all components into the SocraticPath inference\n# pipeline.  It differs from the original in three ways:\n#\n# 1. Gemini API removed \u2014 replaced with Wikipedia API only.\n#    Rationale: Gemini responses are non-deterministic (same query \u2192 different\n#    answers each run).  This makes the system non-reproducible, which is\n#    unacceptable for academic evaluation.  Wikipedia is deterministic, free,\n#    requires no API key, and is the same source used by the SOQG paper for\n#    context enrichment.\n#\n# 2. Correct model loading order (fixes embedding-mismatch RuntimeError).\n#    Sequence: load tokenizer \u2192 load base model \u2192 resize embeddings \u2192 load adapter.\n#\n# 3. Inference prompt matches training format.\n#    Training format: \"Generate a Socratic question for this context:\n#                      {question_type}: {context}\"\n#    For user free-text (no known type): default to \"reasons_evidence\" (most\n#    common type, 35% of SocratiQ training set).\n#\n# Pipeline flow:\n#   User Input \u2192 KeyBERT (keyphrases) \u2192 Wikipedia (context) \u2192 FLAN-T5 \u2192 Question\n#        \u2193              \u2193                       \u2193                   \u2193\n#    Context       Concept nodes          Enriched prompt       Response\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nimport os\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass, field, asdict\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\nfrom keybert import KeyBERT\nimport wikipediaapi\n\n# Rationale: deterministic, no API key required, academically reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# \u2500\u2500 Configuration \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\nMODEL_PATH = Path(\"../models/flan-t5-socratic-lora/adapter\")\nMODEL_NAME = \"google/flan-t5-small\"   # matches the trained adapter   # base model name (used for loading)\nDEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Default question type used when the user enters free-form text.\n# \"reasons_evidence\" is the most common type in SocratiQ (35% of training data).\nDEFAULT_QUESTION_TYPE = \"reasons_evidence\"\n\n# Generation configs \u2014 see 04_evaluation.ipynb for rationale.\nEVAL_GENERATION_CONFIG = dict(max_length=80, num_beams=4, do_sample=False)\nDEMO_GENERATION_CONFIG = dict(\n    max_length=80, num_beams=2, do_sample=True,\n    temperature=0.8, top_p=0.9,\n    repetition_penalty=1.2, no_repeat_ngram_size=3,\n)\n\nprint(f\"Model path : {MODEL_PATH}\")\nprint(f\"Device     : {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Keyphrase:\n",
    "    phrase: str\n",
    "    score: float\n",
    "    source: str = \"input\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievedContext:\n",
    "    keyphrase: str\n",
    "    context: str\n",
    "    source: str\n",
    "    url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConceptNode:\n",
    "    id: str\n",
    "    label: str\n",
    "    node_type: str\n",
    "    score: float = 0.0\n",
    "    sources: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResponse:\n",
    "    user_input: str\n",
    "    socratic_question: str\n",
    "    keyphrases: List[Keyphrase]\n",
    "    retrieved_contexts: List[RetrievedContext]\n",
    "    concept_nodes: List[ConceptNode]\n",
    "    processing_time_ms: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Load Components \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n# 1. FLAN-T5 + LoRA adapter \u2014 correct load sequence\nprint(\"Loading FLAN-T5 + LoRA adapter...\")\ntokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\nbase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\nbase_model.resize_token_embeddings(len(tokenizer))   # resize BEFORE adapter load\nmodel = PeftModel.from_pretrained(base_model, str(MODEL_PATH))\nmodel = model.to(DEVICE)\nmodel.eval()\nprint(f\"  \u2713 FLAN-T5 loaded ({model.num_parameters():,} params, vocab={len(tokenizer)})\")\n\n# 2. KeyBERT for keyphrase extraction (concept map nodes)\nprint(\"\\nLoading KeyBERT (all-MiniLM-L6-v2)...\")\nkw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\nprint(\"  \u2713 KeyBERT loaded\")\n\n# 3. Wikipedia API for deterministic context retrieval\nprint(\"\\nInitialising Wikipedia API...\")\nwiki = wikipediaapi.Wikipedia(\n    user_agent=\"SocraticPath/1.0 (dissertation; contact: anuhas0123@gmail.com)\",\n    language=\"en\",\n)\nprint(\"  \u2713 Wikipedia API ready\")\nprint(\"\\nAll components loaded.\")\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Pipeline Functions \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\ndef extract_keyphrases(text: str, top_n: int = 5) -> List[dict]:\n    \"\"\"Extract keyphrases with KeyBERT (MMR for diversity).\"\"\"\n    if not text or len(text.strip()) < 10:\n        return []\n    keywords = kw_model.extract_keywords(\n        text,\n        keyphrase_ngram_range=(1, 2),\n        stop_words=\"english\",\n        top_n=top_n,\n        use_mmr=True,\n        diversity=0.5,\n    )\n    return [{\"phrase\": kw, \"score\": float(score)} for kw, score in keywords]\n\n\ndef retrieve_context_wikipedia(keyphrase: str) -> Optional[Dict]:\n    \"\"\"\n    Retrieve a short factual summary from Wikipedia for a keyphrase.\n\n    Returns a dict with 'summary' and 'url', or None if no page found.\n    Wikipedia is used in preference to an LLM API because:\n      - Responses are deterministic (same query \u2192 same answer every run).\n      - No API key or cost required.\n      - Content is stable and citable.\n      - Matches the SOQG paper's context enrichment approach.\n    \"\"\"\n    try:\n        page = wiki.page(keyphrase)\n        if page.exists():\n            # Take up to ~3 sentences (400 chars) of the summary\n            summary = page.summary[:400]\n            last_period = summary.rfind(\".\")\n            if last_period > 150:\n                summary = summary[: last_period + 1]\n            return {\"summary\": summary, \"url\": page.fullurl, \"title\": page.title}\n    except Exception as exc:\n        print(f\"  Wikipedia lookup failed for '{keyphrase}': {exc}\")\n    return None\n\n\ndef retrieve_contexts(keyphrases: List[dict]) -> List[dict]:\n    \"\"\"Look up Wikipedia context for the top-3 keyphrases.\"\"\"\n    results = []\n    for kp in keyphrases[:3]:       # limit to 3 to keep prompt length manageable\n        result = retrieve_context_wikipedia(kp[\"phrase\"])\n        if result:\n            results.append({\n                \"keyphrase\": kp[\"phrase\"],\n                \"context\": result[\"summary\"],\n                \"source\": \"wikipedia\",\n                \"url\": result[\"url\"],\n            })\n    return results\n\n\ndef generate_socratic_question(\n    user_input: str,\n    question_type: str = DEFAULT_QUESTION_TYPE,\n    retrieved_context: str = \"\",\n    use_sampling: bool = False,\n) -> str:\n    \"\"\"\n    Generate a Socratic question with the fine-tuned FLAN-T5.\n\n    The prompt format mirrors the training data exactly:\n      \"Generate a Socratic question for this context: {type}: {context}\"\n    If retrieved_context is provided it is appended (truncated to 500 chars)\n    after a newline so the encoder can attend to external knowledge.\n    \"\"\"\n    prompt = (\n        f\"Generate a Socratic question for this context: \"\n        f\"{question_type}: {user_input}\"\n    )\n    if retrieved_context:\n        prompt += f\"\\n\\nAdditional context: {retrieved_context[:500]}\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True)\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n\n    gen_cfg = DEMO_GENERATION_CONFIG if use_sampling else EVAL_GENERATION_CONFIG\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **gen_cfg)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated.replace(\"[Question]\", \"\").strip()\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# \u2500\u2500 Complete Pipeline Function \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\ndef run_pipeline(\n    user_input: str,\n    question_type: str = DEFAULT_QUESTION_TYPE,\n    use_retrieval: bool = True,\n    use_sampling: bool = False,        # False = deterministic (for demos & eval)\n) -> dict:\n    \"\"\"\n    End-to-end SocraticPath pipeline.\n\n    Args:\n        user_input:     Raw text from the user (opinion/argument).\n        question_type:  SocratiQ category prefix.  Default: \"reasons_evidence\".\n                        Valid values: \"reasons_evidence\", \"clarity\",\n                        \"implication_consequences\", \"alternate_viewpoints_perspectives\",\n                        \"assumptions\".\n        use_retrieval:  Whether to query Wikipedia for context enrichment.\n        use_sampling:   True for demo/frontend (varied outputs);\n                        False for evaluation (deterministic ROUGE).\n\n    Returns:\n        dict with keys: user_input, question_type, socratic_question,\n        keyphrases, retrieved_contexts, concept_nodes, processing_time_ms.\n    \"\"\"\n    t0 = time.time()\n\n    # Step 1: Extract keyphrases \u2192 concept map nodes\n    keyphrases = extract_keyphrases(user_input, top_n=5)\n\n    # Step 2: Retrieve Wikipedia context for top keyphrases\n    contexts = []\n    combined_context = \"\"\n    if use_retrieval and keyphrases:\n        contexts = retrieve_contexts(keyphrases)\n        combined_context = \" \".join(c[\"context\"] for c in contexts)\n\n    # Step 3: Generate Socratic question\n    socratic_question = generate_socratic_question(\n        user_input,\n        question_type=question_type,\n        retrieved_context=combined_context,\n        use_sampling=use_sampling,\n    )\n\n    # Step 4: Build concept map node list for React Flow\n    concept_nodes = [\n        {\"id\": \"user_input\", \"type\": \"input\",    \"label\": \"User Input\",       \"score\": 1.0},\n        {\"id\": \"sq\",          \"type\": \"question\", \"label\": socratic_question[:60] + \"\u2026\"\n                                                  if len(socratic_question) > 60\n                                                  else socratic_question,    \"score\": 1.0},\n    ] + [\n        {\n            \"id\": f\"concept_{i}\",\n            \"type\": \"concept\",\n            \"label\": kp[\"phrase\"],\n            \"score\": kp[\"score\"],\n            \"has_context\": any(c[\"keyphrase\"].lower() == kp[\"phrase\"].lower()\n                               for c in contexts),\n        }\n        for i, kp in enumerate(keyphrases)\n    ]\n\n    return {\n        \"user_input\": user_input,\n        \"question_type\": question_type,\n        \"socratic_question\": socratic_question,\n        \"keyphrases\": keyphrases,\n        \"retrieved_contexts\": contexts,\n        \"concept_nodes\": concept_nodes,\n        \"processing_time_ms\": (time.time() - t0) * 1000,\n    }\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n    \"I believe that social media is harmful to teenagers and should be banned for anyone under 18.\",\n    \"Climate change is exaggerated by scientists who want more research funding.\",\n    \"Artificial intelligence will make most human jobs obsolete within the next decade.\"\n]\n\nfor i, user_input in enumerate(test_inputs, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"TEST {i}\")\n    print(f\"{'='*70}\")\n    \n    response = run_pipeline(user_input)\n    \n    print(f\"\\nUser Input:\\n  {response[\"user_input\"]}\")\n    print(f\"\\nSocratic Question:\\n  {response[\"socratic_question\"]}\")\n    print(f\"\\nKeyphrases:\")\n    for kp in response[\"keyphrases\"]:\n        print(f\"  - {kp[\"phrase\"]} ({kp[\"score\"]:.3f})\")\n    print(f\"\\nRetrieved Contexts: {len(response[\"retrieved_contexts\"])}\")\n    for ctx in response[\"retrieved_contexts\"]:\n        print(f\"  [{ctx[\"source\"]}] {ctx[\"keyphrase\"]}: {ctx[\"context\"][:100]}...\")\n    print(f\"\\nConcept Nodes: {len(response[\"concept_nodes\"])}\")\n    print(f\"Processing Time: {response[\"processing_time_ms\"]:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_widget = widgets.Textarea(\n    value='',\n    placeholder='Enter your opinion or statement here...',\n    description='Input:',\n    layout=widgets.Layout(width='100%', height='100px')\n)\n\nretrieval_checkbox = widgets.Checkbox(\n    value=True,\n    description='Use Context Retrieval',\n    indent=False\n)\n\nsubmit_button = widgets.Button(\n    description='Generate Socratic Question',\n    button_style='primary',\n    icon='question'\n)\n\noutput_area = widgets.Output()\n\ndef on_submit(b):\n    with output_area:\n        clear_output()\n        user_input = input_widget.value.strip()\n        \n        if not user_input:\n            print(\"Please enter a statement or opinion.\")\n            return\n        \n        print(\"Processing...\")\n        \n        response = run_pipeline(user_input, use_retrieval=retrieval_checkbox.value)\n        \n        clear_output()\n        \n        html_output = f\"\"\"\n        <div style=\"font-family: Arial, sans-serif; max-width: 800px;\">\n            <h3 style=\"color: #1a73e8;\">\ud83e\udd14 Socratic Question</h3>\n            <div style=\"background: #e8f0fe; padding: 15px; border-radius: 8px; margin-bottom: 20px;\">\n                <strong>{response[\"socratic_question\"]}</strong>\n            </div>\n            \n            <h4>\ud83d\udccc Key Concepts</h4>\n            <div style=\"display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 20px;\">\n        \"\"\"\n        \n        for kp in response[\"keyphrases\"]:\n            html_output += f'<span style=\"background: #f1f3f4; padding: 4px 12px; border-radius: 16px; font-size: 14px;\">{kp[\"phrase\"]}</span>'\n        \n        html_output += \"</div>\"\n        \n        if response[\"retrieved_contexts\"]:\n            html_output += \"<h4>\ud83d\udcda Retrieved Context</h4>\"\n            for ctx in response[\"retrieved_contexts\"]:\n                source_badge = \"\ud83e\udd16 Gemini\" if ctx[\"source\"] == \"gemini\" else \"\ud83d\udcd6 Wikipedia\"\n                html_output += f\"\"\"\n                <div style=\"background: #fafafa; padding: 10px; border-radius: 8px; margin-bottom: 10px; border-left: 3px solid #4285f4;\">\n                    <strong>{ctx[\"keyphrase\"]}</strong> <span style=\"font-size: 12px; color: #666;\">{source_badge}</span>\n                    <p style=\"margin: 5px 0 0 0; font-size: 14px; color: #444;\">{ctx[\"context\"][:200]}...</p>\n                </div>\n                \"\"\"\n        \n        html_output += f\"\"\"\n            <p style=\"font-size: 12px; color: #666; margin-top: 20px;\">\n                \u23f1\ufe0f Processing time: {response[\"processing_time_ms\"]:.0f}ms\n            </p>\n        </div>\n        \"\"\"\n        \n        display(HTML(html_output))\n\nsubmit_button.on_click(on_submit)\n\ndisplay(widgets.VBox([\n    widgets.HTML(\"<h2>\ud83c\udf93 SocraticPath Demo</h2>\"),\n    input_widget,\n    widgets.HBox([retrieval_checkbox, submit_button]),\n    output_area\n]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pipeline for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_dict(response: dict) -> dict:\n    \"\"\"\n    Return the pipeline response as a JSON-serializable dict.\n\n    run_pipeline() already returns a plain dict, so this function\n    is a passthrough with type coercion for any numpy/tensor values.\n    It exists as a named interface so the FastAPI backend can call it\n    uniformly without needing to know the internal representation.\n    \"\"\"\n    import numpy as np\n    def _coerce(v):\n        if isinstance(v, (np.floating, np.integer)):\n            return float(v)\n        return v\n\n    return {\n        \"user_input\": response[\"user_input\"],\n        \"question_type\": response.get(\"question_type\", \"reasons_evidence\"),\n        \"socratic_question\": response[\"socratic_question\"],\n        \"keyphrases\": [\n            {\"phrase\": kp[\"phrase\"], \"score\": _coerce(kp[\"score\"])}\n            for kp in response[\"keyphrases\"]\n        ],\n        \"retrieved_contexts\": [\n            {\n                \"keyphrase\": ctx[\"keyphrase\"],\n                \"context\": ctx[\"context\"],\n                \"source\": ctx[\"source\"],\n                \"url\": ctx.get(\"url\"),\n            }\n            for ctx in response[\"retrieved_contexts\"]\n        ],\n        \"concept_nodes\": response[\"concept_nodes\"],\n        \"processing_time_ms\": _coerce(response[\"processing_time_ms\"]),\n    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response = run_pipeline(\n",
    "    \"Universal basic income is necessary because automation will eliminate most jobs.\"\n",
    ")\n",
    "\n",
    "api_response = pipeline_to_dict(sample_response)\n",
    "print(\"API Response Format:\")\n",
    "print(json.dumps(api_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../models/pipeline_config\")\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nconfig = {\n    \"model\": {\n        \"name\": MODEL_NAME,\n        \"adapter_path\": str(MODEL_PATH),\n        \"type\": \"flan-t5 + lora\",\n        \"generation_eval\": EVAL_GENERATION_CONFIG,\n        \"generation_demo\": DEMO_GENERATION_CONFIG,\n    },\n    \"keybert\": {\n        \"model\": \"all-MiniLM-L6-v2\",\n        \"top_n\": 5,\n        \"ngram_range\": [1, 2],\n        \"diversity\": 0.5,\n    },\n    \"retrieval\": {\n        \"backend\": \"wikipedia\",\n        \"max_keyphrases_for_retrieval\": 3,\n        \"note\": \"Wikipedia used for deterministic, reproducible context retrieval.\",\n    },\n}\n\nimport json as _json\nwith open(OUTPUT_PATH / \"config.json\", \"w\") as f:\n    _json.dump(config, f, indent=2)\n\nprint(f\"\u2713 Pipeline configuration saved to {OUTPUT_PATH / 'config.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "test_statements = [\n",
    "    \"I think video games make children violent.\",\n",
    "    \"We should abolish the electoral college.\",\n",
    "    \"Space exploration is a waste of money.\",\n",
    "    \"Nuclear energy is too dangerous to use.\",\n",
    "    \"Social media should be regulated by the government.\"\n",
    "]\n",
    "\n",
    "print(\"Running performance benchmark...\\n\")\n",
    "\n",
    "times = []\n",
    "for stmt in test_statements:\n",
    "    response = run_pipeline(stmt)\n",
    "    times.append(response.processing_time_ms)\n",
    "    print(f\"\u2713 {stmt[:50]}... ({response.processing_time_ms:.0f}ms)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Performance Summary:\")\n",
    "print(f\"  Average: {statistics.mean(times):.0f}ms\")\n",
    "print(f\"  Median: {statistics.median(times):.0f}ms\")\n",
    "print(f\"  Min: {min(times):.0f}ms\")\n",
    "print(f\"  Max: {max(times):.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete!\n",
    "\n",
    "**Components:**\n",
    "1. \u2705 FLAN-T5 Socratic Question Generation\n",
    "2. \u2705 KeyBERT Keyphrase Extraction\n",
    "3. \u2705 Gemini + Wikipedia Context Retrieval\n",
    "4. \u2705 Concept Node Generation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy as FastAPI backend\n",
    "2. Build React Flow frontend\n",
    "3. Add concept map visualization\n",
    "4. Implement user session management\n",
    "\n",
    "---\n",
    "\n",
    "**Files Created:**\n",
    "- `../models/pipeline_config/config.json` - Pipeline configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}