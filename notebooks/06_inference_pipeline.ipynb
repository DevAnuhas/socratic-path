{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# 06 — Inference Pipeline\n\nThis notebook assembles the three trained components — KeyBERT keyphrase extraction, Wikipedia context retrieval, and FLAN-T5-small + LoRA question generation — into a single end-to-end inference pipeline. The Gemini API is excluded in favour of Wikipedia because LLM-generated context is non-deterministic: the same query can return different text across runs, making outputs unreproducible for academic evaluation. Wikipedia responses are stable, require no API credentials, and align with the factual background knowledge approach of the SOQG paper.\n\n**Pipeline:** User Input → KeyBERT → Wikipedia → FLAN-T5 → Socratic Question + Concept Map Nodes"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nimport os\nimport json\nimport time\nfrom pathlib import Path\nfrom typing import List, Dict, Optional\nfrom dataclasses import dataclass, field, asdict\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom peft import PeftModel\nfrom keybert import KeyBERT\nimport wikipediaapi\n\n# Rationale: deterministic, no API key required, academically reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n# ── Configuration ─────────────────────────────────────────────────────────────\n\nMODEL_PATH = Path(\"../models/flan-t5-socratic-lora/adapter\")\nMODEL_NAME = \"google/flan-t5-small\"   # matches the trained adapter   # base model name (used for loading)\nDEVICE     = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Default question type used when the user enters free-form text.\n# \"reasons_evidence\" is the most common type in SocratiQ (35% of training data).\nDEFAULT_QUESTION_TYPE = \"reasons_evidence\"\n\n# Generation configs — see 04_evaluation.ipynb for rationale.\nEVAL_GENERATION_CONFIG = dict(max_length=80, num_beams=4, do_sample=False)\nDEMO_GENERATION_CONFIG = dict(\n    max_length=80, num_beams=2, do_sample=True,\n    temperature=0.8, top_p=0.9,\n    repetition_penalty=1.2, no_repeat_ngram_size=3,\n)\n\nprint(f\"Model path : {MODEL_PATH}\")\nprint(f\"Device     : {DEVICE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Keyphrase:\n",
    "    phrase: str\n",
    "    score: float\n",
    "    source: str = \"input\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievedContext:\n",
    "    keyphrase: str\n",
    "    context: str\n",
    "    source: str\n",
    "    url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConceptNode:\n",
    "    id: str\n",
    "    label: str\n",
    "    node_type: str\n",
    "    score: float = 0.0\n",
    "    sources: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResponse:\n",
    "    user_input: str\n",
    "    socratic_question: str\n",
    "    keyphrases: List[Keyphrase]\n",
    "    retrieved_contexts: List[RetrievedContext]\n",
    "    concept_nodes: List[ConceptNode]\n",
    "    processing_time_ms: float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# ── Load Components ───────────────────────────────────────────────────────────\n\n# 1. FLAN-T5 + LoRA adapter — correct load sequence\nprint(\"Loading FLAN-T5 + LoRA adapter...\")\ntokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\nbase_model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)\nbase_model.resize_token_embeddings(len(tokenizer))   # resize BEFORE adapter load\nmodel = PeftModel.from_pretrained(base_model, str(MODEL_PATH))\nmodel = model.to(DEVICE)\nmodel.eval()\nprint(f\"  ✓ FLAN-T5 loaded ({model.num_parameters():,} params, vocab={len(tokenizer)})\")\n\n# 2. KeyBERT for keyphrase extraction (concept map nodes)\nprint(\"\\nLoading KeyBERT (all-MiniLM-L6-v2)...\")\nkw_model = KeyBERT(model=\"all-MiniLM-L6-v2\")\nprint(\"  ✓ KeyBERT loaded\")\n\n# 3. Wikipedia API for deterministic context retrieval\nprint(\"\\nInitialising Wikipedia API...\")\nwiki = wikipediaapi.Wikipedia(\n    user_agent=\"SocraticPath/1.0 (dissertation; contact: anuhas0123@gmail.com)\",\n    language=\"en\",\n)\nprint(\"  ✓ Wikipedia API ready\")\nprint(\"\\nAll components loaded.\")\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# ── Pipeline Functions ────────────────────────────────────────────────────────\n\n\ndef extract_keyphrases(text: str, top_n: int = 5) -> List[dict]:\n    \"\"\"Extract keyphrases with KeyBERT (MMR for diversity).\"\"\"\n    if not text or len(text.strip()) < 10:\n        return []\n    keywords = kw_model.extract_keywords(\n        text,\n        keyphrase_ngram_range=(1, 2),\n        stop_words=\"english\",\n        top_n=top_n,\n        use_mmr=True,\n        diversity=0.5,\n    )\n    return [{\"phrase\": kw, \"score\": float(score)} for kw, score in keywords]\n\n\ndef retrieve_context_wikipedia(keyphrase: str) -> Optional[Dict]:\n    \"\"\"\n    Retrieve a short factual summary from Wikipedia for a keyphrase.\n\n    Returns a dict with 'summary' and 'url', or None if no page found.\n    Wikipedia is used in preference to an LLM API because:\n      - Responses are deterministic (same query → same answer every run).\n      - No API key or cost required.\n      - Content is stable and citable.\n      - Matches the SOQG paper's context enrichment approach.\n    \"\"\"\n    try:\n        page = wiki.page(keyphrase)\n        if page.exists():\n            # Take up to ~3 sentences (400 chars) of the summary\n            summary = page.summary[:400]\n            last_period = summary.rfind(\".\")\n            if last_period > 150:\n                summary = summary[: last_period + 1]\n            return {\"summary\": summary, \"url\": page.fullurl, \"title\": page.title}\n    except Exception as exc:\n        print(f\"  Wikipedia lookup failed for '{keyphrase}': {exc}\")\n    return None\n\n\ndef retrieve_contexts(keyphrases: List[dict]) -> List[dict]:\n    \"\"\"Look up Wikipedia context for the top-3 keyphrases.\"\"\"\n    results = []\n    for kp in keyphrases[:3]:       # limit to 3 to keep prompt length manageable\n        result = retrieve_context_wikipedia(kp[\"phrase\"])\n        if result:\n            results.append({\n                \"keyphrase\": kp[\"phrase\"],\n                \"context\": result[\"summary\"],\n                \"source\": \"wikipedia\",\n                \"url\": result[\"url\"],\n            })\n    return results\n\n\ndef generate_socratic_question(\n    user_input: str,\n    question_type: str = DEFAULT_QUESTION_TYPE,\n    retrieved_context: str = \"\",\n    use_sampling: bool = False,\n) -> str:\n    \"\"\"\n    Generate a Socratic question with the fine-tuned FLAN-T5.\n\n    The prompt format mirrors the training data exactly:\n      \"Generate a Socratic question for this context: {type}: {context}\"\n    If retrieved_context is provided it is appended (truncated to 500 chars)\n    after a newline so the encoder can attend to external knowledge.\n    \"\"\"\n    prompt = (\n        f\"Generate a Socratic question for this context: \"\n        f\"{question_type}: {user_input}\"\n    )\n    if retrieved_context:\n        prompt += f\"\\n\\nAdditional context: {retrieved_context[:500]}\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=400, truncation=True)\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n\n    gen_cfg = DEMO_GENERATION_CONFIG if use_sampling else EVAL_GENERATION_CONFIG\n\n    with torch.no_grad():\n        outputs = model.generate(**inputs, **gen_cfg)\n\n    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return generated.replace(\"[Question]\", \"\").strip()\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n# ── Complete Pipeline Function ────────────────────────────────────────────────\n\ndef run_pipeline(\n    user_input: str,\n    question_type: str = DEFAULT_QUESTION_TYPE,\n    use_retrieval: bool = True,\n    use_sampling: bool = False,        # False = deterministic (for demos & eval)\n) -> dict:\n    \"\"\"\n    End-to-end SocraticPath pipeline.\n\n    Args:\n        user_input:     Raw text from the user (opinion/argument).\n        question_type:  SocratiQ category prefix.  Default: \"reasons_evidence\".\n                        Valid values: \"reasons_evidence\", \"clarity\",\n                        \"implication_consequences\", \"alternate_viewpoints_perspectives\",\n                        \"assumptions\".\n        use_retrieval:  Whether to query Wikipedia for context enrichment.\n        use_sampling:   True for demo/frontend (varied outputs);\n                        False for evaluation (deterministic ROUGE).\n\n    Returns:\n        dict with keys: user_input, question_type, socratic_question,\n        keyphrases, retrieved_contexts, concept_nodes, processing_time_ms.\n    \"\"\"\n    t0 = time.time()\n\n    # Step 1: Extract keyphrases → concept map nodes\n    keyphrases = extract_keyphrases(user_input, top_n=5)\n\n    # Step 2: Retrieve Wikipedia context for top keyphrases\n    contexts = []\n    combined_context = \"\"\n    if use_retrieval and keyphrases:\n        contexts = retrieve_contexts(keyphrases)\n        combined_context = \" \".join(c[\"context\"] for c in contexts)\n\n    # Step 3: Generate Socratic question\n    socratic_question = generate_socratic_question(\n        user_input,\n        question_type=question_type,\n        retrieved_context=combined_context,\n        use_sampling=use_sampling,\n    )\n\n    # Step 4: Build concept map node list for React Flow\n    concept_nodes = [\n        {\"id\": \"user_input\", \"type\": \"input\",    \"label\": \"User Input\",       \"score\": 1.0},\n        {\"id\": \"sq\",          \"type\": \"question\", \"label\": socratic_question[:60] + \"…\"\n                                                  if len(socratic_question) > 60\n                                                  else socratic_question,    \"score\": 1.0},\n    ] + [\n        {\n            \"id\": f\"concept_{i}\",\n            \"type\": \"concept\",\n            \"label\": kp[\"phrase\"],\n            \"score\": kp[\"score\"],\n            \"has_context\": any(c[\"keyphrase\"].lower() == kp[\"phrase\"].lower()\n                               for c in contexts),\n        }\n        for i, kp in enumerate(keyphrases)\n    ]\n\n    return {\n        \"user_input\": user_input,\n        \"question_type\": question_type,\n        \"socratic_question\": socratic_question,\n        \"keyphrases\": keyphrases,\n        \"retrieved_contexts\": contexts,\n        \"concept_nodes\": concept_nodes,\n        \"processing_time_ms\": (time.time() - t0) * 1000,\n    }\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n    \"I believe that social media is harmful to teenagers and should be banned for anyone under 18.\",\n    \"Climate change is exaggerated by scientists who want more research funding.\",\n    \"Artificial intelligence will make most human jobs obsolete within the next decade.\"\n]\n\nfor i, user_input in enumerate(test_inputs, 1):\n    print(f\"\\n{'='*70}\")\n    print(f\"TEST {i}\")\n    print(f\"{'='*70}\")\n    \n    response = run_pipeline(user_input)\n    \n    print(f\"\\nUser Input:\\n  {response[\"user_input\"]}\")\n    print(f\"\\nSocratic Question:\\n  {response[\"socratic_question\"]}\")\n    print(f\"\\nKeyphrases:\")\n    for kp in response[\"keyphrases\"]:\n        print(f\"  - {kp[\"phrase\"]} ({kp[\"score\"]:.3f})\")\n    print(f\"\\nRetrieved Contexts: {len(response[\"retrieved_contexts\"])}\")\n    for ctx in response[\"retrieved_contexts\"]:\n        print(f\"  [{ctx[\"source\"]}] {ctx[\"keyphrase\"]}: {ctx[\"context\"][:100]}...\")\n    print(f\"\\nConcept Nodes: {len(response[\"concept_nodes\"])}\")\n    print(f\"Processing Time: {response[\"processing_time_ms\"]:.1f}ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_dict(response: dict) -> dict:\n    \"\"\"\n    Return the pipeline response as a JSON-serializable dict.\n\n    run_pipeline() already returns a plain dict, so this function\n    is a passthrough with type coercion for any numpy/tensor values.\n    It exists as a named interface so the FastAPI backend can call it\n    uniformly without needing to know the internal representation.\n    \"\"\"\n    import numpy as np\n    def _coerce(v):\n        if isinstance(v, (np.floating, np.integer)):\n            return float(v)\n        return v\n\n    return {\n        \"user_input\": response[\"user_input\"],\n        \"question_type\": response.get(\"question_type\", \"reasons_evidence\"),\n        \"socratic_question\": response[\"socratic_question\"],\n        \"keyphrases\": [\n            {\"phrase\": kp[\"phrase\"], \"score\": _coerce(kp[\"score\"])}\n            for kp in response[\"keyphrases\"]\n        ],\n        \"retrieved_contexts\": [\n            {\n                \"keyphrase\": ctx[\"keyphrase\"],\n                \"context\": ctx[\"context\"],\n                \"source\": ctx[\"source\"],\n                \"url\": ctx.get(\"url\"),\n            }\n            for ctx in response[\"retrieved_contexts\"]\n        ],\n        \"concept_nodes\": response[\"concept_nodes\"],\n        \"processing_time_ms\": _coerce(response[\"processing_time_ms\"]),\n    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response = run_pipeline(\n",
    "    \"Universal basic income is necessary because automation will eliminate most jobs.\"\n",
    ")\n",
    "\n",
    "api_response = pipeline_to_dict(sample_response)\n",
    "print(\"API Response Format:\")\n",
    "print(json.dumps(api_response, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../models/pipeline_config\")\nOUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n\nconfig = {\n    \"model\": {\n        \"name\": MODEL_NAME,\n        \"adapter_path\": str(MODEL_PATH),\n        \"type\": \"flan-t5 + lora\",\n        \"generation_eval\": EVAL_GENERATION_CONFIG,\n        \"generation_demo\": DEMO_GENERATION_CONFIG,\n    },\n    \"keybert\": {\n        \"model\": \"all-MiniLM-L6-v2\",\n        \"top_n\": 5,\n        \"ngram_range\": [1, 2],\n        \"diversity\": 0.5,\n    },\n    \"retrieval\": {\n        \"backend\": \"wikipedia\",\n        \"max_keyphrases_for_retrieval\": 3,\n        \"note\": \"Wikipedia used for deterministic, reproducible context retrieval.\",\n    },\n}\n\nimport json as _json\nwith open(OUTPUT_PATH / \"config.json\", \"w\") as f:\n    _json.dump(config, f, indent=2)\n\nprint(f\"✓ Pipeline configuration saved to {OUTPUT_PATH / 'config.json'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "test_statements = [\n",
    "    \"I think video games make children violent.\",\n",
    "    \"We should abolish the electoral college.\",\n",
    "    \"Space exploration is a waste of money.\",\n",
    "    \"Nuclear energy is too dangerous to use.\",\n",
    "    \"Social media should be regulated by the government.\"\n",
    "]\n",
    "\n",
    "print(\"Running performance benchmark...\\n\")\n",
    "\n",
    "times = []\n",
    "for stmt in test_statements:\n",
    "    response = run_pipeline(stmt)\n",
    "    times.append(response.processing_time_ms)\n",
    "    print(f\"✓ {stmt[:50]}... ({response.processing_time_ms:.0f}ms)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Performance Summary:\")\n",
    "print(f\"  Average: {statistics.mean(times):.0f}ms\")\n",
    "print(f\"  Median: {statistics.median(times):.0f}ms\")\n",
    "print(f\"  Min: {min(times):.0f}ms\")\n",
    "print(f\"  Max: {max(times):.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complete pipeline is encapsulated in `run_pipeline()`, which returns a JSON-serializable dict consumed by the FastAPI backend. The `pipeline_to_dict()` function coerces any numpy scalar types to Python floats to ensure clean JSON serialisation. Pipeline configuration is persisted to `models/pipeline_config/config.json`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
