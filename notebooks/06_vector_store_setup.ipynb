{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Vector Store Setup\n",
    "\n",
    "This notebook sets up ChromaDB for semantic search and context retrieval.\n",
    "\n",
    "## Objectives\n",
    "- Initialize ChromaDB with persistent storage\n",
    "- Chunk and embed training contexts from SoQG\n",
    "- Build a searchable vector index\n",
    "- Test retrieval with sample queries\n",
    "- Integrate with the retrieval pipeline\n",
    "\n",
    "## Why Vector Store?\n",
    "The vector store enables:\n",
    "1. **Local Context Retrieval** - Find similar contexts from training data\n",
    "2. **Hybrid Search** - Combine with Wikipedia/Gemini for comprehensive results\n",
    "3. **Fast Inference** - Sub-second similarity search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DATA_DIR = Path(\"../datasets/processed\")\n",
    "VECTOR_STORE_DIR = Path(\"../backend/vector_store\")\n",
    "VECTOR_STORE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Vector store will be saved to: {VECTOR_STORE_DIR.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"embedding_dim\": 384,\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 100,\n",
    "    \"collection_name\": \"soqg_contexts\",\n",
    "    \"distance_metric\": \"cosine\"\n",
    "}\n",
    "\n",
    "print(\"Vector Store Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(DATA_DIR / \"train_clean.csv\")\n",
    "\n",
    "print(f\"Loaded {len(train_df)} training samples\")\n",
    "print(f\"\\nColumns: {train_df.columns.tolist()}\")\n",
    "print(f\"\\nQuestion types: {train_df['question_type'].unique().tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contexts = train_df['context'].dropna().unique().tolist()\n",
    "print(f\"Unique contexts: {len(contexts)}\")\n",
    "\n",
    "context_lengths = [len(c) for c in contexts]\n",
    "print(f\"\\nContext length stats:\")\n",
    "print(f\"  Mean: {np.mean(context_lengths):.0f} chars\")\n",
    "print(f\"  Median: {np.median(context_lengths):.0f} chars\")\n",
    "print(f\"  Max: {np.max(context_lengths)} chars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Chunking\n",
    "\n",
    "Split long contexts into overlapping chunks for better retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, overlap=100):\n",
    "    \"\"\"Split text into overlapping chunks.\"\"\"\n",
    "    if len(text) <= chunk_size:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        \n",
    "        if end < len(text):\n",
    "            space_idx = text.rfind(' ', start, end)\n",
    "            if space_idx > start:\n",
    "                end = space_idx\n",
    "        \n",
    "        chunk = text[start:end].strip()\n",
    "        if chunk:\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        start = end - overlap\n",
    "        if start >= len(text) - overlap:\n",
    "            break\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_chunks = []\n",
    "chunk_metadata = []\n",
    "\n",
    "for idx, context in enumerate(tqdm(contexts, desc=\"Chunking\")):\n",
    "    chunks = chunk_text(context, config['chunk_size'], config['chunk_overlap'])\n",
    "    \n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        all_chunks.append(chunk)\n",
    "        chunk_metadata.append({\n",
    "            \"context_id\": idx,\n",
    "            \"chunk_idx\": chunk_idx,\n",
    "            \"total_chunks\": len(chunks),\n",
    "            \"char_length\": len(chunk)\n",
    "        })\n",
    "\n",
    "print(f\"\\nCreated {len(all_chunks)} chunks from {len(contexts)} contexts\")\n",
    "print(f\"Average chunks per context: {len(all_chunks)/len(contexts):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer(config['embedding_model'])\n",
    "\n",
    "test_embedding = embedding_model.encode([\"test sentence\"])\n",
    "print(f\"Embedding model: {config['embedding_model']}\")\n",
    "print(f\"Embedding dimension: {test_embedding.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "all_embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"Embedding\"):\n",
    "    batch = all_chunks[i:i+batch_size]\n",
    "    embeddings = embedding_model.encode(batch, show_progress_bar=False)\n",
    "    all_embeddings.extend(embeddings)\n",
    "\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "print(f\"\\nGenerated embeddings shape: {all_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = chromadb.PersistentClient(path=str(VECTOR_STORE_DIR))\n",
    "\n",
    "try:\n",
    "    client.delete_collection(config['collection_name'])\n",
    "    print(f\"Deleted existing collection: {config['collection_name']}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.create_collection(\n",
    "    name=config['collection_name'],\n",
    "    metadata={\"hnsw:space\": config['distance_metric']}\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {config['collection_name']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Add Documents to Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "for i in tqdm(range(0, len(all_chunks), batch_size), desc=\"Adding to ChromaDB\"):\n",
    "    end_idx = min(i + batch_size, len(all_chunks))\n",
    "    \n",
    "    batch_ids = [f\"chunk_{j}\" for j in range(i, end_idx)]\n",
    "    batch_documents = all_chunks[i:end_idx]\n",
    "    batch_embeddings = all_embeddings[i:end_idx].tolist()\n",
    "    batch_metadata = chunk_metadata[i:end_idx]\n",
    "    \n",
    "    collection.add(\n",
    "        ids=batch_ids,\n",
    "        documents=batch_documents,\n",
    "        embeddings=batch_embeddings,\n",
    "        metadatas=batch_metadata\n",
    "    )\n",
    "\n",
    "print(f\"\\nTotal documents in collection: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query, n_results=5):\n",
    "    \"\"\"Search the vector store for similar contexts.\"\"\"\n",
    "    query_embedding = embedding_model.encode([query]).tolist()\n",
    "    \n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding,\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "    )\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"climate change and global warming effects\",\n",
    "    \"machine learning algorithms and neural networks\",\n",
    "    \"photosynthesis in plants\",\n",
    "    \"economic policies and market regulation\",\n",
    "    \"human rights and social justice\"\n",
    "]\n",
    "\n",
    "print(\"Retrieval Test Results:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search(query, n_results=3)\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    for i, (doc, dist) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "        similarity = 1 - dist\n",
    "        print(f\"  [{i+1}] Similarity: {similarity:.4f}\")\n",
    "        print(f\"      {doc[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Retrieval Service Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorStoreRetriever:\n",
    "    \"\"\"Retrieval service for the vector store.\"\"\"\n",
    "    \n",
    "    def __init__(self, persist_dir, collection_name, embedding_model_name='all-MiniLM-L6-v2'):\n",
    "        self.client = chromadb.PersistentClient(path=str(persist_dir))\n",
    "        self.collection = self.client.get_collection(collection_name)\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    \n",
    "    def search(self, query, n_results=5, min_similarity=0.3):\n",
    "        \"\"\"Search for similar contexts.\"\"\"\n",
    "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "        \n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "        )\n",
    "        \n",
    "        formatted = []\n",
    "        for doc, dist, meta in zip(\n",
    "            results['documents'][0],\n",
    "            results['distances'][0],\n",
    "            results['metadatas'][0]\n",
    "        ):\n",
    "            similarity = 1 - dist\n",
    "            if similarity >= min_similarity:\n",
    "                formatted.append({\n",
    "                    \"text\": doc,\n",
    "                    \"similarity\": float(similarity),\n",
    "                    \"metadata\": meta\n",
    "                })\n",
    "        \n",
    "        return formatted\n",
    "    \n",
    "    def get_context_for_question(self, question, keyphrases=None, n_results=5):\n",
    "        \"\"\"Get context for question generation.\"\"\"\n",
    "        if keyphrases:\n",
    "            query = question + \" \" + \" \".join(keyphrases)\n",
    "        else:\n",
    "            query = question\n",
    "        \n",
    "        results = self.search(query, n_results=n_results)\n",
    "        \n",
    "        combined_context = \"\\n\\n\".join([r['text'] for r in results])\n",
    "        return {\n",
    "            \"context\": combined_context,\n",
    "            \"sources\": results\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = VectorStoreRetriever(\n",
    "    persist_dir=VECTOR_STORE_DIR,\n",
    "    collection_name=config['collection_name'],\n",
    "    embedding_model_name=config['embedding_model']\n",
    ")\n",
    "\n",
    "test_result = retriever.get_context_for_question(\n",
    "    \"What are the effects of deforestation?\",\n",
    "    keyphrases=[\"environment\", \"trees\", \"ecosystem\"]\n",
    ")\n",
    "\n",
    "print(\"Test Retrieval:\")\n",
    "print(f\"Query: What are the effects of deforestation?\")\n",
    "print(f\"Retrieved {len(test_result['sources'])} sources\")\n",
    "print(f\"\\nFirst source (similarity: {test_result['sources'][0]['similarity']:.3f}):\")\n",
    "print(test_result['sources'][0]['text'][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Retriever Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever_code = '''\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "class VectorStoreRetriever:\n",
    "    def __init__(self, persist_dir: str, collection_name: str, embedding_model_name: str = 'all-MiniLM-L6-v2'):\n",
    "        self.client = chromadb.PersistentClient(path=persist_dir)\n",
    "        self.collection = self.client.get_collection(collection_name)\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name)\n",
    "    \n",
    "    def search(self, query: str, n_results: int = 5, min_similarity: float = 0.3) -> List[Dict]:\n",
    "        query_embedding = self.embedding_model.encode([query]).tolist()\n",
    "        results = self.collection.query(\n",
    "            query_embeddings=query_embedding,\n",
    "            n_results=n_results,\n",
    "            include=[\"documents\", \"distances\", \"metadatas\"]\n",
    "        )\n",
    "        formatted = []\n",
    "        for doc, dist, meta in zip(results['documents'][0], results['distances'][0], results['metadatas'][0]):\n",
    "            similarity = 1 - dist\n",
    "            if similarity >= min_similarity:\n",
    "                formatted.append({\"text\": doc, \"similarity\": float(similarity), \"metadata\": meta})\n",
    "        return formatted\n",
    "    \n",
    "    def get_context_for_question(self, question: str, keyphrases: Optional[List[str]] = None, n_results: int = 5) -> Dict:\n",
    "        query = question + \" \" + \" \".join(keyphrases) if keyphrases else question\n",
    "        results = self.search(query, n_results=n_results)\n",
    "        combined_context = \"\\\\n\\\\n\".join([r['text'] for r in results])\n",
    "        return {\"context\": combined_context, \"sources\": results}\n",
    "'''\n",
    "\n",
    "with open(VECTOR_STORE_DIR / \"retriever.py\", \"w\") as f:\n",
    "    f.write(retriever_code)\n",
    "\n",
    "print(f\"Retriever module saved to {VECTOR_STORE_DIR / 'retriever.py'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config[\"created_at\"] = datetime.now().isoformat()\n",
    "config[\"total_chunks\"] = len(all_chunks)\n",
    "config[\"total_contexts\"] = len(contexts)\n",
    "\n",
    "with open(VECTOR_STORE_DIR / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(\"Configuration saved:\")\n",
    "print(json.dumps(config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Retrieval Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_questions = train_df.sample(100, random_state=SEED)\n",
    "\n",
    "retrieval_scores = []\n",
    "\n",
    "for _, row in tqdm(sample_questions.iterrows(), total=len(sample_questions), desc=\"Testing\"):\n",
    "    context = row['context']\n",
    "    question = row['target']\n",
    "    \n",
    "    results = retriever.search(question, n_results=5)\n",
    "    \n",
    "    if results:\n",
    "        top_similarity = results[0]['similarity']\n",
    "        found_exact = any(context in r['text'] or r['text'] in context for r in results)\n",
    "    else:\n",
    "        top_similarity = 0\n",
    "        found_exact = False\n",
    "    \n",
    "    retrieval_scores.append({\n",
    "        'top_similarity': top_similarity,\n",
    "        'found_exact': found_exact\n",
    "    })\n",
    "\n",
    "scores_df = pd.DataFrame(retrieval_scores)\n",
    "\n",
    "print(\"\\nRetrieval Quality Metrics:\")\n",
    "print(f\"  Mean top similarity: {scores_df['top_similarity'].mean():.4f}\")\n",
    "print(f\"  Exact match rate: {scores_df['found_exact'].mean()*100:.1f}%\")\n",
    "print(f\"  Queries with similarity > 0.5: {(scores_df['top_similarity'] > 0.5).mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(scores_df['top_similarity'], bins=30, color='#3B82F6', edgecolor='white', alpha=0.7)\n",
    "plt.axvline(scores_df['top_similarity'].mean(), color='red', linestyle='--', label=f\"Mean: {scores_df['top_similarity'].mean():.3f}\")\n",
    "plt.xlabel('Top Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Top Retrieval Similarity Scores')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(VECTOR_STORE_DIR / \"retrieval_quality.png\", dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Summary\n",
    "\n",
    "### What Was Built\n",
    "- ChromaDB vector store with ~X chunks from training data\n",
    "- Sentence-transformer embeddings (all-MiniLM-L6-v2)\n",
    "- Semantic search retrieval service\n",
    "- Quality evaluation on sample questions\n",
    "\n",
    "### Files Created\n",
    "| File | Purpose |\n",
    "|------|--------|\n",
    "| `vector_store/` | ChromaDB persistent storage |\n",
    "| `retriever.py` | Python module for retrieval |\n",
    "| `config.json` | Vector store configuration |\n",
    "| `retrieval_quality.png` | Quality analysis chart |\n",
    "\n",
    "### Integration\n",
    "Load in backend:\n",
    "```python\n",
    "from vector_store.retriever import VectorStoreRetriever\n",
    "\n",
    "retriever = VectorStoreRetriever(\n",
    "    persist_dir=\"./vector_store\",\n",
    "    collection_name=\"soqg_contexts\"\n",
    ")\n",
    "\n",
    "results = retriever.search(\"your query here\")\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "1. Integrate retriever with FastAPI backend\n",
    "2. Combine with Wikipedia/Gemini for hybrid search\n",
    "3. Test end-to-end question generation pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
