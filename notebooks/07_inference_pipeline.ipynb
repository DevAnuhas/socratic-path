{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Complete Inference Pipeline\n",
    "\n",
    "This notebook integrates all components into a production-ready inference pipeline. We will:\n",
    "\n",
    "1. Load the fine-tuned FLAN-T5 model\n",
    "2. Initialize KeyBERT for keyphrase extraction\n",
    "3. Set up Gemini context retrieval\n",
    "4. Build an end-to-end pipeline\n",
    "5. Create an interactive demo\n",
    "\n",
    "---\n",
    "\n",
    "## Pipeline Flow\n",
    "\n",
    "```\n",
    "User Input ‚Üí KeyBERT ‚Üí Gemini Retrieval ‚Üí FLAN-T5 ‚Üí Response\n",
    "     ‚Üì           ‚Üì              ‚Üì              ‚Üì\n",
    "  Context    Keyphrases    Context        Question\n",
    "     ‚Üì           ‚Üì              ‚Üì              ‚Üì\n",
    "     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                         ‚Üì\n",
    "                  Concept Map Nodes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from keybert import KeyBERT\n",
    "import google.generativeai as genai\n",
    "import wikipediaapi\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = Path(\"../models/flan-t5-socratic/final\")\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    print(\"Gemini API configured.\")\n",
    "else:\n",
    "    print(\"Warning: GEMINI_API_KEY not found. Set it in .env file.\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Keyphrase:\n",
    "    phrase: str\n",
    "    score: float\n",
    "    source: str = \"input\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class RetrievedContext:\n",
    "    keyphrase: str\n",
    "    context: str\n",
    "    source: str\n",
    "    url: Optional[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ConceptNode:\n",
    "    id: str\n",
    "    label: str\n",
    "    node_type: str\n",
    "    score: float = 0.0\n",
    "    sources: List[str] = None\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineResponse:\n",
    "    user_input: str\n",
    "    socratic_question: str\n",
    "    keyphrases: List[Keyphrase]\n",
    "    retrieved_contexts: List[RetrievedContext]\n",
    "    concept_nodes: List[ConceptNode]\n",
    "    processing_time_ms: float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading FLAN-T5 model...\")\n",
    "if MODEL_PATH.exists():\n",
    "    tokenizer = T5Tokenizer.from_pretrained(str(MODEL_PATH))\n",
    "    model = T5ForConditionalGeneration.from_pretrained(str(MODEL_PATH))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    print(f\"Model loaded: {model.num_parameters():,} parameters\")\n",
    "else:\n",
    "    print(f\"Model not found at {MODEL_PATH}. Using base model.\")\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "    tokenizer.add_tokens([\"[Question]\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading KeyBERT...\")\n",
    "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "print(\"KeyBERT loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing Gemini...\")\n",
    "gemini_model = genai.GenerativeModel('gemini-1.5-flash') if GEMINI_API_KEY else None\n",
    "\n",
    "print(\"Initializing Wikipedia...\")\n",
    "wiki = wikipediaapi.Wikipedia(\n",
    "    user_agent='SocraticPath/1.0',\n",
    "    language='en'\n",
    ")\n",
    "print(\"All components loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyphrases(text: str, top_n: int = 5) -> List[Keyphrase]:\n",
    "    \"\"\"Extract keyphrases using KeyBERT.\"\"\"\n",
    "    if not text or len(text.strip()) < 10:\n",
    "        return []\n",
    "    \n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words='english',\n",
    "        top_n=top_n,\n",
    "        use_mmr=True,\n",
    "        diversity=0.5\n",
    "    )\n",
    "    \n",
    "    return [Keyphrase(phrase=kw, score=score) for kw, score in keywords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_gemini(keyphrases: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"Retrieve context using Gemini API.\"\"\"\n",
    "    if not gemini_model or not keyphrases:\n",
    "        return {}\n",
    "    \n",
    "    prompt = f\"\"\"Provide brief, factual context for these topics (2-3 sentences each):\n",
    "\n",
    "Topics: {', '.join(keyphrases)}\n",
    "\n",
    "Format:\n",
    "TOPIC: [name]\n",
    "CONTEXT: [explanation]\"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = gemini_model.generate_content(\n",
    "            prompt,\n",
    "            generation_config=genai.GenerationConfig(\n",
    "                temperature=0.3,\n",
    "                max_output_tokens=500\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result = {}\n",
    "        current_topic = None\n",
    "        current_context = []\n",
    "        \n",
    "        for line in response.text.strip().split('\\n'):\n",
    "            line = line.strip()\n",
    "            if line.startswith('TOPIC:'):\n",
    "                if current_topic and current_context:\n",
    "                    result[current_topic] = ' '.join(current_context)\n",
    "                current_topic = line.replace('TOPIC:', '').strip()\n",
    "                current_context = []\n",
    "            elif line.startswith('CONTEXT:'):\n",
    "                current_context.append(line.replace('CONTEXT:', '').strip())\n",
    "            elif current_topic and line:\n",
    "                current_context.append(line)\n",
    "        \n",
    "        if current_topic and current_context:\n",
    "            result[current_topic] = ' '.join(current_context)\n",
    "        \n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Gemini error: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context_wikipedia(keyphrase: str) -> Optional[Dict]:\n",
    "    \"\"\"Retrieve context from Wikipedia.\"\"\"\n",
    "    try:\n",
    "        page = wiki.page(keyphrase)\n",
    "        if page.exists():\n",
    "            summary = page.summary[:400]\n",
    "            last_period = summary.rfind('.')\n",
    "            if last_period > 200:\n",
    "                summary = summary[:last_period + 1]\n",
    "            return {\n",
    "                'summary': summary,\n",
    "                'url': page.fullurl\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_contexts(keyphrases: List[Keyphrase]) -> List[RetrievedContext]:\n",
    "    \"\"\"Retrieve context for all keyphrases with fallback.\"\"\"\n",
    "    results = []\n",
    "    kp_strings = [kp.phrase for kp in keyphrases]\n",
    "    \n",
    "    gemini_contexts = retrieve_context_gemini(kp_strings)\n",
    "    \n",
    "    for kp in keyphrases:\n",
    "        matched = None\n",
    "        for key, value in gemini_contexts.items():\n",
    "            if kp.phrase.lower() in key.lower() or key.lower() in kp.phrase.lower():\n",
    "                matched = value\n",
    "                break\n",
    "        \n",
    "        if matched:\n",
    "            results.append(RetrievedContext(\n",
    "                keyphrase=kp.phrase,\n",
    "                context=matched,\n",
    "                source='gemini'\n",
    "            ))\n",
    "        else:\n",
    "            wiki_result = retrieve_context_wikipedia(kp.phrase)\n",
    "            if wiki_result:\n",
    "                results.append(RetrievedContext(\n",
    "                    keyphrase=kp.phrase,\n",
    "                    context=wiki_result['summary'],\n",
    "                    source='wikipedia',\n",
    "                    url=wiki_result['url']\n",
    "                ))\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_socratic_question(\n",
    "    user_input: str,\n",
    "    retrieved_context: str = \"\"\n",
    ") -> str:\n",
    "    \"\"\"Generate a Socratic question using the fine-tuned model.\"\"\"\n",
    "    \n",
    "    if retrieved_context:\n",
    "        input_text = f\"Generate a Socratic question for this context: {user_input}\\n\\nAdditional context: {retrieved_context[:500]}\"\n",
    "    else:\n",
    "        input_text = f\"Generate a Socratic question for this context: {user_input}\"\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        input_text,\n",
    "        return_tensors=\"pt\",\n",
    "        max_length=400,\n",
    "        truncation=True\n",
    "    )\n",
    "    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=80,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            top_p=0.6,\n",
    "            repetition_penalty=1.2,\n",
    "            no_repeat_ngram_size=3\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    generated = generated.replace(\"[Question]\", \"\").strip()\n",
    "    \n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_nodes(\n",
    "    user_input: str,\n",
    "    socratic_question: str,\n",
    "    keyphrases: List[Keyphrase],\n",
    "    contexts: List[RetrievedContext]\n",
    ") -> List[ConceptNode]:\n",
    "    \"\"\"Create concept map nodes for visualization.\"\"\"\n",
    "    nodes = []\n",
    "    \n",
    "    nodes.append(ConceptNode(\n",
    "        id=\"user_input\",\n",
    "        label=\"User Input\",\n",
    "        node_type=\"input\",\n",
    "        score=1.0\n",
    "    ))\n",
    "    \n",
    "    nodes.append(ConceptNode(\n",
    "        id=\"socratic_question\",\n",
    "        label=socratic_question[:50] + \"...\" if len(socratic_question) > 50 else socratic_question,\n",
    "        node_type=\"question\",\n",
    "        score=1.0\n",
    "    ))\n",
    "    \n",
    "    for i, kp in enumerate(keyphrases):\n",
    "        sources = [\"input\"]\n",
    "        for ctx in contexts:\n",
    "            if ctx.keyphrase.lower() == kp.phrase.lower():\n",
    "                sources.append(ctx.source)\n",
    "        \n",
    "        nodes.append(ConceptNode(\n",
    "            id=f\"concept_{i}\",\n",
    "            label=kp.phrase,\n",
    "            node_type=\"concept\",\n",
    "            score=kp.score,\n",
    "            sources=sources\n",
    "        ))\n",
    "    \n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline(user_input: str, use_retrieval: bool = True) -> PipelineResponse:\n",
    "    \"\"\"\n",
    "    Run the complete SocraticPath inference pipeline.\n",
    "    \n",
    "    Args:\n",
    "        user_input: The user's statement or opinion\n",
    "        use_retrieval: Whether to use context retrieval\n",
    "    \n",
    "    Returns:\n",
    "        PipelineResponse with all outputs\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    keyphrases = extract_keyphrases(user_input, top_n=5)\n",
    "    \n",
    "    contexts = []\n",
    "    combined_context = \"\"\n",
    "    \n",
    "    if use_retrieval and keyphrases:\n",
    "        contexts = retrieve_contexts(keyphrases[:3])\n",
    "        combined_context = \" \".join([c.context for c in contexts])\n",
    "    \n",
    "    socratic_question = generate_socratic_question(user_input, combined_context)\n",
    "    \n",
    "    concept_nodes = create_concept_nodes(\n",
    "        user_input,\n",
    "        socratic_question,\n",
    "        keyphrases,\n",
    "        contexts\n",
    "    )\n",
    "    \n",
    "    processing_time = (time.time() - start_time) * 1000\n",
    "    \n",
    "    return PipelineResponse(\n",
    "        user_input=user_input,\n",
    "        socratic_question=socratic_question,\n",
    "        keyphrases=keyphrases,\n",
    "        retrieved_contexts=contexts,\n",
    "        concept_nodes=concept_nodes,\n",
    "        processing_time_ms=processing_time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_inputs = [\n",
    "    \"I believe that social media is harmful to teenagers and should be banned for anyone under 18.\",\n",
    "    \"Climate change is exaggerated by scientists who want more research funding.\",\n",
    "    \"Artificial intelligence will make most human jobs obsolete within the next decade.\"\n",
    "]\n",
    "\n",
    "for i, user_input in enumerate(test_inputs, 1):\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"TEST {i}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    response = run_pipeline(user_input)\n",
    "    \n",
    "    print(f\"\\nUser Input:\\n  {response.user_input}\")\n",
    "    print(f\"\\nSocratic Question:\\n  {response.socratic_question}\")\n",
    "    print(f\"\\nKeyphrases:\")\n",
    "    for kp in response.keyphrases:\n",
    "        print(f\"  - {kp.phrase} ({kp.score:.3f})\")\n",
    "    print(f\"\\nRetrieved Contexts: {len(response.retrieved_contexts)}\")\n",
    "    for ctx in response.retrieved_contexts:\n",
    "        print(f\"  [{ctx.source}] {ctx.keyphrase}: {ctx.context[:100]}...\")\n",
    "    print(f\"\\nConcept Nodes: {len(response.concept_nodes)}\")\n",
    "    print(f\"Processing Time: {response.processing_time_ms:.1f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_widget = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter your opinion or statement here...',\n",
    "    description='Input:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "retrieval_checkbox = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='Use Context Retrieval',\n",
    "    indent=False\n",
    ")\n",
    "\n",
    "submit_button = widgets.Button(\n",
    "    description='Generate Socratic Question',\n",
    "    button_style='primary',\n",
    "    icon='question'\n",
    ")\n",
    "\n",
    "output_area = widgets.Output()\n",
    "\n",
    "def on_submit(b):\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        user_input = input_widget.value.strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            print(\"Please enter a statement or opinion.\")\n",
    "            return\n",
    "        \n",
    "        print(\"Processing...\")\n",
    "        \n",
    "        response = run_pipeline(user_input, use_retrieval=retrieval_checkbox.value)\n",
    "        \n",
    "        clear_output()\n",
    "        \n",
    "        html_output = f\"\"\"\n",
    "        <div style=\"font-family: Arial, sans-serif; max-width: 800px;\">\n",
    "            <h3 style=\"color: #1a73e8;\">ü§î Socratic Question</h3>\n",
    "            <div style=\"background: #e8f0fe; padding: 15px; border-radius: 8px; margin-bottom: 20px;\">\n",
    "                <strong>{response.socratic_question}</strong>\n",
    "            </div>\n",
    "            \n",
    "            <h4>üìå Key Concepts</h4>\n",
    "            <div style=\"display: flex; flex-wrap: wrap; gap: 8px; margin-bottom: 20px;\">\n",
    "        \"\"\"\n",
    "        \n",
    "        for kp in response.keyphrases:\n",
    "            html_output += f'<span style=\"background: #f1f3f4; padding: 4px 12px; border-radius: 16px; font-size: 14px;\">{kp.phrase}</span>'\n",
    "        \n",
    "        html_output += \"</div>\"\n",
    "        \n",
    "        if response.retrieved_contexts:\n",
    "            html_output += \"<h4>üìö Retrieved Context</h4>\"\n",
    "            for ctx in response.retrieved_contexts:\n",
    "                source_badge = \"ü§ñ Gemini\" if ctx.source == \"gemini\" else \"üìñ Wikipedia\"\n",
    "                html_output += f\"\"\"\n",
    "                <div style=\"background: #fafafa; padding: 10px; border-radius: 8px; margin-bottom: 10px; border-left: 3px solid #4285f4;\">\n",
    "                    <strong>{ctx.keyphrase}</strong> <span style=\"font-size: 12px; color: #666;\">{source_badge}</span>\n",
    "                    <p style=\"margin: 5px 0 0 0; font-size: 14px; color: #444;\">{ctx.context[:200]}...</p>\n",
    "                </div>\n",
    "                \"\"\"\n",
    "        \n",
    "        html_output += f\"\"\"\n",
    "            <p style=\"font-size: 12px; color: #666; margin-top: 20px;\">\n",
    "                ‚è±Ô∏è Processing time: {response.processing_time_ms:.0f}ms\n",
    "            </p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        \n",
    "        display(HTML(html_output))\n",
    "\n",
    "submit_button.on_click(on_submit)\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<h2>üéì SocraticPath Demo</h2>\"),\n",
    "    input_widget,\n",
    "    widgets.HBox([retrieval_checkbox, submit_button]),\n",
    "    output_area\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Pipeline for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_to_dict(response: PipelineResponse) -> Dict:\n",
    "    \"\"\"Convert pipeline response to JSON-serializable dict.\"\"\"\n",
    "    return {\n",
    "        \"user_input\": response.user_input,\n",
    "        \"socratic_question\": response.socratic_question,\n",
    "        \"keyphrases\": [\n",
    "            {\"phrase\": kp.phrase, \"score\": kp.score}\n",
    "            for kp in response.keyphrases\n",
    "        ],\n",
    "        \"contexts\": [\n",
    "            {\n",
    "                \"keyphrase\": ctx.keyphrase,\n",
    "                \"context\": ctx.context,\n",
    "                \"source\": ctx.source,\n",
    "                \"url\": ctx.url\n",
    "            }\n",
    "            for ctx in response.retrieved_contexts\n",
    "        ],\n",
    "        \"concept_nodes\": [\n",
    "            {\n",
    "                \"id\": node.id,\n",
    "                \"label\": node.label,\n",
    "                \"type\": node.node_type,\n",
    "                \"score\": node.score\n",
    "            }\n",
    "            for node in response.concept_nodes\n",
    "        ],\n",
    "        \"processing_time_ms\": response.processing_time_ms\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_response = run_pipeline(\n",
    "    \"Universal basic income is necessary because automation will eliminate most jobs.\"\n",
    ")\n",
    "\n",
    "api_response = pipeline_to_dict(sample_response)\n",
    "print(\"API Response Format:\")\n",
    "print(json.dumps(api_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Pipeline Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"../models/pipeline_config\")\n",
    "OUTPUT_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "config = {\n",
    "    \"model\": {\n",
    "        \"path\": str(MODEL_PATH),\n",
    "        \"type\": \"flan-t5\",\n",
    "        \"generation\": {\n",
    "            \"max_length\": 80,\n",
    "            \"num_beams\": 4,\n",
    "            \"do_sample\": True,\n",
    "            \"top_k\": 5,\n",
    "            \"top_p\": 0.6,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"no_repeat_ngram_size\": 3\n",
    "        }\n",
    "    },\n",
    "    \"keybert\": {\n",
    "        \"model\": \"all-MiniLM-L6-v2\",\n",
    "        \"top_n\": 5,\n",
    "        \"ngram_range\": [1, 2],\n",
    "        \"diversity\": 0.5\n",
    "    },\n",
    "    \"retrieval\": {\n",
    "        \"gemini_model\": \"gemini-1.5-flash\",\n",
    "        \"max_keyphrases_for_retrieval\": 3,\n",
    "        \"wikipedia_fallback\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUTPUT_PATH / \"config.json\", \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"Pipeline configuration saved to {OUTPUT_PATH / 'config.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "test_statements = [\n",
    "    \"I think video games make children violent.\",\n",
    "    \"We should abolish the electoral college.\",\n",
    "    \"Space exploration is a waste of money.\",\n",
    "    \"Nuclear energy is too dangerous to use.\",\n",
    "    \"Social media should be regulated by the government.\"\n",
    "]\n",
    "\n",
    "print(\"Running performance benchmark...\\n\")\n",
    "\n",
    "times = []\n",
    "for stmt in test_statements:\n",
    "    response = run_pipeline(stmt)\n",
    "    times.append(response.processing_time_ms)\n",
    "    print(f\"‚úì {stmt[:50]}... ({response.processing_time_ms:.0f}ms)\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Performance Summary:\")\n",
    "print(f\"  Average: {statistics.mean(times):.0f}ms\")\n",
    "print(f\"  Median: {statistics.median(times):.0f}ms\")\n",
    "print(f\"  Min: {min(times):.0f}ms\")\n",
    "print(f\"  Max: {max(times):.0f}ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Complete!\n",
    "\n",
    "**Components:**\n",
    "1. ‚úÖ FLAN-T5 Socratic Question Generation\n",
    "2. ‚úÖ KeyBERT Keyphrase Extraction\n",
    "3. ‚úÖ Gemini + Wikipedia Context Retrieval\n",
    "4. ‚úÖ Concept Node Generation\n",
    "\n",
    "**Next Steps:**\n",
    "1. Deploy as FastAPI backend\n",
    "2. Build React Flow frontend\n",
    "3. Add concept map visualization\n",
    "4. Implement user session management\n",
    "\n",
    "---\n",
    "\n",
    "**Files Created:**\n",
    "- `../models/pipeline_config/config.json` - Pipeline configuration"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
